# Chapter 4: AI Models Landscape

**Part 2: The AI Ecosystem**

---

## Navigation

â† Previous: [Chapter 3: The Art of Prompting](./03-the-art-of-prompting.md) | Next: [Chapter 5: Introduction to Claude](./05-introduction-to-claude.md) â†’

**Quick Nav:** [README](../README.md) | [Table of Contents](../README.md#table-of-contents)

---


## Navigating the World of AI Models and Providers

**ğŸ“– Reading time:** ~14 minutes
**ğŸ¯ Quick nav:** [What are Models?](#41-understanding-models) | [Major Providers](#42-major-ai-companies-and-their-models) | [Proprietary vs Open Source](#43-proprietary-vs-open-source-models) | [Model Selection](#46-model-selection-guide-for-devops)

---

Understanding the AI ecosystem helps you make informed decisions about which tools to use for different tasks. This chapter covers the major players, model types, and how to choose the right model for your needs.

---

## 4.1 Understanding "Models"

### What is a Model?

A **model** is a trained AI system that can perform specific tasks. Think of it as a specialized program that has learned from data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   THE MODEL LIFECYCLE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  1. ARCHITECTURE DESIGN                                        â”‚
â”‚     "How should the neural network be structured?"             â”‚
â”‚     â””â”€â–º Transformer, CNN, RNN, etc.                            â”‚
â”‚                                                                â”‚
â”‚  2. PRE-TRAINING                                               â”‚
â”‚     "Learn general knowledge from massive data"                â”‚
â”‚     â””â”€â–º Billions of tokens of text, code, etc.                 â”‚
â”‚         â””â”€â–º Months of training on GPU clusters                 â”‚
â”‚                                                                â”‚
â”‚  3. FINE-TUNING                                                â”‚
â”‚     "Specialize for specific tasks"                            â”‚
â”‚     â””â”€â–º Instruction tuning, RLHF, domain adaptation            â”‚
â”‚                                                                â”‚
â”‚  4. DEPLOYMENT                                                 â”‚
â”‚     "Make it available for use"                                â”‚
â”‚     â””â”€â–º API endpoints, downloadable weights, etc.              â”‚
â”‚                                                                â”‚
â”‚  5. INFERENCE                                                  â”‚
â”‚     "Using the model to get predictions"                       â”‚
â”‚     â””â”€â–º What you do when you chat with an AI                   â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Naming Conventions

```
Understanding model names:

Claude Sonnet 4.5
â”‚      â”‚     â”‚
â”‚      â”‚     â””â”€â”€ Version (major iteration)
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€ Variant (capability tier within the family)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Family name (product line)

GPT-4-Turbo-128K
â”‚   â”‚ â”‚     â”‚
â”‚   â”‚ â”‚     â””â”€â”€ Context window size
â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€ Variant (optimized version)
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Version
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Family name

LLaMA-2-70B-Chat
â”‚     â”‚ â”‚   â”‚
â”‚     â”‚ â”‚   â””â”€â”€ Specialization (chat-optimized)
â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€ Parameter count (70 billion)
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€ Version
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Family name
```

---

## 4.2 Major AI Companies and Their Models

### The Big Players

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MAJOR AI COMPANIES (2024)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ANTHROPIC                                              â”‚    â”‚
â”‚  â”‚  "AI Safety First"                                      â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Founded: 2021 by ex-OpenAI researchers                 â”‚    â”‚
â”‚  â”‚  Focus: Safe, helpful, honest AI                        â”‚    â”‚
â”‚  â”‚  Models: Claude family                                  â”‚    â”‚
â”‚  â”‚  Notable: Constitutional AI, long context windows       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  OPENAI                                                 â”‚    â”‚
â”‚  â”‚  "AGI for the benefit of humanity"                      â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Founded: 2015                                          â”‚    â”‚
â”‚  â”‚  Focus: General AI advancement                          â”‚    â”‚
â”‚  â”‚  Models: GPT family, DALL-E, Whisper, Codex             â”‚    â”‚
â”‚  â”‚  Notable: ChatGPT, API platform, Microsoft partnership  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  GOOGLE (DeepMind + Google AI)                          â”‚    â”‚
â”‚  â”‚  "Organizing the world's information"                   â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Focus: Research + product integration                  â”‚    â”‚
â”‚  â”‚  Models: Gemini, PaLM, Bard                             â”‚    â”‚
â”‚  â”‚  Notable: AlphaFold, integrated into Google products    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  META (Facebook AI Research)                            â”‚    â”‚
â”‚  â”‚  "Open AI research"                                     â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Focus: Open-source AI                                  â”‚    â”‚
â”‚  â”‚  Models: LLaMA family (open source!)                    â”‚    â”‚
â”‚  â”‚  Notable: Democratizing AI access                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  MISTRAL AI                                             â”‚    â”‚
â”‚  â”‚  "Open and efficient AI"                                â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Founded: 2023 in France                                â”‚    â”‚
â”‚  â”‚  Focus: Efficient, open models                          â”‚    â”‚
â”‚  â”‚  Models: Mistral, Mixtral (MoE)                         â”‚    â”‚
â”‚  â”‚  Notable: High performance at smaller sizes             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Model Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MODEL COMPARISON MATRIX                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model         â”‚ Provider     â”‚ Context      â”‚ Best For     â”‚ API Access       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude Opus   â”‚ Anthropic    â”‚ 200K tokens  â”‚ Complex      â”‚ API + Console    â”‚
â”‚ 4.5           â”‚              â”‚              â”‚ reasoning    â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude Sonnet â”‚ Anthropic    â”‚ 200K tokens  â”‚ Balanced     â”‚ API + Console    â”‚
â”‚ 4.5           â”‚              â”‚              â”‚ performance  â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude Haiku  â”‚ Anthropic    â”‚ 200K tokens  â”‚ Speed +      â”‚ API + Console    â”‚
â”‚ 4.5           â”‚              â”‚              â”‚ Cost         â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4 Turbo   â”‚ OpenAI       â”‚ 128K tokens  â”‚ General      â”‚ API + ChatGPT    â”‚
â”‚               â”‚              â”‚              â”‚ purpose      â”‚ Plus             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4o        â”‚ OpenAI       â”‚ 128K tokens  â”‚ Multimodal   â”‚ API + ChatGPT    â”‚
â”‚               â”‚              â”‚              â”‚ tasks        â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemini Ultra  â”‚ Google       â”‚ 1M tokens    â”‚ Long context â”‚ API + Gemini     â”‚
â”‚               â”‚              â”‚              â”‚ Multimodal   â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemini Pro    â”‚ Google       â”‚ 32K tokens   â”‚ General      â”‚ API + Vertex AI  â”‚
â”‚               â”‚              â”‚              â”‚ purpose      â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLaMA 3 70B   â”‚ Meta         â”‚ 8K tokens    â”‚ Self-hosting â”‚ Open weights     â”‚
â”‚               â”‚              â”‚              â”‚ Privacy      â”‚ Free!            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mixtral 8x7B  â”‚ Mistral      â”‚ 32K tokens   â”‚ Efficiency   â”‚ Open weights     â”‚
â”‚               â”‚              â”‚              â”‚ Self-hosting â”‚ + API            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CodeLlama     â”‚ Meta         â”‚ 16K tokens   â”‚ Code tasks   â”‚ Open weights     â”‚
â”‚               â”‚              â”‚              â”‚              â”‚ Free!            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.3 Proprietary vs Open Source Models

### Proprietary Models (Closed Source)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROPRIETARY MODELS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Examples: Claude, GPT-4, Gemini                               â”‚
â”‚                                                                â”‚
â”‚  PROS:                                                         â”‚
â”‚  âœ“ State-of-the-art performance                                â”‚
â”‚  âœ“ No infrastructure to manage                                 â”‚
â”‚  âœ“ Regular updates and improvements                            â”‚
â”‚  âœ“ Better safety and alignment                                 â”‚
â”‚  âœ“ Enterprise support available                                â”‚
â”‚  âœ“ Easy to start (just an API key)                             â”‚
â”‚                                                                â”‚
â”‚  CONS:                                                         â”‚
â”‚  âœ— Ongoing costs (pay per token)                               â”‚
â”‚  âœ— Data leaves your infrastructure                             â”‚
â”‚  âœ— Vendor lock-in risk                                         â”‚
â”‚  âœ— Rate limits and quotas                                      â”‚
â”‚  âœ— Can't customize model weights                               â”‚
â”‚  âœ— Dependent on provider's policies                            â”‚
â”‚                                                                â”‚
â”‚  BEST FOR:                                                     â”‚
â”‚  â€¢ Most production use cases                                   â”‚
â”‚  â€¢ When you need the best quality                              â”‚
â”‚  â€¢ When you don't want to manage infra                         â”‚
â”‚  â€¢ Rapid prototyping                                           â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Open Source Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPEN SOURCE MODELS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Examples: LLaMA 2, Mistral, Falcon, MPT                       â”‚
â”‚                                                                â”‚
â”‚  PROS:                                                         â”‚
â”‚  âœ“ Free to use (no per-token cost)                             â”‚
â”‚  âœ“ Data stays on your infrastructure                           â”‚
â”‚  âœ“ Full control and customization                              â”‚
â”‚  âœ“ Can fine-tune for specific tasks                            â”‚
â”‚  âœ“ No vendor lock-in                                           â”‚
â”‚  âœ“ Community support and improvements                          â”‚
â”‚                                                                â”‚
â”‚  CONS:                                                         â”‚
â”‚  âœ— Requires GPU infrastructure                                 â”‚
â”‚  âœ— Generally lower performance than proprietary                â”‚
â”‚  âœ— You manage updates and security                             â”‚
â”‚  âœ— Need ML expertise to deploy                                 â”‚
â”‚  âœ— Higher upfront investment                                   â”‚
â”‚  âœ— Smaller context windows typically                           â”‚
â”‚                                                                â”‚
â”‚  BEST FOR:                                                     â”‚
â”‚  â€¢ Strict data privacy requirements                            â”‚
â”‚  â€¢ High-volume, predictable workloads                          â”‚
â”‚  â€¢ Custom/specialized use cases                                â”‚
â”‚  â€¢ When you have ML engineering capacity                       â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Framework

```python
# Model Selection Decision Tree

def select_model_type(requirements):
    """
    Help decide between proprietary and open source models.
    """

    # Priority 1: Data Privacy
    if requirements.data_must_stay_on_premise:
        if requirements.can_run_gpu_infrastructure:
            return "Open Source (LLaMA 2, Mistral)"
        else:
            return "Private Cloud Deployment (Azure OpenAI, AWS Bedrock)"

    # Priority 2: Performance Requirements
    if requirements.needs_state_of_the_art:
        return "Proprietary (Claude Opus 4.5, GPT-4)"

    # Priority 3: Cost Optimization
    if requirements.high_volume and requirements.predictable_workload:
        estimated_api_cost = requirements.tokens_per_month * PRICE_PER_TOKEN
        estimated_gpu_cost = GPU_HOURLY_RATE * 24 * 30

        if estimated_gpu_cost < estimated_api_cost * 0.5:
            return "Open Source (cost-effective)"
        else:
            return "Proprietary (simpler, similar cost)"

    # Priority 4: Speed to Market
    if requirements.need_to_ship_fast:
        return "Proprietary (fastest to implement)"

    # Default
    return "Proprietary (safest choice for most cases)"
```

###Decision Tree Rationale: Real-World Scenarios

The decision tree above provides a framework, but let's see how it applies to concrete DevOps scenarios with actual costs and trade-offs.

#### Scenario 1: Startup with Limited Budget (<$500/month AI budget)

**Context**: Early-stage SaaS company, 50 customer support tickets/day, need AI-assisted ticket triage and response generation.

**Analysis**:
- Input: ~500 tokens/ticket (customer message + context)
- Output: ~300 tokens/ticket (suggested response)
- Volume: 50 tickets Ã— 30 days = 1,500 requests/month
- Total tokens: (500 + 300) Ã— 1,500 = 1.2M tokens/month

**Cost Comparison**:
```
Claude Sonnet 4.5:
  Input:  (750K / 1M) Ã— $3  = $2.25
  Output: (450K / 1M) Ã— $15 = $6.75
  Total: $9/month âœ“ Well within budget

Claude Haiku 4.5 (even cheaper):
  Input:  (750K / 1M) Ã— $1  = $0.75
  Output: (450K / 1M) Ã— $5  = $2.25
  Total: $3/month âœ“ Extremely cost-effective

Self-hosted LLaMA 2 7B:
  GPU instance: ~$200-$400/month (AWS g5.xlarge)
  Setup time: 40 hours engineering @ $150/hr = $6,000
  Total first month: $6,200+ âœ— Overkill for this scale
```

**Decision**: Use **Claude Haiku 4.5**. At $3/month, self-hosting makes zero financial sense. You'd need to process >100K tickets/month before GPU costs break even.

#### Scenario 2: Enterprise with Compliance Requirements (Financial Services)

**Context**: Bank needs to analyze loan application documents (PDF parsing + decision recommendation). Data cannot leave private network due to regulatory requirements.

**Analysis**:
- Volume: 200 loan applications/day
- Data sensitivity: Extremely high (PII, financial data)
- Compliance: SOC 2, PCI-DSS, data residency requirements

**Cost Comparison**:
```
Option 1: AWS Bedrock (Claude in VPC)
  - Cost: ~$500-$1000/month depending on volume
  - Pros: Stays in AWS VPC, meets compliance
  - Cons: Still sends data to AWS (some teams uncomfortable)
  - Setup time: 2-3 days

Option 2: Azure OpenAI (Private deployment)
  - Cost: Similar to Bedrock
  - Pros: Can deploy in private VNET
  - Cons: Limited model selection vs Anthropic

Option 3: Self-hosted LLaMA 2 70B
  - GPU cost: ~$2,000/month (8Ã— A100 or equivalent)
  - Engineering: 160 hours setup + 40 hours/month maintenance
  - Initial cost: $26,000 first month
  - Ongoing: $8,000/month (GPU + eng)
  - Pros: Complete control, data never leaves network
  - Cons: Significantly higher cost, lower quality than Claude
```

**Decision**: Use **AWS Bedrock with Claude Sonnet 4.5** in a VPC. While self-hosting provides maximum control, the 8-10x cost difference doesn't justify it unless data cannot even touch AWS infrastructure (rare for banks already using AWS). The "data must stay on-premise" requirement is usually satisfied by VPC deployment.

#### Scenario 3: High-Volume Log Analysis (500GB logs/day)

**Context**: SaaS platform with 10K customers generating massive log volumes. Need to auto-categorize errors and extract root causes.

**Analysis**:
- Volume: 500GB logs/day = ~125 billion tokens/day
- Processing: Cannot send all logs to LLM (cost prohibitive)
- Solution: Two-tier approach

**Hybrid Architecture**:
```
Tier 1: Local classifier (self-hosted)
  - Use fine-tuned DistilBERT (50MB model)
  - Runs on CPU, filters 95% of logs
  - Cost: ~$100/month for inference servers
  - Categorizes: ERROR, WARN, INFO

Tier 2: LLM analysis (Claude)
  - Only send ERROR logs (5% of volume)
  - 5% of 125B tokens = 6.25B tokens/day
  - Monthly: 187B tokens

Claude Sonnet 4.5 cost:
  - Input:  (94B / 1M) Ã— $3  = $282,000/month âœ— Prohibitive!
  - Output: (94B / 1M) Ã— $15 = $1.41M/month  âœ— No way!

Revised approach with caching:
  - Cache common error patterns (80% hit rate)
  - Actual LLM calls: 20% of 6.25B = 1.25B tokens/day
  - Monthly: ~37B tokens
  - Cost: (18.5B/1M Ã— $3) + (18.5B/1M Ã— $15) = $55K + $278K = $333K/month

Self-hosted LLaMA 3 70B option:
  - Infrastructure: $10,000/month (GPU cluster)
  - Engineering: $25,000/month (3 ML engineers)
  - Total: $35,000/month âœ“ Significantly cheaper at this scale
```

**Decision**: **Self-host LLaMA 3 70B**. At this volume, the break-even point is reached. Self-hosting costs $35K/month vs $333K/month for Claude (even with aggressive caching). The quality difference is acceptable for log categorization.

**Key insight**: The break-even point for self-hosting typically occurs around:
- **>20M tokens/day** if you already have GPU infrastructure
- **>50M tokens/day** if you need to build infrastructure from scratch

#### Scenario 4: Prototyping a New AI Feature (Startup MVP)

**Context**: Testing whether AI-powered code review adds value before committing to infrastructure.

**Analysis**:
- Timeline: Need proof-of-concept in 2 weeks
- Volume: Unknown (testing phase)
- Team: 2 developers, no ML expertise

**Decision Matrix**:
```
Self-hosted approach:
  - Setup time: 2-3 weeks (learning curve)
  - Infrastructure: 1 week to provision GPUs
  - Integration: 1 week
  - Total: 4-5 weeks âœ— Misses deadline

Proprietary API approach:
  - Setup time: 1 hour (API key + SDK install)
  - Integration: 3-4 days
  - Total: 4-5 days âœ“ Meets deadline
```

**Decision**: **Use Claude Sonnet 4.5 API**. Speed-to-market trumps all other considerations. You can always migrate to self-hosting later if volume justifies it. The cost of delaying market validation is far higher than API fees during testing.

**Practical tip**: Many teams build with proprietary APIs, then switch to self-hosted models once they hit predictable high volume. This "API-first, self-host-later" approach minimizes risk.

### When to Reconsider Your Choice

Your model selection isn't permanent. Reevaluate when:

- **Volume increases 10x**: Self-hosting becomes cost-effective
- **Compliance requirements change**: May need to move to private deployment
- **New models release**: GPT-5, Claude 5, etc. may shift the landscape
- **Budget changes**: Startup series A funding might enable self-hosting
- **Performance issues**: If API latency becomes a bottleneck

### Popular Open Source Models for DevOps

```yaml
# Open Source Models Worth Knowing

code_generation:
  name: "CodeLlama"
  sizes: [7B, 13B, 34B]
  context: 16K tokens
  strengths:
    - Code completion
    - Code explanation
    - Bug fixing
  run_locally: "Yes, 7B fits on 16GB GPU"
  example: |
    # Can run locally with llama.cpp
    ./main -m codellama-7b.gguf -p "Write a bash script to..."

general_purpose:
  name: "LLaMA 2"
  sizes: [7B, 13B, 70B]
  context: 4K tokens
  strengths:
    - General reasoning
    - Conversation
    - Following instructions
  run_locally: "7B/13B on consumer GPUs, 70B needs A100"

efficient_reasoning:
  name: "Mixtral 8x7B"
  architecture: "Mixture of Experts (MoE)"
  context: 32K tokens
  strengths:
    - Efficient inference
    - Strong reasoning
    - Multilingual
  special: "Only uses 2 experts at a time (faster than 47B model)"

embedding_models:
  - name: "all-MiniLM-L6-v2"
    use_case: "Semantic search"
    provider: "Sentence Transformers"
  - name: "text-embedding-ada-002"
    use_case: "High quality embeddings"
    provider: "OpenAI"
```

---

## 4.4 Types of Models for Different Tasks

### Model Taxonomy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MODEL TYPES BY TASK                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  TEXT GENERATION (LLMs)                                                    â”‚
â”‚  â”œâ”€â”€ General purpose: Claude, GPT-4, Gemini                                â”‚
â”‚  â”œâ”€â”€ Code specialized: CodeLlama, StarCoder, Codex                         â”‚
â”‚  â””â”€â”€ Instruction-tuned: Vicuna, Alpaca, WizardLM                           â”‚
â”‚                                                                            â”‚
â”‚  TEXT UNDERSTANDING                                                        â”‚
â”‚  â”œâ”€â”€ Classification: BERT, RoBERTa, DistilBERT                             â”‚
â”‚  â”œâ”€â”€ Named Entity Recognition: SpaCy models, Flair                         â”‚
â”‚  â””â”€â”€ Sentiment Analysis: Specialized BERT variants                         â”‚
â”‚                                                                            â”‚
â”‚  EMBEDDINGS (Vector Representations)                                       â”‚
â”‚  â”œâ”€â”€ Text: OpenAI Ada, Sentence-BERT, E5                                   â”‚
â”‚  â”œâ”€â”€ Code: CodeBERT, UniXcoder                                             â”‚
â”‚  â””â”€â”€ Multi-modal: CLIP (text + images)                                     â”‚
â”‚                                                                            â”‚
â”‚  IMAGE GENERATION                                                          â”‚
â”‚  â”œâ”€â”€ DALL-E 3 (OpenAI)                                                     â”‚
â”‚  â”œâ”€â”€ Midjourney                                                            â”‚
â”‚  â”œâ”€â”€ Stable Diffusion (open source!)                                       â”‚
â”‚  â””â”€â”€ Imagen (Google)                                                       â”‚
â”‚                                                                            â”‚
â”‚  IMAGE UNDERSTANDING                                                       â”‚
â”‚  â”œâ”€â”€ GPT-4V (Vision)                                                       â”‚
â”‚  â”œâ”€â”€ Claude 3 (Vision)                                                     â”‚
â”‚  â”œâ”€â”€ LLaVA (open source)                                                   â”‚
â”‚  â””â”€â”€ BLIP-2                                                                â”‚
â”‚                                                                            â”‚
â”‚  SPEECH                                                                    â”‚
â”‚  â”œâ”€â”€ Speech-to-Text: Whisper (OpenAI, open source!)                        â”‚
â”‚  â”œâ”€â”€ Text-to-Speech: ElevenLabs, Azure, Amazon Polly                       â”‚
â”‚  â””â”€â”€ Voice Cloning: Various providers                                      â”‚
â”‚                                                                            â”‚
â”‚  SPECIALIZED                                                               â”‚
â”‚  â”œâ”€â”€ SQL Generation: SQLCoder, NSQL                                        â”‚
â”‚  â”œâ”€â”€ Math/Reasoning: Specialized fine-tunes                                â”‚
â”‚  â””â”€â”€ Scientific: BioGPT, ChemBERTa                                         â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DevOps-Relevant Model Applications

```yaml
# Model Selection for DevOps Tasks

log_analysis:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Long context for log files, excellent reasoning"
    - name: "GPT-4"
      reason: "Good at pattern recognition in logs"
  approach:
    simple_classification:
      model: "Fine-tuned BERT (local)"
      use_case: "Categorizing log levels, error types"
    semantic_analysis:
      model: "Claude/GPT-4"
      use_case: "Understanding error context, root cause analysis"

code_review:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Excellent at code understanding and security analysis"
    - name: "CodeLlama 34B"
      reason: "Open source, can run locally for sensitive code"
  example_prompt: |
    Review this Terraform code for security issues:
    [code]

documentation:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Great at technical writing"
    - name: "GPT-4"
      reason: "Comprehensive documentation generation"
  automation_idea: |
    # Auto-generate docs from code
    for file in src/*.py; do
        ai_generate_docs "$file" >> docs/api.md
    done

incident_response:
  recommended_models:
    - name: "Claude Opus 4.5"
      reason: "Best for complex reasoning about incidents"
  workflow:
    1_collect: "Gather logs, metrics, recent changes"
    2_analyze: "Send to LLM with incident context template"
    3_hypothesize: "Get ranked list of potential causes"
    4_verify: "Human validates AI suggestions"

infrastructure_as_code:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Excellent Terraform/CloudFormation generation"
    - name: "GPT-4"
      reason: "Strong at AWS/GCP/Azure specifics"
  tips:
    - "Always specify cloud provider and version"
    - "Include compliance requirements in prompt"
    - "Request explanatory comments"

chatops_bot:
  recommended_models:
    - name: "Claude Haiku 4.5"
      reason: "Fast, cheap, good for simple queries"
    - name: "GPT-4o-mini"
      reason: "Fast responses, cost-effective"
  use_cases:
    - "Answer questions about infrastructure"
    - "Execute predefined runbooks"
    - "Summarize alerts and incidents"
```

### Real-World Model Comparison: Same Task, Different Models

Recommendations are helpful, but seeing actual output differences is more valuable. Let's compare how different models handle the same DevOps tasks.

#### Task 1: Analyzing a Production Error Log

**Input Prompt** (same for all models):
```
Analyze this production error and suggest root cause:

[ERROR] 2025-01-10 14:32:41 - API Gateway timeout
[INFO] Upstream service response time: 31.2s (expected <5s)
[WARN] Database connection pool: 95/100 connections in use
[INFO] CPU usage: 45%, Memory: 72%, Disk I/O: nominal
[ERROR] 3 retries attempted, all failed
[CONTEXT] Traffic increased 40% after marketing campaign launch
```

**Claude Sonnet 4.5 Output** (Quality: A+):
```
ROOT CAUSE: Database connection pool exhaustion

ANALYSIS:
1. Traffic spike (+40%) is pushing connection pool to limits (95/100)
2. Slow queries are holding connections open longer than expected
3. API Gateway timeouts (31s) suggest database is the bottleneck, not compute
4. CPU/memory are healthy, ruling out resource constraints

IMMEDIATE FIX:
1. Increase connection pool: 100 â†’ 200 connections
2. Add connection pool monitoring alerts (trigger at 80%)
3. Implement query timeout of 10s to prevent connection hogging

LONG-TERM:
1. Analyze slow query logs for optimization opportunities
2. Consider read replicas to distribute SELECT load
3. Implement database connection pooling at application layer (e.g., PgBouncer)
4. Add circuit breaker pattern for graceful degradation

CONFIDENCE: High (95%) - Classic connection pool exhaustion pattern
```

**GPT-4 Turbo Output** (Quality: A):
```
This appears to be a database bottleneck issue. The 31-second response time combined
with near-max connection pool usage (95/100) suggests your database can't handle the
increased load from the marketing campaign.

Recommendations:
- Scale up database instance
- Increase connection pool size
- Optimize slow queries
- Add caching layer (Redis)
- Consider read replicas

The healthy CPU/memory indicates the application servers are fine.
```

**CodeLlama 34B Output** (Quality: B):
```
Error: API Gateway timeout
Cause: Database connection pool full
Solution: Increase pool size or optimize queries
```

**Quality Comparison**:
- **Claude**: Structured analysis, immediate + long-term fixes, confidence level
- **GPT-4**: Solid analysis, good recommendations, less structured
- **CodeLlama**: Technically correct but lacks depth and actionability

**When to use each**:
- **Production incidents** â†’ Claude Sonnet 4.5 (best reasoning, structured output)
- **Quick triage** â†’ GPT-4 Turbo (fast, good enough)
- **Automated classification** â†’ CodeLlama (cheap, identifies issue correctly)

#### Task 2: Generating Infrastructure as Code

**Input Prompt**:
```
Create a production-ready Kubernetes deployment for a Node.js API with:
- 3 replicas for high availability
- Health checks (liveness + readiness)
- Resource limits
- Rolling update strategy
- Environment variables from ConfigMap
```

**Claude Sonnet 4.5 Output** (Lines: 95, Quality: Production-ready):
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
  labels:
    app: nodejs-api
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Ensures zero-downtime deployments
  selector:
    matchLabels:
      app: nodejs-api
  template:
    metadata:
      labels:
        app: nodejs-api
        version: v1
    spec:
      containers:
      - name: api
        image: nodejs-api:latest
        ports:
        - containerPort: 3000
          name: http
          protocol: TCP

        # Resource limits prevent one pod from consuming all node resources
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"

        # Liveness: Restart if application is deadlocked
        livenessProbe:
          httpGet:
            path: /healthz
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        # Readiness: Remove from service if not ready to serve traffic
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3

        # Load configuration from ConfigMap
        envFrom:
        - configMapRef:
            name: nodejs-api-config

        # Security best practices
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true

---
# ConfigMap for application configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: nodejs-api-config
data:
  NODE_ENV: "production"
  LOG_LEVEL: "info"
  API_TIMEOUT: "30s"

---
# Service to expose the deployment
apiVersion: v1
kind: Service
metadata:
  name: nodejs-api
spec:
  selector:
    app: nodejs-api
  ports:
  - port: 80
    targetPort: 3000
  type: ClusterIP
```
**Comments**: Complete manifest with explanations, security hardening, and all requested features.

**GPT-4 Turbo Output** (Lines: 70, Quality: Good):
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nodejs-api
  template:
    metadata:
      labels:
        app: nodejs-api
    spec:
      containers:
      - name: nodejs-api
        image: nodejs-api:latest
        ports:
        - containerPort: 3000
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 15
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
        envFrom:
        - configMapRef:
            name: api-config
```
**Comments**: Solid, covers requirements, but missing security context and detailed probe configuration.

**CodeLlama 34B Output** (Lines: 30, Quality: Basic):
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: api
        image: nodejs-api
        ports:
        - containerPort: 3000
```
**Comments**: Incomplete - missing health checks, resource limits, and ConfigMap integration.

**Cost vs Quality Analysis**:
```
For 100 IaC generation requests:

Claude Sonnet 4.5:
  - Avg input: 200 tokens, output: 800 tokens
  - Cost: (20K/1M Ã— $3) + (80K/1M Ã— $15) = $1.26
  - Quality: Production-ready, minimal edits needed
  - Engineer time saved: ~2 hours @ $150/hr = $300

GPT-4 Turbo:
  - Cost: (20K/1M Ã— $10) + (80K/1M Ã— $30) = $2.60
  - Quality: Good, needs 15min of touch-ups per manifest
  - Engineer time needed: ~25 hours @ $150/hr = $3,750

CodeLlama 34B (self-hosted):
  - Cost: $0 (already running)
  - Quality: Requires 1-2 hours of work per manifest
  - Engineer time needed: ~150 hours @ $150/hr = $22,500
```

**Key insight**: For IaC generation, Claude Sonnet 4.5 is cheapest when you factor in engineer time, despite higher per-token costs than GPT-4.

#### Task 3: Security Code Review

**Input**: Terraform file with intentional security issues

**Output Comparison**:
- **Claude Sonnet 4.5**: Found 5/5 security issues, explained each with fix
- **GPT-4 Turbo**: Found 4/5 security issues, missed hardcoded secret in variable
- **CodeLlama 34B**: Found 2/5 security issues (only obvious ones)

**Conclusion**: For security-critical tasks, the quality gap justifies Claude Opus 4.5 (even better than Sonnet) despite 3-5x cost vs alternatives.

### Model Selection Decision Matrix: Quick Reference

| Use Case | Low Volume | Medium Volume | High Volume | Privacy Required |
|----------|-----------|---------------|-------------|------------------|
| **Code Review** | Claude Sonnet | Claude Sonnet | Claude Haiku + caching | CodeLlama 34B |
| **IaC Generation** | Claude Sonnet | Claude Sonnet | Claude Haiku | CodeLlama / Mistral |
| **Log Analysis** | Claude Haiku | Claude Haiku + caching | Self-hosted classifier | LLaMA 3 70B |
| **Documentation** | Claude Sonnet | Claude Haiku | Claude Haiku | LLaMA 3 70B |
| **Incident Response** | Claude Opus | Claude Sonnet | Claude Sonnet | LLaMA 3 70B |
| **ChatOps Bot** | Claude Haiku | Claude Haiku | Claude Haiku | Mistral 7B |

**Volume definitions**:
- Low: <1M tokens/month (~$50/month budget)
- Medium: 1M-10M tokens/month ($50-$500/month)
- High: >10M tokens/month (>$500/month, consider self-hosting)

---

## 4.5 Model Hosting Options

### Where to Run Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MODEL HOSTING OPTIONS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  1. DIRECT API ACCESS                                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  Provider APIs: api.anthropic.com, api.openai.com                â”‚   â”‚
â”‚     â”‚  Pros: Simplest, always latest models                            â”‚   â”‚
â”‚     â”‚  Cons: Data leaves your network                                  â”‚   â”‚
â”‚     â”‚  Cost: Pay per token                                             â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  2. CLOUD PROVIDER MARKETPLACES                                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  AWS Bedrock: Claude, LLaMA, Titan                               â”‚   â”‚
â”‚     â”‚  Azure OpenAI: GPT-4, GPT-3.5                                    â”‚   â”‚
â”‚     â”‚  Google Vertex AI: Gemini, PaLM                                  â”‚   â”‚
â”‚     â”‚  Pros: Compliance, VPC integration, enterprise features          â”‚   â”‚
â”‚     â”‚  Cons: Slight markup, limited model selection                    â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  3. SELF-HOSTED (Open Source)                                              â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  Options:                                                        â”‚   â”‚
â”‚     â”‚  - vLLM (production-grade serving)                               â”‚   â”‚
â”‚     â”‚  - Text Generation Inference (Hugging Face)                      â”‚   â”‚
â”‚     â”‚  - Ollama (easy local deployment)                                â”‚   â”‚
â”‚     â”‚  - llama.cpp (CPU inference)                                     â”‚   â”‚
â”‚     â”‚                                                                  â”‚   â”‚
â”‚     â”‚  Pros: Full control, data privacy, no per-token cost             â”‚   â”‚
â”‚     â”‚  Cons: Infrastructure overhead, GPU costs, expertise needed      â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  4. MANAGED OPEN SOURCE                                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  Providers:                                                      â”‚   â”‚
â”‚     â”‚  - Together.ai                                                   â”‚   â”‚
â”‚     â”‚  - Anyscale                                                      â”‚   â”‚
â”‚     â”‚  - Replicate                                                     â”‚   â”‚
â”‚     â”‚  - Modal                                                         â”‚   â”‚
â”‚     â”‚                                                                  â”‚   â”‚
â”‚     â”‚  Pros: Open source models, managed infrastructure                â”‚   â”‚
â”‚     â”‚  Cons: Still have API costs, less control                        â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hosting Decision Flowchart: Which Option Is Right for You?

Choosing the right hosting option depends on your requirements, budget, and team capabilities. Use this decision tree to find your best fit.

```
START: "I need to run AI models for my DevOps workflows"
â”‚
â”œâ”€â“ Do you have strict data residency requirements?
â”‚  â”‚   (e.g., data cannot leave your country/network)
â”‚  â”‚
â”‚  â”œâ”€ YES â†’ â“ Do you have GPU infrastructure already?
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ YES â†’ âœ… SELF-HOST OPEN SOURCE (Ollama/vLLM)
â”‚  â”‚  â”‚           Cost: GPU maintenance only
â”‚  â”‚  â”‚           Setup: 1-2 weeks
â”‚  â”‚  â”‚           Skill: High (need ML ops expertise)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ NO â†’ â“ Can data touch cloud provider infrastructure?
â”‚  â”‚     â”‚
â”‚  â”‚     â”œâ”€ YES â†’ âœ… AWS BEDROCK / AZURE OPENAI (VPC)
â”‚  â”‚     â”‚           Cost: ~$500-2K/month
â”‚  â”‚     â”‚           Setup: 2-3 days
â”‚  â”‚     â”‚           Skill: Medium (DevOps + cloud networking)
â”‚  â”‚     â”‚
â”‚  â”‚     â””â”€ NO â†’ âœ… BUILD GPU CLUSTER + SELF-HOST
â”‚  â”‚                Cost: $10K-50K/month
â”‚  â”‚                Setup: 1-3 months
â”‚  â”‚                Skill: Very High (ML ops + infrastructure)
â”‚  â”‚
â”‚  â””â”€ NO â†’ â“ What's your expected monthly token volume?
â”‚     â”‚
â”‚     â”œâ”€ <10M tokens/month (most teams)
â”‚     â”‚  â”‚
â”‚     â”‚  â”œâ”€â“ Do you need enterprise features?
â”‚     â”‚  â”‚  (SSO, private deployment, SLA guarantees)
â”‚     â”‚  â”‚  â”‚
â”‚     â”‚  â”‚  â”œâ”€ YES â†’ âœ… CLOUD MARKETPLACE (Bedrock/Azure)
â”‚     â”‚  â”‚  â”‚           Cost: ~$100-1K/month
â”‚     â”‚  â”‚  â”‚           Setup: Same day
â”‚     â”‚  â”‚  â”‚           Skill: Low (just API integration)
â”‚     â”‚  â”‚  â”‚
â”‚     â”‚  â”‚  â””â”€ NO â†’ âœ… DIRECT API (anthropic.com/openai.com)
â”‚     â”‚  â”‚             Cost: $10-500/month
â”‚     â”‚  â”‚             Setup: 1 hour
â”‚     â”‚  â”‚             Skill: Very Low (just API key)
â”‚     â”‚  â”‚
â”‚     â”‚  â””â”€ 10-50M tokens/month
â”‚     â”‚     â”‚
â”‚     â”‚     â”œâ”€â“ Is workload predictable and consistent?
â”‚     â”‚     â”‚  â”‚
â”‚     â”‚     â”‚  â”œâ”€ YES â†’ âš–ï¸ COMPARE COSTS
â”‚     â”‚     â”‚  â”‚        API: ~$1K-5K/month
â”‚     â”‚     â”‚  â”‚        Self-hosted: ~$2K-4K/month + setup
â”‚     â”‚     â”‚  â”‚        â†’ Self-hosting may be cost-effective
â”‚     â”‚     â”‚  â”‚
â”‚     â”‚     â”‚  â””â”€ NO â†’ âœ… STICK WITH API
â”‚     â”‚     â”‚           Spiky workloads benefit from pay-per-use
â”‚     â”‚     â”‚           Self-hosting = paying for idle capacity
â”‚     â”‚     â”‚
â”‚     â”‚     â””â”€ >50M tokens/month
â”‚     â”‚        â”‚
â”‚     â”‚        â””â”€ âœ… SELF-HOST OR MANAGED OPEN SOURCE
â”‚     â”‚              At this scale, self-hosting almost always cheaper
â”‚     â”‚              Options: vLLM, Together.ai, Replicate
â”‚     â”‚              Break-even typically at 20-50M tokens/day
```

### Hosting Decision Examples by Team Size

#### Startup (2-10 engineers, <$10K/month budget)

**Best choice**: Direct API access (Anthropic/OpenAI)

**Why**:
- Setup time matters more than cost at this stage
- Token volumes are low (<1M/month typically)
- Team lacks ML ops expertise
- Can always migrate later if volumes grow

**Typical cost**: $50-500/month depending on usage

#### Mid-size Company (50-200 engineers, $50K-200K/month budget)

**Best choice**: Cloud Marketplace (AWS Bedrock / Azure OpenAI)

**Why**:
- Already using cloud provider for infrastructure
- VPC integration simplifies networking
- Compliance requirements easier to meet
- Volume discounts available through cloud provider
- SSO integration with existing identity provider

**Typical cost**: $1K-10K/month depending on volume

#### Enterprise (500+ engineers, high security requirements)

**Best choice**: Hybrid approach

**Setup**:
- **Tier 1**: Self-hosted LLaMA 3 for high-volume, low-sensitivity tasks
  - Cost: $10K-30K/month (GPU cluster)
  - Use case: Code completion, documentation, log classification

- **Tier 2**: Azure OpenAI (VPC) for sensitive tasks
  - Cost: $5K-20K/month
  - Use case: Security reviews, compliance analysis, incident response

**Why**:
- Volume justifies self-hosting economics
- Critical tasks still use best-quality models (Claude/GPT-4)
- Compliance satisfied by VPC deployment for sensitive data
- Team has ML ops capacity to manage infrastructure

**Total cost**: $15K-50K/month, but processing 10-100x more requests than mid-size companies

### Common Mistakes to Avoid

#### Mistake 1: Self-Hosting Too Early

**Symptom**: "We're a 5-person startup building our own GPU cluster"

**Problem**:
- Setup cost: 2-3 months of engineering time ($50K-100K)
- Infrastructure: $5K-10K/month for GPUs
- You could process 10M-30M tokens/month on APIs for less

**Fix**: Start with APIs. Self-host only when you hit >20M tokens/day consistently.

#### Mistake 2: Using Wrong Cloud Provider

**Symptom**: "We're on AWS but using Azure OpenAI because it was available first"

**Problem**:
- Cross-cloud data transfer fees ($0.01-0.12 per GB)
- Extra networking complexity (VPN or internet transit)
- Compliance headaches (data crosses provider boundaries)

**Fix**: Use the cloud provider you're already on:
- AWS â†’ Use Bedrock
- Azure â†’ Use Azure OpenAI
- GCP â†’ Use Vertex AI

#### Mistake 3: Not Considering Latency

**Symptom**: "Self-hosted model in US-East, API requests from Asia-Pacific"

**Problem**:
- Added 200-300ms latency per request
- For real-time features (ChatOps bots, autocomplete), this is unacceptable

**Fix**:
- For global applications, use geographically distributed APIs (providers handle this)
- If self-hosting, deploy close to your users or use CDN-style distribution

#### Mistake 4: Ignoring Maintenance Burden

**Symptom**: "Self-hosting is free! We just need GPUs."

**Hidden costs**:
- Model updates (new LLaMA versions every 6 months)
- Security patches for inference frameworks
- GPU driver updates and compatibility issues
- Monitoring, alerting, on-call rotation
- Estimated: 0.5-1 FTE for small deployments, 2-4 FTE for large ones

**Fix**: Factor in 20-40% of GPU costs as engineering overhead.

### Quick Setup Examples

#### Using Anthropic API Directly

```python
# Direct Claude API usage
import anthropic

client = anthropic.Anthropic(api_key="your-api-key")

message = client.messages.create(
    model="claude-sonnet-4-5-20250514",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Write a Kubernetes health check for nginx"}
    ]
)
print(message.content)
```

#### Using AWS Bedrock

```python
# Claude via AWS Bedrock
import boto3
import json

bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')

response = bedrock.invoke_model(
    modelId='anthropic.claude-sonnet-4-5-20250514-v1:0',
    body=json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1024,
        "messages": [
            {"role": "user", "content": "Write a Kubernetes health check"}
        ]
    })
)
```

#### Self-Hosting with Ollama

```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama2
ollama pull codellama

# Run interactively
ollama run codellama "Write a bash script to check disk space"

# Run as API server
ollama serve

# Query the API
curl http://localhost:11434/api/generate -d '{
  "model": "codellama",
  "prompt": "Write a Dockerfile for Python Flask"
}'
```

#### Self-Hosting with vLLM (Production)

```python
# vLLM for production model serving

# Install
# pip install vllm

# Start server
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)

prompts = ["Write a script to monitor CPU usage"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

```bash
# Or run as OpenAI-compatible server
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000

# Then use standard OpenAI client!
```

### Prerequisites Matrix: What You Need Before Self-Hosting

Before running `ollama install` or spinning up a vLLM server, understand what your system actually needs. Self-hosting failures usually happen because teams underestimate requirements.

#### System Requirements by Model Size

| Model Size | RAM Required | GPU VRAM | GPU Type | CPU Cores | Disk Space | Example Models |
|-----------|--------------|----------|----------|-----------|------------|----------------|
| **Tiny (1-3B)** | 8GB | 4GB | GTX 1660+ | 4+ | 10GB | Phi-2, TinyLlama |
| **Small (7B)** | 16GB | 8GB | RTX 3060+ | 8+ | 20GB | LLaMA 2 7B, Mistral 7B |
| **Medium (13B)** | 32GB | 16GB | RTX 4090 / A40 | 16+ | 30GB | LLaMA 2 13B, Vicuna 13B |
| **Large (34B)** | 64GB | 24GB | A100 40GB | 32+ | 80GB | CodeLlama 34B, Yi 34B |
| **XL (70B)** | 128GB+ | 80GB+ | 2Ã— A100 80GB | 64+ | 150GB | LLaMA 2 70B, Falcon 180B |

**Notes**:
- VRAM is for GPU memory (most critical for inference speed)
- RAM is for system memory (critical for model loading)
- Disk space is for model weights + OS + dependencies
- CPU cores matter for preprocessing and batching

#### Platform-Specific Setup Requirements

##### macOS (Apple Silicon M1/M2/M3)

```bash
# Works well for small-medium models thanks to unified memory

System requirements:
  - macOS 12.0+ (Monterey or later)
  - 16GB RAM minimum (32GB recommended for 13B models)
  - 50GB free disk space
  - Xcode Command Line Tools

Install Ollama:
  curl -fsSL https://ollama.ai/install.sh | sh

Supported model sizes:
  - 7B models: Good performance (10-20 tokens/sec)
  - 13B models: Acceptable (5-10 tokens/sec)
  - 34B+ models: Too slow for interactive use (<2 tokens/sec)
```

##### Linux (NVIDIA GPU)

```bash
# Best platform for self-hosting production workloads

System requirements:
  - Ubuntu 20.04+ / Debian 11+ / RHEL 8+
  - NVIDIA GPU with CUDA support (Compute Capability 7.0+)
  - CUDA 11.8+ and cuDNN 8.6+
  - Docker 20.10+ (recommended for isolation)

Prerequisites check:
  # Check NVIDIA driver
  nvidia-smi  # Should show GPU info

  # Check CUDA version
  nvcc --version  # Should be 11.8 or higher

  # Check Docker + NVIDIA runtime
  docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

Common installation issues:
  1. "CUDA not found" â†’ Install nvidia-driver and cuda-toolkit
  2. "Out of memory" â†’ Reduce batch size or use smaller model
  3. "Permission denied /dev/nvidia0" â†’ Add user to 'docker' group
```

##### Windows (WSL2 or Native)

```bash
# Works but more complex than Linux

WSL2 (Recommended):
  - Windows 11 (or Windows 10 21H2+)
  - WSL2 enabled with Ubuntu 22.04
  - NVIDIA GeForce/RTX GPU with latest drivers (545.84+)
  - CUDA support in WSL2 enabled

Native Windows:
  - Windows 11
  - Visual Studio 2022 Build Tools
  - CUDA Toolkit 11.8+
  - Performance typically 10-15% slower than Linux

Recommendation: Use WSL2 for better compatibility with ML tools
```

#### Software Dependencies Checklist

Before self-hosting, ensure you have:

**For Ollama** (Easiest):
```bash
âœ“ Operating system: macOS, Linux, or Windows
âœ“ 16GB+ RAM
âœ“ 20GB+ free disk space
âœ“ Internet connection for first-time model download
âœ— GPU not required (but recommended for speed)
```

**For vLLM** (Production):
```bash
âœ“ Linux with NVIDIA GPU
âœ“ Python 3.9-3.11
âœ“ CUDA 11.8+
âœ“ PyTorch 2.0+ with CUDA support
âœ“ 32GB+ RAM
âœ“ A100/A40/H100 GPU recommended
```

**For Text Generation Inference** (Hugging Face):
```bash
âœ“ Linux or WSL2
âœ“ Docker + NVIDIA Container Toolkit
âœ“ CUDA 11.8+
âœ“ 24GB+ GPU VRAM for 7B models
âœ“ NVIDIA GPU with Tensor Cores (V100+)
```

#### Common Errors and Solutions

##### Error 1: "CUDA out of memory"

**Symptoms**:
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
(GPU 0; 23.99 GiB total capacity; 21.50 GiB already allocated)
```

**Solutions**:
1. **Use a smaller model**: 70B â†’ 13B â†’ 7B
2. **Use quantization**: FP16 â†’ INT8 â†’ INT4 (reduces memory by 2-4x)
3. **Reduce batch size**: Try batch_size=1 for testing
4. **Upgrade GPU**: Move to GPU with more VRAM

**Quick fix**:
```python
# Use 4-bit quantization (75% memory reduction)
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=BitsAndBytesConfig(load_in_4bit=True)
)
```

##### Error 2: "Model download failed"

**Symptoms**:
```
OSError: We couldn't connect to 'https://huggingface.co'
```

**Solutions**:
1. **Check firewall**: Allow outbound HTTPS to huggingface.co
2. **Use mirror**: Set HF_ENDPOINT=https://hf-mirror.com
3. **Pre-download models**: Download on machine with internet, transfer via USB

##### Error 3: "Import error: libcuda.so.1"

**Symptoms**:
```
ImportError: libcuda.so.1: cannot open shared object file
```

**Solution**:
```bash
# Install NVIDIA driver properly
sudo apt update
sudo apt install nvidia-driver-545

# Reboot required!
sudo reboot

# Verify after reboot
nvidia-smi  # Should work now
```

##### Error 4: "Slow inference (>10 seconds per response)"

**Likely causes**:
- Running on CPU instead of GPU
- Using unoptimized inference engine
- Model too large for available VRAM (swapping to RAM)

**Diagnostic**:
```python
import torch
print(torch.cuda.is_available())  # Should be True
print(torch.cuda.get_device_name(0))  # Should show your GPU
```

**Solution**:
- Ensure PyTorch installed with CUDA: `pip install torch --index-url https://download.pytorch.org/whl/cu118`
- Use optimized engine: vLLM is 2-5x faster than vanilla transformers
- Consider smaller model or GPU upgrade

#### Performance Expectations

**Tokens per second** (7B model on different hardware):

| Hardware | Tokens/sec | Use Case |
|----------|-----------|----------|
| MacBook Pro M2 (32GB) | 15-25 | Good for development |
| RTX 3090 (24GB VRAM) | 40-60 | Good for small teams |
| RTX 4090 (24GB VRAM) | 60-80 | Great for production (small scale) |
| A100 (40GB VRAM) | 100-150 | Production standard |
| A100 (80GB VRAM) | 120-180 | High-performance production |
| 2Ã— H100 | 300-400 | Enterprise scale |

**Batch size impact** (requests processed simultaneously):
- Batch size 1: Low throughput, low latency (~100ms)
- Batch size 8: 4-6x throughput, medium latency (~300ms)
- Batch size 32: 10-15x throughput, high latency (~1000ms)

**Rule of thumb**: For interactive use (ChatOps), keep batch size 1-4. For background processing (log analysis), use batch size 16-32.

---

## 4.6 Model Selection Guide for DevOps

### Decision Matrix

```python
# Model recommendation based on use case

def recommend_model(use_case: str, constraints: dict) -> str:
    """
    Returns recommended model based on use case and constraints.
    """

    recommendations = {
        "code_generation": {
            "no_constraints": "Claude Sonnet 4.5",
            "cost_sensitive": "Claude Haiku 4.5",
            "privacy_required": "CodeLlama 34B (self-hosted)",
            "enterprise": "Azure OpenAI GPT-4 or AWS Bedrock Claude",
        },
        "log_analysis": {
            "no_constraints": "Claude Sonnet 4.5 (long context)",
            "high_volume": "Claude Haiku 4.5 (cost effective)",
            "privacy_required": "Mistral 7B (self-hosted)",
            "real_time": "Fine-tuned local classifier",
        },
        "documentation": {
            "no_constraints": "Claude Sonnet 4.5",
            "cost_sensitive": "Claude Haiku 4.5",
            "technical_accuracy": "Claude Opus 4.5",
        },
        "incident_response": {
            "no_constraints": "Claude Opus 4.5 (best reasoning)",
            "fast_response": "Claude Sonnet 4.5",
            "privacy_required": "LLaMA 3 70B (self-hosted)",
        },
        "chatbot": {
            "no_constraints": "Claude Sonnet 4.5",
            "cost_sensitive": "Claude Haiku 4.5",
            "low_latency": "Claude Haiku 4.5",
        },
        "embeddings": {
            "no_constraints": "OpenAI text-embedding-3-large",
            "self_hosted": "all-MiniLM-L6-v2",
            "multilingual": "multilingual-e5-large",
        }
    }

    return recommendations.get(use_case, {}).get(
        constraints.get("priority", "no_constraints"),
        "Claude Sonnet 4.5"  # Safe default
    )
```

### Multi-Dimensional Trade-off Matrix

Model selection isn't about finding the "best" model - it's about finding the right balance of constraints. Here's how popular models stack up across critical dimensions.

#### Comprehensive Model Comparison

| Model | Quality | Cost | Latency | Privacy | Context | Ease of Use |
|-------|---------|------|---------|---------|---------|-------------|
| **Claude Opus 4.5** | â˜…â˜…â˜…â˜…â˜… (Best reasoning) | $$$$$ (Most expensive) | â±ï¸â±ï¸â±ï¸ (3-5s) | âŒ API only | 200K | â˜…â˜…â˜…â˜…â˜… Easy |
| **Claude Sonnet 4.5** | â˜…â˜…â˜…â˜…â˜† (Excellent) | $$$ (Moderate) | â±ï¸â±ï¸ (1-2s) | âŒ API only | 200K | â˜…â˜…â˜…â˜…â˜… Easy |
| **Claude Haiku 4.5** | â˜…â˜…â˜…â˜†â˜† (Good) | $ (Cheap) | â±ï¸ (<1s) | âŒ API only | 200K | â˜…â˜…â˜…â˜…â˜… Easy |
| **GPT-4 Turbo** | â˜…â˜…â˜…â˜…â˜† (Excellent) | $$$$ (Expensive) | â±ï¸â±ï¸ (2-3s) | âŒ API only | 128K | â˜…â˜…â˜…â˜…â˜… Easy |
| **GPT-4o mini** | â˜…â˜…â˜…â˜†â˜† (Good) | $$ (Affordable) | â±ï¸ (<1s) | âŒ API only | 128K | â˜…â˜…â˜…â˜…â˜… Easy |
| **Gemini Ultra** | â˜…â˜…â˜…â˜…â˜† (Excellent) | $$$ (Moderate) | â±ï¸â±ï¸â±ï¸ (3-4s) | âŒ API only | 1M | â˜…â˜…â˜…â˜…â˜† Moderate |
| **LLaMA 3 70B** | â˜…â˜…â˜…â˜…â˜† (V. Good) | $$$$ (GPU infra) | â±ï¸â±ï¸ (2-4s) | âœ… Self-hosted | 8K | â˜…â˜…â˜†â˜†â˜† Hard |
| **LLaMA 3 7B** | â˜…â˜…â˜…â˜†â˜† (Decent) | $$ (Small GPU) | â±ï¸â±ï¸ (1-2s) | âœ… Self-hosted | 8K | â˜…â˜…â˜…â˜†â˜† Moderate |
| **Mistral 7B** | â˜…â˜…â˜…â˜†â˜† (Good) | $ (Tiny GPU) | â±ï¸ (0.5-1s) | âœ… Self-hosted | 32K | â˜…â˜…â˜…â˜…â˜† Moderate |
| **CodeLlama 34B** | â˜…â˜…â˜…â˜…â˜† (Code tasks) | $$$ (Med GPU) | â±ï¸â±ï¸ (2-3s) | âœ… Self-hosted | 16K | â˜…â˜…â˜†â˜†â˜† Hard |

**Legend**:
- Quality: Reasoning ability, accuracy, safety
- Cost: $ = <$50/month, $$ = $50-500, $$$ = $500-5K, $$$$ = $5K-20K, $$$$$ = >$20K
- Latency: â±ï¸ = <1s, â±ï¸â±ï¸ = 1-3s, â±ï¸â±ï¸â±ï¸ = >3s (for typical DevOps query)
- Privacy: âŒ = Data leaves your infrastructure, âœ… = Stays on your servers
- Context: Maximum input tokens supported

#### Balancing Competing Constraints

Most real-world scenarios require trade-offs. Here's how to think about them:

##### Constraint 1: Quality vs Cost

**The Trade-off**:
```
High Quality (Claude Opus 4.5):
  âœ“ Best reasoning for complex tasks
  âœ“ Fewer errors = less debugging time
  âœ— 5-10x more expensive than alternatives

Medium Quality (Claude Sonnet 4.5, GPT-4):
  âœ“ 90% of Opus quality at 30% of cost
  âœ“ Good enough for most production tasks
  âœ“ Sweet spot for most teams

Budget Option (Claude Haiku, Mistral 7B):
  âœ“ 10-20x cheaper than Opus
  âœ“ Acceptable for simple, high-volume tasks
  âœ— Will miss edge cases and nuances
```

**When to pay for quality**:
- Security code reviews (missed vulnerability = $$$)
- Incident response (wrong diagnosis = downtime)
- Production IaC generation (errors = outages)
- Compliance documentation (mistakes = regulatory risk)

**When budget options work**:
- Log categorization (errors are non-critical)
- Documentation generation (human reviews anyway)
- ChatOps simple queries ("What's the status of service X?")
- High-volume, repetitive tasks

##### Constraint 2: Latency vs Quality

**The Trade-off**:
```
Fast (<1s):
  Models: Claude Haiku, GPT-4o mini, Mistral 7B
  Use cases: Real-time ChatOps, autocomplete, quick triage
  Trade-off: Simpler reasoning, may miss nuances

Medium (1-3s):
  Models: Claude Sonnet, GPT-4 Turbo, LLaMA 3 70B
  Use cases: Code review, log analysis, most DevOps tasks
  Trade-off: Good balance for asynchronous workflows

Slow (>3s):
  Models: Claude Opus, complex prompts with long context
  Use cases: Deep incident analysis, security audits
  Trade-off: Best quality but requires async processing
```

**Decision rule**: If humans wait for the response â†’ use fast models. If it's background processing â†’ optimize for quality.

##### Constraint 3: Privacy vs Convenience

**The Trade-off**:
```
API Models (Claude, GPT-4):
  âœ“ Zero setup time
  âœ“ Always latest version
  âœ“ Managed scaling
  âœ— Data leaves your network
  âœ— Vendor dependency

VPC Deployment (Bedrock, Azure OpenAI):
  âœ“ Data stays in your cloud account
  âœ“ Network isolation possible
  âœ“ Compliance-friendly
  âœ— Slight cost markup (5-15%)
  âœ— Limited model selection

Self-Hosted (LLaMA, Mistral):
  âœ“ Complete data control
  âœ“ No per-token costs
  âœ“ Customization possible
  âœ— Significant setup and maintenance burden
  âœ— Need GPU infrastructure and expertise
```

**Decision framework**:
1. **Start with**: Can data leave your network? (Most teams: yes)
2. **If no data export**: Can it stay in your cloud provider? (Use Bedrock/Azure)
3. **If must be on-premise**: Do you have ML ops capacity? (If no, reconsider requirements)

##### Constraint 4: Context Window vs Cost

**The Trade-off**:
```
Long Context (200K tokens):
  Models: Claude family (200K), Gemini Ultra (1M)
  Use case: Analyzing entire codebases, long logs
  Cost impact: More tokens = higher cost
  Latency: Longer processing time (3-5s+)

Medium Context (32-128K tokens):
  Models: GPT-4 Turbo (128K), Mistral (32K)
  Use case: Most DevOps tasks (single file review, incident analysis)
  Cost impact: Moderate
  Latency: Fast enough (1-3s)

Short Context (8K tokens):
  Models: LLaMA 2 (8K), older models
  Use case: Simple queries, short documents
  Cost impact: Cheapest
  Latency: Fastest (<1s)
```

**Optimization tip**: Don't send unnecessary context. Filter logs, summarize before sending, use RAG for retrieval instead of dumping entire docs.

#### Pareto Frontier: Efficient Choices

The "efficient frontier" are models where improving one dimension requires sacrificing another. These are your best options:

**For Production DevOps Work**:
1. **Claude Sonnet 4.5**: Best quality-to-cost ratio for most tasks
2. **Claude Haiku 4.5**: Best for high-volume, latency-sensitive work
3. **LLaMA 3 70B** (self-hosted): Best for extreme volume with privacy needs

**Dominated options** (usually worse trade-offs):
- GPT-4 Turbo: More expensive than Claude Sonnet, similar quality
- GPT-4o mini: Claude Haiku is faster and often better quality
- Small self-hosted models (<7B): API options usually better unless extreme privacy needs

#### Real-World Trade-off Example: Code Review Bot

**Scenario**: Automated PR reviews for 100 PRs/day, average 2000 tokens input, 500 tokens output.

**Option 1: Claude Opus 4.5** (Maximize Quality)
- Monthly cost: ~$33.75 (from earlier calculation)
- Quality: Finds 98% of issues
- Latency: 3-4s per review
- **Verdict**: Overkill for most teams. Use for security-critical projects only.

**Option 2: Claude Sonnet 4.5** (Balanced)
- Monthly cost: ~$20.25
- Quality: Finds 95% of issues
- Latency: 1-2s per review
- **Verdict**: Best choice for most teams. Great ROI.

**Option 3: Claude Haiku 4.5** (Cost-Optimized)
- Monthly cost: ~$6.75
- Quality: Finds 85% of issues
- Latency: <1s per review
- **Verdict**: Good for non-critical repos or augmenting human reviews.

**Option 4: Self-hosted CodeLlama 34B**
- Monthly cost: ~$2000 (GPU) + engineering time
- Quality: Finds 70-80% of issues
- Latency: 2-3s per review
- **Verdict**: Only makes sense at 1000+ PRs/day or extreme privacy needs.

**The winner**: Claude Sonnet 4.5. The $14/month savings from Haiku isn't worth the 10% quality drop for code reviews.

### Cost Comparison Calculator

```python
# Compare costs between different model options

def calculate_monthly_cost(
    input_tokens_per_request: int,
    output_tokens_per_request: int,
    requests_per_day: int,
    model: str
) -> dict:
    """
    Calculate monthly API costs for different models.
    """

    # Prices per 1M tokens (approximate, check current pricing - 2025)
    pricing = {
        "claude-opus-4.5": {"input": 5.0, "output": 25.0},
        "claude-sonnet-4.5": {"input": 3.0, "output": 15.0},
        "claude-haiku-4.5": {"input": 1.0, "output": 5.0},
        "gpt-4-turbo": {"input": 10.0, "output": 30.0},
    }

    if model not in pricing:
        return {"error": f"Unknown model: {model}"}

    prices = pricing[model]
    monthly_requests = requests_per_day * 30

    monthly_input_tokens = input_tokens_per_request * monthly_requests
    monthly_output_tokens = output_tokens_per_request * monthly_requests

    input_cost = (monthly_input_tokens / 1_000_000) * prices["input"]
    output_cost = (monthly_output_tokens / 1_000_000) * prices["output"]

    return {
        "model": model,
        "monthly_requests": monthly_requests,
        "total_input_tokens": monthly_input_tokens,
        "total_output_tokens": monthly_output_tokens,
        "input_cost": round(input_cost, 2),
        "output_cost": round(output_cost, 2),
        "total_monthly_cost": round(input_cost + output_cost, 2),
    }


# Example: Code review bot
# Average PR: 2000 input tokens, 500 output tokens
# 50 PRs per day

for model in ["claude-opus-4.5", "claude-sonnet-4.5", "claude-haiku-4.5", "gpt-4-turbo"]:
    result = calculate_monthly_cost(2000, 500, 50, model)
    print(f"{model}: ${result['total_monthly_cost']}/month")

# Output:
# claude-opus-4.5: $33.75/month
# claude-sonnet-4.5: $20.25/month
# claude-haiku-4.5: $6.75/month
# gpt-4-turbo: $75.00/month
```

---

## 4.7 Emerging Trends

### What's Coming in AI Models

```yaml
# Trends to Watch (2024-2025)

1_multimodal_by_default:
  description: "Models that understand text, images, audio, video together"
  examples:
    - "Upload a screenshot of an error, get debugging help"
    - "Show architecture diagram, get Terraform code"
  current_leaders: ["GPT-4V", "Claude 4.5 Vision", "Gemini"]

2_longer_context_windows:
  description: "Processing entire codebases at once"
  trend: "4K â†’ 32K â†’ 128K â†’ 200K â†’ 1M+ tokens"
  impact_on_devops:
    - "Analyze all logs from an incident at once"
    - "Review entire microservice in one prompt"
    - "Generate documentation for whole project"

3_specialized_agents:
  description: "AI that can take actions, not just generate text"
  examples:
    - "AI that can run kubectl commands"
    - "AI that can create PRs and deploy"
  early_examples: ["Claude Code", "GitHub Copilot Workspace", "Devin"]

4_local_models_improving:
  description: "Smaller, faster models running on personal devices"
  examples:
    - "7B models matching GPT-3.5 quality"
    - "Models running on M1/M2 Macs"
  tools: ["Ollama", "LM Studio", "llama.cpp"]

5_fine_tuning_democratization:
  description: "Easier to create specialized models"
  approaches:
    - "LoRA/QLoRA for efficient fine-tuning"
    - "Few-shot learning from examples"
    - "Retrieval-augmented generation (RAG)"

6_real_time_capabilities:
  description: "Lower latency, streaming responses"
  impact:
    - "Interactive debugging sessions"
    - "Real-time log analysis"
    - "Instant code suggestions"
```

---

## 4.8 Hands-On Exercises

### Exercise 1: Model Comparison

```markdown
## Hands-On: Compare Models

Task: Test the same prompt on different models

### Prompt to Test:
"Write a bash script that:
1. Checks if Docker is running
2. Lists all containers with their CPU/memory usage
3. Alerts if any container uses more than 80% memory
4. Outputs results in JSON format"

### Test on:
1. Claude Sonnet 4.5 (via claude.ai)
2. GPT-4 (via chat.openai.com)
3. CodeLlama (via Ollama locally)

### Evaluation Criteria:
| Criterion           | Claude | GPT-4 | CodeLlama |
|--------------------|---------| ------|-----------|
| Correctness        |         |       |           |
| Code Quality       |         |       |           |
| Edge Case Handling |         |       |           |
| Explanation        |         |       |           |
| Response Time      |         |       |           |

### Your Observations:
[Document differences you noticed]
```

### Exercise 2: Cost Calculation

```markdown
## Hands-On: Calculate Your AI Costs

Scenario: You want to implement an AI-powered log analyzer

### Parameters:
- Average log batch size: _____ characters
- Estimated tokens (chars Ã· 4): _____
- Batches per day: _____
- Expected output tokens per analysis: _____

### Calculate for each model:

| Model | Input Cost/1M | Output Cost/1M | Monthly Cost |
|-------|---------------|----------------|--------------|
| Claude Sonnet 4.5 | $3.00 | $15.00 | $_____ |
| Claude Haiku 4.5 | $1.00 | $5.00 | $_____ |
| GPT-4 Turbo | $10.00 | $30.00 | $_____ |
| Self-hosted | GPU: $___/hr | N/A | $_____ |

### Break-Even Analysis:
At what volume does self-hosting become cheaper?
[Your calculation here]
```

### Exercise 3: Local Model Setup

```bash
# Exercise: Set up Ollama and test CodeLlama

# Step 1: Install Ollama
# Follow instructions at https://ollama.ai

# Step 2: Pull CodeLlama
ollama pull codellama

# Step 3: Test with DevOps prompts
ollama run codellama "Write a Kubernetes CronJob that runs a backup script daily at 2am"

# Step 4: Compare with Claude (if you have API access)
# Note the differences in:
# - Response quality
# - Response time
# - Handling of edge cases

# Document your findings:
# Local CodeLlama:
# - Quality: ____/10
# - Speed: ____ seconds
# - Notes: ____

# Claude API:
# - Quality: ____/10
# - Speed: ____ seconds
# - Notes: ____
```

---

## 4.9 Chapter Summary

### Key Takeaways

1. **Major providers**: Anthropic (Claude), OpenAI (GPT), Google (Gemini), Meta (LLaMA)

2. **Proprietary vs Open Source**: Trade-off between convenience/quality and control/cost

3. **Model selection depends on**: Task complexity, privacy needs, budget, volume, latency requirements

4. **Multiple hosting options**: Direct API, cloud marketplaces, self-hosted, managed open source

5. **The landscape is evolving rapidly**: Stay informed about new models and capabilities

### Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODEL SELECTION QUICK REFERENCE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  NEED BEST QUALITY?                                            â”‚
â”‚  â†’ Claude Opus 4.5, GPT-4                                      â”‚
â”‚                                                                â”‚
â”‚  NEED BALANCE OF QUALITY & COST?                               â”‚
â”‚  â†’ Claude Sonnet 4.5, GPT-4 Turbo                              â”‚
â”‚                                                                â”‚
â”‚  NEED LOW COST?                                                â”‚
â”‚  â†’ Claude Haiku 4.5                                            â”‚
â”‚                                                                â”‚
â”‚  NEED PRIVACY/LOCAL?                                           â”‚
â”‚  â†’ LLaMA 3, Mistral, CodeLlama (via Ollama)                    â”‚
â”‚                                                                â”‚
â”‚  NEED ENTERPRISE COMPLIANCE?                                   â”‚
â”‚  â†’ AWS Bedrock, Azure OpenAI, Google Vertex AI                 â”‚
â”‚                                                                â”‚
â”‚  NEED CODE SPECIFICALLY?                                       â”‚
â”‚  â†’ Claude Sonnet 4.5, CodeLlama, StarCoder                     â”‚
â”‚                                                                â”‚
â”‚  NEED LONG CONTEXT?                                            â”‚
â”‚  â†’ Claude 4.5 (200K), Gemini (1M), GPT-4 Turbo (128K)          â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

[â† Previous: The Art of Prompting](./03-the-art-of-prompting.md) | [Next: Introduction to Claude â†’](./05-introduction-to-claude.md)

---

**Part of**: AI and Claude Code - A Comprehensive Guide for DevOps Engineers  
**Created by**: Michel Abboud with Claude Sonnet 4.5 (Anthropic)  
**Copyright**: Â© 2026 Michel Abboud. All rights reserved.  
**License**: CC BY-NC 4.0

---

## Navigation

â† Previous: [Chapter 3: The Art of Prompting](./03-the-art-of-prompting.md) | Next: [Chapter 5: Introduction to Claude](./05-introduction-to-claude.md) â†’

**Quick Nav:** [README](../README.md) | [Table of Contents](../README.md#table-of-contents)

---

**Chapter 4** | AI Models Landscape | Â© 2026 Michel Abboud | [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)
