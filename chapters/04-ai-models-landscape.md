# Chapter 4: AI Models Landscape

**Part 2: The AI Ecosystem**

---

## Navigation

â† Previous: [Chapter 3: The Art of Prompting](./03-the-art-of-prompting.md) | Next: [Chapter 5: Introduction to Claude](./05-introduction-to-claude.md) â†’

**Quick Nav:** [README](../README.md) | [Table of Contents](../README.md#table-of-contents)

---


## Navigating the World of AI Models and Providers

**ğŸ“– Reading time:** ~14 minutes
**ğŸ¯ Quick nav:** [What are Models?](#41-understanding-models) | [Major Providers](#42-major-ai-companies-and-their-models) | [Proprietary vs Open Source](#43-proprietary-vs-open-source-models) | [Model Selection](#46-model-selection-guide-for-devops)

---

Understanding the AI ecosystem helps you make informed decisions about which tools to use for different tasks. This chapter covers the major players, model types, and how to choose the right model for your needs.

---

## 4.1 Understanding "Models"

### What is a Model?

A **model** is a trained AI system that can perform specific tasks. Think of it as a specialized program that has learned from data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   THE MODEL LIFECYCLE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  1. ARCHITECTURE DESIGN                                        â”‚
â”‚     "How should the neural network be structured?"             â”‚
â”‚     â””â”€â–º Transformer, CNN, RNN, etc.                            â”‚
â”‚                                                                â”‚
â”‚  2. PRE-TRAINING                                               â”‚
â”‚     "Learn general knowledge from massive data"                â”‚
â”‚     â””â”€â–º Billions of tokens of text, code, etc.                 â”‚
â”‚         â””â”€â–º Months of training on GPU clusters                 â”‚
â”‚                                                                â”‚
â”‚  3. FINE-TUNING                                                â”‚
â”‚     "Specialize for specific tasks"                            â”‚
â”‚     â””â”€â–º Instruction tuning, RLHF, domain adaptation            â”‚
â”‚                                                                â”‚
â”‚  4. DEPLOYMENT                                                 â”‚
â”‚     "Make it available for use"                                â”‚
â”‚     â””â”€â–º API endpoints, downloadable weights, etc.              â”‚
â”‚                                                                â”‚
â”‚  5. INFERENCE                                                  â”‚
â”‚     "Using the model to get predictions"                       â”‚
â”‚     â””â”€â–º What you do when you chat with an AI                   â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Naming Conventions

```
Understanding model names:

Claude Sonnet 4.5
â”‚      â”‚     â”‚
â”‚      â”‚     â””â”€â”€ Version (major iteration)
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€ Variant (capability tier within the family)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Family name (product line)

GPT-4-Turbo-128K
â”‚   â”‚ â”‚     â”‚
â”‚   â”‚ â”‚     â””â”€â”€ Context window size
â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€ Variant (optimized version)
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Version
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Family name

LLaMA-2-70B-Chat
â”‚     â”‚ â”‚   â”‚
â”‚     â”‚ â”‚   â””â”€â”€ Specialization (chat-optimized)
â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€ Parameter count (70 billion)
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€ Version
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Family name
```

---

## 4.2 Major AI Companies and Their Models

### The Big Players

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MAJOR AI COMPANIES (2024)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ANTHROPIC                                              â”‚    â”‚
â”‚  â”‚  "AI Safety First"                                      â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Founded: 2021 by ex-OpenAI researchers                 â”‚    â”‚
â”‚  â”‚  Focus: Safe, helpful, honest AI                        â”‚    â”‚
â”‚  â”‚  Models: Claude family                                  â”‚    â”‚
â”‚  â”‚  Notable: Constitutional AI, long context windows       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  OPENAI                                                 â”‚    â”‚
â”‚  â”‚  "AGI for the benefit of humanity"                      â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Founded: 2015                                          â”‚    â”‚
â”‚  â”‚  Focus: General AI advancement                          â”‚    â”‚
â”‚  â”‚  Models: GPT family, DALL-E, Whisper, Codex             â”‚    â”‚
â”‚  â”‚  Notable: ChatGPT, API platform, Microsoft partnership  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  GOOGLE (DeepMind + Google AI)                          â”‚    â”‚
â”‚  â”‚  "Organizing the world's information"                   â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Focus: Research + product integration                  â”‚    â”‚
â”‚  â”‚  Models: Gemini, PaLM, Bard                             â”‚    â”‚
â”‚  â”‚  Notable: AlphaFold, integrated into Google products    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  META (Facebook AI Research)                            â”‚    â”‚
â”‚  â”‚  "Open AI research"                                     â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Focus: Open-source AI                                  â”‚    â”‚
â”‚  â”‚  Models: LLaMA family (open source!)                    â”‚    â”‚
â”‚  â”‚  Notable: Democratizing AI access                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  MISTRAL AI                                             â”‚    â”‚
â”‚  â”‚  "Open and efficient AI"                                â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Founded: 2023 in France                                â”‚    â”‚
â”‚  â”‚  Focus: Efficient, open models                          â”‚    â”‚
â”‚  â”‚  Models: Mistral, Mixtral (MoE)                         â”‚    â”‚
â”‚  â”‚  Notable: High performance at smaller sizes             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Model Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MODEL COMPARISON MATRIX                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model         â”‚ Provider     â”‚ Context      â”‚ Best For     â”‚ API Access       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude Opus   â”‚ Anthropic    â”‚ 200K tokens  â”‚ Complex      â”‚ API + Console    â”‚
â”‚ 4.5           â”‚              â”‚              â”‚ reasoning    â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude Sonnet â”‚ Anthropic    â”‚ 200K tokens  â”‚ Balanced     â”‚ API + Console    â”‚
â”‚ 4.5           â”‚              â”‚              â”‚ performance  â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude Haiku  â”‚ Anthropic    â”‚ 200K tokens  â”‚ Speed +      â”‚ API + Console    â”‚
â”‚ 4.5           â”‚              â”‚              â”‚ Cost         â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4 Turbo   â”‚ OpenAI       â”‚ 128K tokens  â”‚ General      â”‚ API + ChatGPT    â”‚
â”‚               â”‚              â”‚              â”‚ purpose      â”‚ Plus             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4o        â”‚ OpenAI       â”‚ 128K tokens  â”‚ Multimodal   â”‚ API + ChatGPT    â”‚
â”‚               â”‚              â”‚              â”‚ tasks        â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemini Ultra  â”‚ Google       â”‚ 1M tokens    â”‚ Long context â”‚ API + Gemini     â”‚
â”‚               â”‚              â”‚              â”‚ Multimodal   â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemini Pro    â”‚ Google       â”‚ 32K tokens   â”‚ General      â”‚ API + Vertex AI  â”‚
â”‚               â”‚              â”‚              â”‚ purpose      â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLaMA 3 70B   â”‚ Meta         â”‚ 8K tokens    â”‚ Self-hosting â”‚ Open weights     â”‚
â”‚               â”‚              â”‚              â”‚ Privacy      â”‚ Free!            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mixtral 8x7B  â”‚ Mistral      â”‚ 32K tokens   â”‚ Efficiency   â”‚ Open weights     â”‚
â”‚               â”‚              â”‚              â”‚ Self-hosting â”‚ + API            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CodeLlama     â”‚ Meta         â”‚ 16K tokens   â”‚ Code tasks   â”‚ Open weights     â”‚
â”‚               â”‚              â”‚              â”‚              â”‚ Free!            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.3 Proprietary vs Open Source Models

### Proprietary Models (Closed Source)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROPRIETARY MODELS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Examples: Claude, GPT-4, Gemini                               â”‚
â”‚                                                                â”‚
â”‚  PROS:                                                         â”‚
â”‚  âœ“ State-of-the-art performance                                â”‚
â”‚  âœ“ No infrastructure to manage                                 â”‚
â”‚  âœ“ Regular updates and improvements                            â”‚
â”‚  âœ“ Better safety and alignment                                 â”‚
â”‚  âœ“ Enterprise support available                                â”‚
â”‚  âœ“ Easy to start (just an API key)                             â”‚
â”‚                                                                â”‚
â”‚  CONS:                                                         â”‚
â”‚  âœ— Ongoing costs (pay per token)                               â”‚
â”‚  âœ— Data leaves your infrastructure                             â”‚
â”‚  âœ— Vendor lock-in risk                                         â”‚
â”‚  âœ— Rate limits and quotas                                      â”‚
â”‚  âœ— Can't customize model weights                               â”‚
â”‚  âœ— Dependent on provider's policies                            â”‚
â”‚                                                                â”‚
â”‚  BEST FOR:                                                     â”‚
â”‚  â€¢ Most production use cases                                   â”‚
â”‚  â€¢ When you need the best quality                              â”‚
â”‚  â€¢ When you don't want to manage infra                         â”‚
â”‚  â€¢ Rapid prototyping                                           â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Open Source Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPEN SOURCE MODELS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Examples: LLaMA 2, Mistral, Falcon, MPT                       â”‚
â”‚                                                                â”‚
â”‚  PROS:                                                         â”‚
â”‚  âœ“ Free to use (no per-token cost)                             â”‚
â”‚  âœ“ Data stays on your infrastructure                           â”‚
â”‚  âœ“ Full control and customization                              â”‚
â”‚  âœ“ Can fine-tune for specific tasks                            â”‚
â”‚  âœ“ No vendor lock-in                                           â”‚
â”‚  âœ“ Community support and improvements                          â”‚
â”‚                                                                â”‚
â”‚  CONS:                                                         â”‚
â”‚  âœ— Requires GPU infrastructure                                 â”‚
â”‚  âœ— Generally lower performance than proprietary                â”‚
â”‚  âœ— You manage updates and security                             â”‚
â”‚  âœ— Need ML expertise to deploy                                 â”‚
â”‚  âœ— Higher upfront investment                                   â”‚
â”‚  âœ— Smaller context windows typically                           â”‚
â”‚                                                                â”‚
â”‚  BEST FOR:                                                     â”‚
â”‚  â€¢ Strict data privacy requirements                            â”‚
â”‚  â€¢ High-volume, predictable workloads                          â”‚
â”‚  â€¢ Custom/specialized use cases                                â”‚
â”‚  â€¢ When you have ML engineering capacity                       â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Framework

```python
# Model Selection Decision Tree

def select_model_type(requirements):
    """
    Help decide between proprietary and open source models.
    """

    # Priority 1: Data Privacy
    if requirements.data_must_stay_on_premise:
        if requirements.can_run_gpu_infrastructure:
            return "Open Source (LLaMA 2, Mistral)"
        else:
            return "Private Cloud Deployment (Azure OpenAI, AWS Bedrock)"

    # Priority 2: Performance Requirements
    if requirements.needs_state_of_the_art:
        return "Proprietary (Claude Opus 4.5, GPT-4)"

    # Priority 3: Cost Optimization
    if requirements.high_volume and requirements.predictable_workload:
        estimated_api_cost = requirements.tokens_per_month * PRICE_PER_TOKEN
        estimated_gpu_cost = GPU_HOURLY_RATE * 24 * 30

        if estimated_gpu_cost < estimated_api_cost * 0.5:
            return "Open Source (cost-effective)"
        else:
            return "Proprietary (simpler, similar cost)"

    # Priority 4: Speed to Market
    if requirements.need_to_ship_fast:
        return "Proprietary (fastest to implement)"

    # Default
    return "Proprietary (safest choice for most cases)"
```

###Decision Tree Rationale: Real-World Scenarios

The decision tree above provides a framework, but let's see how it applies to concrete DevOps scenarios with actual costs and trade-offs.

#### Scenario 1: Startup with Limited Budget (<$500/month AI budget)

**Context**: Early-stage SaaS company, 50 customer support tickets/day, need AI-assisted ticket triage and response generation.

**Analysis**:
- Input: ~500 tokens/ticket (customer message + context)
- Output: ~300 tokens/ticket (suggested response)
- Volume: 50 tickets Ã— 30 days = 1,500 requests/month
- Total tokens: (500 + 300) Ã— 1,500 = 1.2M tokens/month

**Cost Comparison**:
```
Claude Sonnet 4.5:
  Input:  (750K / 1M) Ã— $3  = $2.25
  Output: (450K / 1M) Ã— $15 = $6.75
  Total: $9/month âœ“ Well within budget

Claude Haiku 4.5 (even cheaper):
  Input:  (750K / 1M) Ã— $1  = $0.75
  Output: (450K / 1M) Ã— $5  = $2.25
  Total: $3/month âœ“ Extremely cost-effective

Self-hosted LLaMA 2 7B:
  GPU instance: ~$200-$400/month (AWS g5.xlarge)
  Setup time: 40 hours engineering @ $150/hr = $6,000
  Total first month: $6,200+ âœ— Overkill for this scale
```

**Decision**: Use **Claude Haiku 4.5**. At $3/month, self-hosting makes zero financial sense. You'd need to process >100K tickets/month before GPU costs break even.

#### Scenario 2: Enterprise with Compliance Requirements (Financial Services)

**Context**: Bank needs to analyze loan application documents (PDF parsing + decision recommendation). Data cannot leave private network due to regulatory requirements.

**Analysis**:
- Volume: 200 loan applications/day
- Data sensitivity: Extremely high (PII, financial data)
- Compliance: SOC 2, PCI-DSS, data residency requirements

**Cost Comparison**:
```
Option 1: AWS Bedrock (Claude in VPC)
  - Cost: ~$500-$1000/month depending on volume
  - Pros: Stays in AWS VPC, meets compliance
  - Cons: Still sends data to AWS (some teams uncomfortable)
  - Setup time: 2-3 days

Option 2: Azure OpenAI (Private deployment)
  - Cost: Similar to Bedrock
  - Pros: Can deploy in private VNET
  - Cons: Limited model selection vs Anthropic

Option 3: Self-hosted LLaMA 2 70B
  - GPU cost: ~$2,000/month (8Ã— A100 or equivalent)
  - Engineering: 160 hours setup + 40 hours/month maintenance
  - Initial cost: $26,000 first month
  - Ongoing: $8,000/month (GPU + eng)
  - Pros: Complete control, data never leaves network
  - Cons: Significantly higher cost, lower quality than Claude
```

**Decision**: Use **AWS Bedrock with Claude Sonnet 4.5** in a VPC. While self-hosting provides maximum control, the 8-10x cost difference doesn't justify it unless data cannot even touch AWS infrastructure (rare for banks already using AWS). The "data must stay on-premise" requirement is usually satisfied by VPC deployment.

#### Scenario 3: High-Volume Log Analysis (500GB logs/day)

**Context**: SaaS platform with 10K customers generating massive log volumes. Need to auto-categorize errors and extract root causes.

**Analysis**:
- Volume: 500GB logs/day = ~125 billion tokens/day
- Processing: Cannot send all logs to LLM (cost prohibitive)
- Solution: Two-tier approach

**Hybrid Architecture**:
```
Tier 1: Local classifier (self-hosted)
  - Use fine-tuned DistilBERT (50MB model)
  - Runs on CPU, filters 95% of logs
  - Cost: ~$100/month for inference servers
  - Categorizes: ERROR, WARN, INFO

Tier 2: LLM analysis (Claude)
  - Only send ERROR logs (5% of volume)
  - 5% of 125B tokens = 6.25B tokens/day
  - Monthly: 187B tokens

Claude Sonnet 4.5 cost:
  - Input:  (94B / 1M) Ã— $3  = $282,000/month âœ— Prohibitive!
  - Output: (94B / 1M) Ã— $15 = $1.41M/month  âœ— No way!

Revised approach with caching:
  - Cache common error patterns (80% hit rate)
  - Actual LLM calls: 20% of 6.25B = 1.25B tokens/day
  - Monthly: ~37B tokens
  - Cost: (18.5B/1M Ã— $3) + (18.5B/1M Ã— $15) = $55K + $278K = $333K/month

Self-hosted LLaMA 3 70B option:
  - Infrastructure: $10,000/month (GPU cluster)
  - Engineering: $25,000/month (3 ML engineers)
  - Total: $35,000/month âœ“ Significantly cheaper at this scale
```

**Decision**: **Self-host LLaMA 3 70B**. At this volume, the break-even point is reached. Self-hosting costs $35K/month vs $333K/month for Claude (even with aggressive caching). The quality difference is acceptable for log categorization.

**Key insight**: The break-even point for self-hosting typically occurs around:
- **>20M tokens/day** if you already have GPU infrastructure
- **>50M tokens/day** if you need to build infrastructure from scratch

#### Scenario 4: Prototyping a New AI Feature (Startup MVP)

**Context**: Testing whether AI-powered code review adds value before committing to infrastructure.

**Analysis**:
- Timeline: Need proof-of-concept in 2 weeks
- Volume: Unknown (testing phase)
- Team: 2 developers, no ML expertise

**Decision Matrix**:
```
Self-hosted approach:
  - Setup time: 2-3 weeks (learning curve)
  - Infrastructure: 1 week to provision GPUs
  - Integration: 1 week
  - Total: 4-5 weeks âœ— Misses deadline

Proprietary API approach:
  - Setup time: 1 hour (API key + SDK install)
  - Integration: 3-4 days
  - Total: 4-5 days âœ“ Meets deadline
```

**Decision**: **Use Claude Sonnet 4.5 API**. Speed-to-market trumps all other considerations. You can always migrate to self-hosting later if volume justifies it. The cost of delaying market validation is far higher than API fees during testing.

**Practical tip**: Many teams build with proprietary APIs, then switch to self-hosted models once they hit predictable high volume. This "API-first, self-host-later" approach minimizes risk.

### When to Reconsider Your Choice

Your model selection isn't permanent. Reevaluate when:

- **Volume increases 10x**: Self-hosting becomes cost-effective
- **Compliance requirements change**: May need to move to private deployment
- **New models release**: GPT-5, Claude 5, etc. may shift the landscape
- **Budget changes**: Startup series A funding might enable self-hosting
- **Performance issues**: If API latency becomes a bottleneck

### Popular Open Source Models for DevOps

```yaml
# Open Source Models Worth Knowing

code_generation:
  name: "CodeLlama"
  sizes: [7B, 13B, 34B]
  context: 16K tokens
  strengths:
    - Code completion
    - Code explanation
    - Bug fixing
  run_locally: "Yes, 7B fits on 16GB GPU"
  example: |
    # Can run locally with llama.cpp
    ./main -m codellama-7b.gguf -p "Write a bash script to..."

general_purpose:
  name: "LLaMA 2"
  sizes: [7B, 13B, 70B]
  context: 4K tokens
  strengths:
    - General reasoning
    - Conversation
    - Following instructions
  run_locally: "7B/13B on consumer GPUs, 70B needs A100"

efficient_reasoning:
  name: "Mixtral 8x7B"
  architecture: "Mixture of Experts (MoE)"
  context: 32K tokens
  strengths:
    - Efficient inference
    - Strong reasoning
    - Multilingual
  special: "Only uses 2 experts at a time (faster than 47B model)"

embedding_models:
  - name: "all-MiniLM-L6-v2"
    use_case: "Semantic search"
    provider: "Sentence Transformers"
  - name: "text-embedding-ada-002"
    use_case: "High quality embeddings"
    provider: "OpenAI"
```

---

## 4.4 Types of Models for Different Tasks

### Model Taxonomy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MODEL TYPES BY TASK                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  TEXT GENERATION (LLMs)                                                    â”‚
â”‚  â”œâ”€â”€ General purpose: Claude, GPT-4, Gemini                                â”‚
â”‚  â”œâ”€â”€ Code specialized: CodeLlama, StarCoder, Codex                         â”‚
â”‚  â””â”€â”€ Instruction-tuned: Vicuna, Alpaca, WizardLM                           â”‚
â”‚                                                                            â”‚
â”‚  TEXT UNDERSTANDING                                                        â”‚
â”‚  â”œâ”€â”€ Classification: BERT, RoBERTa, DistilBERT                             â”‚
â”‚  â”œâ”€â”€ Named Entity Recognition: SpaCy models, Flair                         â”‚
â”‚  â””â”€â”€ Sentiment Analysis: Specialized BERT variants                         â”‚
â”‚                                                                            â”‚
â”‚  EMBEDDINGS (Vector Representations)                                       â”‚
â”‚  â”œâ”€â”€ Text: OpenAI Ada, Sentence-BERT, E5                                   â”‚
â”‚  â”œâ”€â”€ Code: CodeBERT, UniXcoder                                             â”‚
â”‚  â””â”€â”€ Multi-modal: CLIP (text + images)                                     â”‚
â”‚                                                                            â”‚
â”‚  IMAGE GENERATION                                                          â”‚
â”‚  â”œâ”€â”€ DALL-E 3 (OpenAI)                                                     â”‚
â”‚  â”œâ”€â”€ Midjourney                                                            â”‚
â”‚  â”œâ”€â”€ Stable Diffusion (open source!)                                       â”‚
â”‚  â””â”€â”€ Imagen (Google)                                                       â”‚
â”‚                                                                            â”‚
â”‚  IMAGE UNDERSTANDING                                                       â”‚
â”‚  â”œâ”€â”€ GPT-4V (Vision)                                                       â”‚
â”‚  â”œâ”€â”€ Claude 3 (Vision)                                                     â”‚
â”‚  â”œâ”€â”€ LLaVA (open source)                                                   â”‚
â”‚  â””â”€â”€ BLIP-2                                                                â”‚
â”‚                                                                            â”‚
â”‚  SPEECH                                                                    â”‚
â”‚  â”œâ”€â”€ Speech-to-Text: Whisper (OpenAI, open source!)                        â”‚
â”‚  â”œâ”€â”€ Text-to-Speech: ElevenLabs, Azure, Amazon Polly                       â”‚
â”‚  â””â”€â”€ Voice Cloning: Various providers                                      â”‚
â”‚                                                                            â”‚
â”‚  SPECIALIZED                                                               â”‚
â”‚  â”œâ”€â”€ SQL Generation: SQLCoder, NSQL                                        â”‚
â”‚  â”œâ”€â”€ Math/Reasoning: Specialized fine-tunes                                â”‚
â”‚  â””â”€â”€ Scientific: BioGPT, ChemBERTa                                         â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DevOps-Relevant Model Applications

```yaml
# Model Selection for DevOps Tasks

log_analysis:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Long context for log files, excellent reasoning"
    - name: "GPT-4"
      reason: "Good at pattern recognition in logs"
  approach:
    simple_classification:
      model: "Fine-tuned BERT (local)"
      use_case: "Categorizing log levels, error types"
    semantic_analysis:
      model: "Claude/GPT-4"
      use_case: "Understanding error context, root cause analysis"

code_review:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Excellent at code understanding and security analysis"
    - name: "CodeLlama 34B"
      reason: "Open source, can run locally for sensitive code"
  example_prompt: |
    Review this Terraform code for security issues:
    [code]

documentation:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Great at technical writing"
    - name: "GPT-4"
      reason: "Comprehensive documentation generation"
  automation_idea: |
    # Auto-generate docs from code
    for file in src/*.py; do
        ai_generate_docs "$file" >> docs/api.md
    done

incident_response:
  recommended_models:
    - name: "Claude Opus 4.5"
      reason: "Best for complex reasoning about incidents"
  workflow:
    1_collect: "Gather logs, metrics, recent changes"
    2_analyze: "Send to LLM with incident context template"
    3_hypothesize: "Get ranked list of potential causes"
    4_verify: "Human validates AI suggestions"

infrastructure_as_code:
  recommended_models:
    - name: "Claude Sonnet 4.5"
      reason: "Excellent Terraform/CloudFormation generation"
    - name: "GPT-4"
      reason: "Strong at AWS/GCP/Azure specifics"
  tips:
    - "Always specify cloud provider and version"
    - "Include compliance requirements in prompt"
    - "Request explanatory comments"

chatops_bot:
  recommended_models:
    - name: "Claude Haiku 4.5"
      reason: "Fast, cheap, good for simple queries"
    - name: "GPT-4o-mini"
      reason: "Fast responses, cost-effective"
  use_cases:
    - "Answer questions about infrastructure"
    - "Execute predefined runbooks"
    - "Summarize alerts and incidents"
```

### Real-World Model Comparison: Same Task, Different Models

Recommendations are helpful, but seeing actual output differences is more valuable. Let's compare how different models handle the same DevOps tasks.

#### Task 1: Analyzing a Production Error Log

**Input Prompt** (same for all models):
```
Analyze this production error and suggest root cause:

[ERROR] 2025-01-10 14:32:41 - API Gateway timeout
[INFO] Upstream service response time: 31.2s (expected <5s)
[WARN] Database connection pool: 95/100 connections in use
[INFO] CPU usage: 45%, Memory: 72%, Disk I/O: nominal
[ERROR] 3 retries attempted, all failed
[CONTEXT] Traffic increased 40% after marketing campaign launch
```

**Claude Sonnet 4.5 Output** (Quality: A+):
```
ROOT CAUSE: Database connection pool exhaustion

ANALYSIS:
1. Traffic spike (+40%) is pushing connection pool to limits (95/100)
2. Slow queries are holding connections open longer than expected
3. API Gateway timeouts (31s) suggest database is the bottleneck, not compute
4. CPU/memory are healthy, ruling out resource constraints

IMMEDIATE FIX:
1. Increase connection pool: 100 â†’ 200 connections
2. Add connection pool monitoring alerts (trigger at 80%)
3. Implement query timeout of 10s to prevent connection hogging

LONG-TERM:
1. Analyze slow query logs for optimization opportunities
2. Consider read replicas to distribute SELECT load
3. Implement database connection pooling at application layer (e.g., PgBouncer)
4. Add circuit breaker pattern for graceful degradation

CONFIDENCE: High (95%) - Classic connection pool exhaustion pattern
```

**GPT-4 Turbo Output** (Quality: A):
```
This appears to be a database bottleneck issue. The 31-second response time combined
with near-max connection pool usage (95/100) suggests your database can't handle the
increased load from the marketing campaign.

Recommendations:
- Scale up database instance
- Increase connection pool size
- Optimize slow queries
- Add caching layer (Redis)
- Consider read replicas

The healthy CPU/memory indicates the application servers are fine.
```

**CodeLlama 34B Output** (Quality: B):
```
Error: API Gateway timeout
Cause: Database connection pool full
Solution: Increase pool size or optimize queries
```

**Quality Comparison**:
- **Claude**: Structured analysis, immediate + long-term fixes, confidence level
- **GPT-4**: Solid analysis, good recommendations, less structured
- **CodeLlama**: Technically correct but lacks depth and actionability

**When to use each**:
- **Production incidents** â†’ Claude Sonnet 4.5 (best reasoning, structured output)
- **Quick triage** â†’ GPT-4 Turbo (fast, good enough)
- **Automated classification** â†’ CodeLlama (cheap, identifies issue correctly)

#### Task 2: Generating Infrastructure as Code

**Input Prompt**:
```
Create a production-ready Kubernetes deployment for a Node.js API with:
- 3 replicas for high availability
- Health checks (liveness + readiness)
- Resource limits
- Rolling update strategy
- Environment variables from ConfigMap
```

**Claude Sonnet 4.5 Output** (Lines: 95, Quality: Production-ready):
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
  labels:
    app: nodejs-api
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Ensures zero-downtime deployments
  selector:
    matchLabels:
      app: nodejs-api
  template:
    metadata:
      labels:
        app: nodejs-api
        version: v1
    spec:
      containers:
      - name: api
        image: nodejs-api:latest
        ports:
        - containerPort: 3000
          name: http
          protocol: TCP

        # Resource limits prevent one pod from consuming all node resources
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"

        # Liveness: Restart if application is deadlocked
        livenessProbe:
          httpGet:
            path: /healthz
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        # Readiness: Remove from service if not ready to serve traffic
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3

        # Load configuration from ConfigMap
        envFrom:
        - configMapRef:
            name: nodejs-api-config

        # Security best practices
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true

---
# ConfigMap for application configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: nodejs-api-config
data:
  NODE_ENV: "production"
  LOG_LEVEL: "info"
  API_TIMEOUT: "30s"

---
# Service to expose the deployment
apiVersion: v1
kind: Service
metadata:
  name: nodejs-api
spec:
  selector:
    app: nodejs-api
  ports:
  - port: 80
    targetPort: 3000
  type: ClusterIP
```
**Comments**: Complete manifest with explanations, security hardening, and all requested features.

**GPT-4 Turbo Output** (Lines: 70, Quality: Good):
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nodejs-api
  template:
    metadata:
      labels:
        app: nodejs-api
    spec:
      containers:
      - name: nodejs-api
        image: nodejs-api:latest
        ports:
        - containerPort: 3000
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 15
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
        envFrom:
        - configMapRef:
            name: api-config
```
**Comments**: Solid, covers requirements, but missing security context and detailed probe configuration.

**CodeLlama 34B Output** (Lines: 30, Quality: Basic):
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: api
        image: nodejs-api
        ports:
        - containerPort: 3000
```
**Comments**: Incomplete - missing health checks, resource limits, and ConfigMap integration.

**Cost vs Quality Analysis**:
```
For 100 IaC generation requests:

Claude Sonnet 4.5:
  - Avg input: 200 tokens, output: 800 tokens
  - Cost: (20K/1M Ã— $3) + (80K/1M Ã— $15) = $1.26
  - Quality: Production-ready, minimal edits needed
  - Engineer time saved: ~2 hours @ $150/hr = $300

GPT-4 Turbo:
  - Cost: (20K/1M Ã— $10) + (80K/1M Ã— $30) = $2.60
  - Quality: Good, needs 15min of touch-ups per manifest
  - Engineer time needed: ~25 hours @ $150/hr = $3,750

CodeLlama 34B (self-hosted):
  - Cost: $0 (already running)
  - Quality: Requires 1-2 hours of work per manifest
  - Engineer time needed: ~150 hours @ $150/hr = $22,500
```

**Key insight**: For IaC generation, Claude Sonnet 4.5 is cheapest when you factor in engineer time, despite higher per-token costs than GPT-4.

#### Task 3: Security Code Review

**Input**: Terraform file with intentional security issues

**Output Comparison**:
- **Claude Sonnet 4.5**: Found 5/5 security issues, explained each with fix
- **GPT-4 Turbo**: Found 4/5 security issues, missed hardcoded secret in variable
- **CodeLlama 34B**: Found 2/5 security issues (only obvious ones)

**Conclusion**: For security-critical tasks, the quality gap justifies Claude Opus 4.5 (even better than Sonnet) despite 3-5x cost vs alternatives.

### Model Selection Decision Matrix: Quick Reference

| Use Case | Low Volume | Medium Volume | High Volume | Privacy Required |
|----------|-----------|---------------|-------------|------------------|
| **Code Review** | Claude Sonnet | Claude Sonnet | Claude Haiku + caching | CodeLlama 34B |
| **IaC Generation** | Claude Sonnet | Claude Sonnet | Claude Haiku | CodeLlama / Mistral |
| **Log Analysis** | Claude Haiku | Claude Haiku + caching | Self-hosted classifier | LLaMA 3 70B |
| **Documentation** | Claude Sonnet | Claude Haiku | Claude Haiku | LLaMA 3 70B |
| **Incident Response** | Claude Opus | Claude Sonnet | Claude Sonnet | LLaMA 3 70B |
| **ChatOps Bot** | Claude Haiku | Claude Haiku | Claude Haiku | Mistral 7B |

**Volume definitions**:
- Low: <1M tokens/month (~$50/month budget)
- Medium: 1M-10M tokens/month ($50-$500/month)
- High: >10M tokens/month (>$500/month, consider self-hosting)

---

## 4.5 Model Hosting Options

### Where to Run Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MODEL HOSTING OPTIONS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  1. DIRECT API ACCESS                                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  Provider APIs: api.anthropic.com, api.openai.com                â”‚   â”‚
â”‚     â”‚  Pros: Simplest, always latest models                            â”‚   â”‚
â”‚     â”‚  Cons: Data leaves your network                                  â”‚   â”‚
â”‚     â”‚  Cost: Pay per token                                             â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  2. CLOUD PROVIDER MARKETPLACES                                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  AWS Bedrock: Claude, LLaMA, Titan                               â”‚   â”‚
â”‚     â”‚  Azure OpenAI: GPT-4, GPT-3.5                                    â”‚   â”‚
â”‚     â”‚  Google Vertex AI: Gemini, PaLM                                  â”‚   â”‚
â”‚     â”‚  Pros: Compliance, VPC integration, enterprise features          â”‚   â”‚
â”‚     â”‚  Cons: Slight markup, limited model selection                    â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  3. SELF-HOSTED (Open Source)                                              â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  Options:                                                        â”‚   â”‚
â”‚     â”‚  - vLLM (production-grade serving)                               â”‚   â”‚
â”‚     â”‚  - Text Generation Inference (Hugging Face)                      â”‚   â”‚
â”‚     â”‚  - Ollama (easy local deployment)                                â”‚   â”‚
â”‚     â”‚  - llama.cpp (CPU inference)                                     â”‚   â”‚
â”‚     â”‚                                                                  â”‚   â”‚
â”‚     â”‚  Pros: Full control, data privacy, no per-token cost             â”‚   â”‚
â”‚     â”‚  Cons: Infrastructure overhead, GPU costs, expertise needed      â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  4. MANAGED OPEN SOURCE                                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚  Providers:                                                      â”‚   â”‚
â”‚     â”‚  - Together.ai                                                   â”‚   â”‚
â”‚     â”‚  - Anyscale                                                      â”‚   â”‚
â”‚     â”‚  - Replicate                                                     â”‚   â”‚
â”‚     â”‚  - Modal                                                         â”‚   â”‚
â”‚     â”‚                                                                  â”‚   â”‚
â”‚     â”‚  Pros: Open source models, managed infrastructure                â”‚   â”‚
â”‚     â”‚  Cons: Still have API costs, less control                        â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quick Setup Examples

#### Using Anthropic API Directly

```python
# Direct Claude API usage
import anthropic

client = anthropic.Anthropic(api_key="your-api-key")

message = client.messages.create(
    model="claude-sonnet-4-5-20250514",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Write a Kubernetes health check for nginx"}
    ]
)
print(message.content)
```

#### Using AWS Bedrock

```python
# Claude via AWS Bedrock
import boto3
import json

bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')

response = bedrock.invoke_model(
    modelId='anthropic.claude-sonnet-4-5-20250514-v1:0',
    body=json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1024,
        "messages": [
            {"role": "user", "content": "Write a Kubernetes health check"}
        ]
    })
)
```

#### Self-Hosting with Ollama

```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama2
ollama pull codellama

# Run interactively
ollama run codellama "Write a bash script to check disk space"

# Run as API server
ollama serve

# Query the API
curl http://localhost:11434/api/generate -d '{
  "model": "codellama",
  "prompt": "Write a Dockerfile for Python Flask"
}'
```

#### Self-Hosting with vLLM (Production)

```python
# vLLM for production model serving

# Install
# pip install vllm

# Start server
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)

prompts = ["Write a script to monitor CPU usage"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

```bash
# Or run as OpenAI-compatible server
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000

# Then use standard OpenAI client!
```

---

## 4.6 Model Selection Guide for DevOps

### Decision Matrix

```python
# Model recommendation based on use case

def recommend_model(use_case: str, constraints: dict) -> str:
    """
    Returns recommended model based on use case and constraints.
    """

    recommendations = {
        "code_generation": {
            "no_constraints": "Claude Sonnet 4.5",
            "cost_sensitive": "Claude Haiku 4.5",
            "privacy_required": "CodeLlama 34B (self-hosted)",
            "enterprise": "Azure OpenAI GPT-4 or AWS Bedrock Claude",
        },
        "log_analysis": {
            "no_constraints": "Claude Sonnet 4.5 (long context)",
            "high_volume": "Claude Haiku 4.5 (cost effective)",
            "privacy_required": "Mistral 7B (self-hosted)",
            "real_time": "Fine-tuned local classifier",
        },
        "documentation": {
            "no_constraints": "Claude Sonnet 4.5",
            "cost_sensitive": "Claude Haiku 4.5",
            "technical_accuracy": "Claude Opus 4.5",
        },
        "incident_response": {
            "no_constraints": "Claude Opus 4.5 (best reasoning)",
            "fast_response": "Claude Sonnet 4.5",
            "privacy_required": "LLaMA 3 70B (self-hosted)",
        },
        "chatbot": {
            "no_constraints": "Claude Sonnet 4.5",
            "cost_sensitive": "Claude Haiku 4.5",
            "low_latency": "Claude Haiku 4.5",
        },
        "embeddings": {
            "no_constraints": "OpenAI text-embedding-3-large",
            "self_hosted": "all-MiniLM-L6-v2",
            "multilingual": "multilingual-e5-large",
        }
    }

    return recommendations.get(use_case, {}).get(
        constraints.get("priority", "no_constraints"),
        "Claude Sonnet 4.5"  # Safe default
    )
```

### Cost Comparison Calculator

```python
# Compare costs between different model options

def calculate_monthly_cost(
    input_tokens_per_request: int,
    output_tokens_per_request: int,
    requests_per_day: int,
    model: str
) -> dict:
    """
    Calculate monthly API costs for different models.
    """

    # Prices per 1M tokens (approximate, check current pricing - 2025)
    pricing = {
        "claude-opus-4.5": {"input": 5.0, "output": 25.0},
        "claude-sonnet-4.5": {"input": 3.0, "output": 15.0},
        "claude-haiku-4.5": {"input": 1.0, "output": 5.0},
        "gpt-4-turbo": {"input": 10.0, "output": 30.0},
    }

    if model not in pricing:
        return {"error": f"Unknown model: {model}"}

    prices = pricing[model]
    monthly_requests = requests_per_day * 30

    monthly_input_tokens = input_tokens_per_request * monthly_requests
    monthly_output_tokens = output_tokens_per_request * monthly_requests

    input_cost = (monthly_input_tokens / 1_000_000) * prices["input"]
    output_cost = (monthly_output_tokens / 1_000_000) * prices["output"]

    return {
        "model": model,
        "monthly_requests": monthly_requests,
        "total_input_tokens": monthly_input_tokens,
        "total_output_tokens": monthly_output_tokens,
        "input_cost": round(input_cost, 2),
        "output_cost": round(output_cost, 2),
        "total_monthly_cost": round(input_cost + output_cost, 2),
    }


# Example: Code review bot
# Average PR: 2000 input tokens, 500 output tokens
# 50 PRs per day

for model in ["claude-opus-4.5", "claude-sonnet-4.5", "claude-haiku-4.5", "gpt-4-turbo"]:
    result = calculate_monthly_cost(2000, 500, 50, model)
    print(f"{model}: ${result['total_monthly_cost']}/month")

# Output:
# claude-opus-4.5: $33.75/month
# claude-sonnet-4.5: $20.25/month
# claude-haiku-4.5: $6.75/month
# gpt-4-turbo: $75.00/month
```

---

## 4.7 Emerging Trends

### What's Coming in AI Models

```yaml
# Trends to Watch (2024-2025)

1_multimodal_by_default:
  description: "Models that understand text, images, audio, video together"
  examples:
    - "Upload a screenshot of an error, get debugging help"
    - "Show architecture diagram, get Terraform code"
  current_leaders: ["GPT-4V", "Claude 4.5 Vision", "Gemini"]

2_longer_context_windows:
  description: "Processing entire codebases at once"
  trend: "4K â†’ 32K â†’ 128K â†’ 200K â†’ 1M+ tokens"
  impact_on_devops:
    - "Analyze all logs from an incident at once"
    - "Review entire microservice in one prompt"
    - "Generate documentation for whole project"

3_specialized_agents:
  description: "AI that can take actions, not just generate text"
  examples:
    - "AI that can run kubectl commands"
    - "AI that can create PRs and deploy"
  early_examples: ["Claude Code", "GitHub Copilot Workspace", "Devin"]

4_local_models_improving:
  description: "Smaller, faster models running on personal devices"
  examples:
    - "7B models matching GPT-3.5 quality"
    - "Models running on M1/M2 Macs"
  tools: ["Ollama", "LM Studio", "llama.cpp"]

5_fine_tuning_democratization:
  description: "Easier to create specialized models"
  approaches:
    - "LoRA/QLoRA for efficient fine-tuning"
    - "Few-shot learning from examples"
    - "Retrieval-augmented generation (RAG)"

6_real_time_capabilities:
  description: "Lower latency, streaming responses"
  impact:
    - "Interactive debugging sessions"
    - "Real-time log analysis"
    - "Instant code suggestions"
```

---

## 4.8 Hands-On Exercises

### Exercise 1: Model Comparison

```markdown
## Hands-On: Compare Models

Task: Test the same prompt on different models

### Prompt to Test:
"Write a bash script that:
1. Checks if Docker is running
2. Lists all containers with their CPU/memory usage
3. Alerts if any container uses more than 80% memory
4. Outputs results in JSON format"

### Test on:
1. Claude Sonnet 4.5 (via claude.ai)
2. GPT-4 (via chat.openai.com)
3. CodeLlama (via Ollama locally)

### Evaluation Criteria:
| Criterion           | Claude | GPT-4 | CodeLlama |
|--------------------|---------| ------|-----------|
| Correctness        |         |       |           |
| Code Quality       |         |       |           |
| Edge Case Handling |         |       |           |
| Explanation        |         |       |           |
| Response Time      |         |       |           |

### Your Observations:
[Document differences you noticed]
```

### Exercise 2: Cost Calculation

```markdown
## Hands-On: Calculate Your AI Costs

Scenario: You want to implement an AI-powered log analyzer

### Parameters:
- Average log batch size: _____ characters
- Estimated tokens (chars Ã· 4): _____
- Batches per day: _____
- Expected output tokens per analysis: _____

### Calculate for each model:

| Model | Input Cost/1M | Output Cost/1M | Monthly Cost |
|-------|---------------|----------------|--------------|
| Claude Sonnet 4.5 | $3.00 | $15.00 | $_____ |
| Claude Haiku 4.5 | $1.00 | $5.00 | $_____ |
| GPT-4 Turbo | $10.00 | $30.00 | $_____ |
| Self-hosted | GPU: $___/hr | N/A | $_____ |

### Break-Even Analysis:
At what volume does self-hosting become cheaper?
[Your calculation here]
```

### Exercise 3: Local Model Setup

```bash
# Exercise: Set up Ollama and test CodeLlama

# Step 1: Install Ollama
# Follow instructions at https://ollama.ai

# Step 2: Pull CodeLlama
ollama pull codellama

# Step 3: Test with DevOps prompts
ollama run codellama "Write a Kubernetes CronJob that runs a backup script daily at 2am"

# Step 4: Compare with Claude (if you have API access)
# Note the differences in:
# - Response quality
# - Response time
# - Handling of edge cases

# Document your findings:
# Local CodeLlama:
# - Quality: ____/10
# - Speed: ____ seconds
# - Notes: ____

# Claude API:
# - Quality: ____/10
# - Speed: ____ seconds
# - Notes: ____
```

---

## 4.9 Chapter Summary

### Key Takeaways

1. **Major providers**: Anthropic (Claude), OpenAI (GPT), Google (Gemini), Meta (LLaMA)

2. **Proprietary vs Open Source**: Trade-off between convenience/quality and control/cost

3. **Model selection depends on**: Task complexity, privacy needs, budget, volume, latency requirements

4. **Multiple hosting options**: Direct API, cloud marketplaces, self-hosted, managed open source

5. **The landscape is evolving rapidly**: Stay informed about new models and capabilities

### Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODEL SELECTION QUICK REFERENCE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  NEED BEST QUALITY?                                            â”‚
â”‚  â†’ Claude Opus 4.5, GPT-4                                      â”‚
â”‚                                                                â”‚
â”‚  NEED BALANCE OF QUALITY & COST?                               â”‚
â”‚  â†’ Claude Sonnet 4.5, GPT-4 Turbo                              â”‚
â”‚                                                                â”‚
â”‚  NEED LOW COST?                                                â”‚
â”‚  â†’ Claude Haiku 4.5                                            â”‚
â”‚                                                                â”‚
â”‚  NEED PRIVACY/LOCAL?                                           â”‚
â”‚  â†’ LLaMA 3, Mistral, CodeLlama (via Ollama)                    â”‚
â”‚                                                                â”‚
â”‚  NEED ENTERPRISE COMPLIANCE?                                   â”‚
â”‚  â†’ AWS Bedrock, Azure OpenAI, Google Vertex AI                 â”‚
â”‚                                                                â”‚
â”‚  NEED CODE SPECIFICALLY?                                       â”‚
â”‚  â†’ Claude Sonnet 4.5, CodeLlama, StarCoder                     â”‚
â”‚                                                                â”‚
â”‚  NEED LONG CONTEXT?                                            â”‚
â”‚  â†’ Claude 4.5 (200K), Gemini (1M), GPT-4 Turbo (128K)          â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

[â† Previous: The Art of Prompting](./03-the-art-of-prompting.md) | [Next: Introduction to Claude â†’](./05-introduction-to-claude.md)

---

**Part of**: AI and Claude Code - A Comprehensive Guide for DevOps Engineers  
**Created by**: Michel Abboud with Claude Sonnet 4.5 (Anthropic)  
**Copyright**: Â© 2026 Michel Abboud. All rights reserved.  
**License**: CC BY-NC 4.0

---

## Navigation

â† Previous: [Chapter 3: The Art of Prompting](./03-the-art-of-prompting.md) | Next: [Chapter 5: Introduction to Claude](./05-introduction-to-claude.md) â†’

**Quick Nav:** [README](../README.md) | [Table of Contents](../README.md#table-of-contents)

---

**Chapter 4** | AI Models Landscape | Â© 2026 Michel Abboud | [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)
