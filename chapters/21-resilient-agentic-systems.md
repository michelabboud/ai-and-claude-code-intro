# Chapter 21: Building Resilient Agentic Systems

## Learning Objectives

By the end of this chapter, you will:

- **Understand the circuit breaker pattern** and when to apply it
- **Implement idempotent agent actions** for safe retries
- **Build checkpointing systems** for crash recovery
- **Master exponential backoff** with jitter
- **Design graceful degradation** strategies
- **Create self-healing architectures** that recover automatically
- **Apply production patterns** from Netflix, Google, and other tech giants

**Prerequisites:**
- Chapter 20: Agent Loop Detection & Prevention Fundamentals
- Python 3.9+ installed
- Understanding of async/await
- Basic knowledge of circuit breakers (we'll deep dive here)

**Time to Complete:** 3-4 hours (including hands-on exercises)

---

## Introduction: From Detection to Prevention

In Chapter 20, you learned how to **detect loops** when they occur. This chapter teaches you how to **prevent them** in the first place through resilient system design.

### The Philosophy Shift

**Chapter 20 mindset:** "Catch loops when they happen"
**Chapter 21 mindset:** "Design systems that can't loop"

Think of it like this:
- **Loop detection** = Smoke detector (alerts you to fire)
- **Resilient design** = Fireproof building (prevents fire from spreading)

You need both. But prevention is always better than detection.

---

## The Resilience Triangle

Every resilient agentic system needs three core patterns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     â”‚
â”‚      CIRCUIT BREAKER                â”‚
â”‚    (When to stop trying)            â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚             â”‚       â”‚               â”‚
â”‚             â”‚       â”‚               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚                           â”‚    â”‚
â”‚ IDEMPOTENCY          RETRY LOGIC   â”‚
â”‚ (Safe to retry)    (How to retry)  â”‚
â”‚    â”‚                           â”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                     â”‚
â”‚    = RESILIENT AGENT SYSTEM         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**All three must work together** for true resilience.

---

## Section 1: The Circuit Breaker Pattern

### What is a Circuit Breaker?

Inspired by electrical circuit breakers, this pattern prevents cascading failures by "opening the circuit" when errors exceed a threshold.

**Three states:**

```
CLOSED (Normal)
  â†“ (3 consecutive failures)
OPEN (Blocking)
  â†“ (30 seconds timeout)
HALF-OPEN (Testing)
  â†“ (2 successes) â†“ (1 failure)
CLOSED          OPEN
```

**Real-world analogy:** Like a breaker in your home's electrical panel:
- **CLOSED**: Power flows normally
- **OPEN**: Breaker trips, power cut off
- **HALF-OPEN**: Test if it's safe to restore power

---

### Why Circuit Breakers Matter

**Without circuit breaker:**
```python
# Agent keeps hammering failing API
for i in range(1000):
    try:
        result = call_api()  # Fails every time
    except:
        continue  # Try again immediately

# Result: 1,000 failed requests, wasted time, DoS on API
```

**With circuit breaker:**
```python
# Circuit opens after 3 failures
# Next 997 requests rejected immediately
# Saves time and resources

CLOSED: request 1 â†’ fail
CLOSED: request 2 â†’ fail
CLOSED: request 3 â†’ fail
OPEN:   request 4-1000 â†’ rejected (instant)
        Wait 30 seconds...
HALF_OPEN: request 1001 â†’ test
```

**Benefits:**
- Fails fast (no wasted retries)
- Protects downstream services
- Gives failing systems time to recover
- Prevents resource exhaustion

---

### Building a Production Circuit Breaker

**Installation:**

```bash
pip install enum34 typing
```

**File: `circuit_breaker.py`**

```python
"""
Production-grade circuit breaker for AI agents.

Based on patterns from Netflix Hystrix and Google SRE practices.
"""

import time
import asyncio
from enum import Enum
from typing import Callable, Any, Optional
from dataclasses import dataclass


class CircuitState(Enum):
    """Circuit breaker states"""
    CLOSED = "closed"        # Normal operation
    OPEN = "open"            # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing recovery


@dataclass
class CircuitBreakerConfig:
    """Configuration for circuit breaker behavior"""

    # Number of failures before opening circuit
    failure_threshold: int = 5

    # Seconds to wait before attempting recovery
    recovery_timeout: int = 60

    # Number of successes needed to close circuit from half-open
    success_threshold: int = 2

    # Time window for counting failures (seconds)
    failure_window: int = 120


class CircuitOpenError(Exception):
    """Raised when circuit breaker is open"""
    pass


class CircuitBreaker:
    """
    Production-ready circuit breaker for AI agents.

    Features:
    - Automatic state transitions
    - Configurable thresholds
    - Metrics tracking
    - Half-open state for safe recovery
    - Time-window failure counting

    Usage:
        breaker = CircuitBreaker()

        async def risky_operation():
            async with breaker:
                return await call_external_api()
    """

    def __init__(self, config: Optional[CircuitBreakerConfig] = None):
        self.config = config or CircuitBreakerConfig()
        self.state = CircuitState.CLOSED

        # Failure tracking
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        self.failure_timestamps = []

        # Metrics
        self.total_calls = 0
        self.total_successes = 0
        self.total_failures = 0
        self.total_rejected = 0

    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """
        Execute function through circuit breaker.

        Args:
            func: Async function to execute
            *args, **kwargs: Arguments to pass to function

        Returns:
            Result from function

        Raises:
            CircuitOpenError: If circuit is open
        """
        self.total_calls += 1

        # Check if circuit is open
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self._transition_to_half_open()
            else:
                self.total_rejected += 1
                raise CircuitOpenError(
                    f"Circuit breaker is OPEN. "
                    f"Retry in {self._time_until_retry():.0f} seconds"
                )

        # Attempt the call
        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result

        except Exception as e:
            self._on_failure()
            raise

    async def __aenter__(self):
        """Context manager entry"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        if exc_type is not None:
            self._on_failure()
        else:
            self._on_success()
        return False

    def _on_success(self):
        """Handle successful call"""
        self.total_successes += 1

        if self.state == CircuitState.HALF_OPEN:
            self.success_count += 1

            if self.success_count >= self.config.success_threshold:
                self._transition_to_closed()

    def _on_failure(self):
        """Handle failed call"""
        self.total_failures += 1
        self.failure_count += 1
        self.last_failure_time = time.time()
        self.failure_timestamps.append(time.time())

        # Clean old failures outside window
        cutoff = time.time() - self.config.failure_window
        self.failure_timestamps = [
            ts for ts in self.failure_timestamps if ts > cutoff
        ]

        # Check if we should open circuit
        if len(self.failure_timestamps) >= self.config.failure_threshold:
            self._transition_to_open()

    def _should_attempt_reset(self) -> bool:
        """Check if enough time has passed to attempt recovery"""
        if not self.last_failure_time:
            return False

        elapsed = time.time() - self.last_failure_time
        return elapsed >= self.config.recovery_timeout

    def _time_until_retry(self) -> float:
        """Calculate seconds until retry is allowed"""
        if not self.last_failure_time:
            return 0

        elapsed = time.time() - self.last_failure_time
        remaining = self.config.recovery_timeout - elapsed
        return max(0, remaining)

    def _transition_to_open(self):
        """Transition to OPEN state"""
        if self.state != CircuitState.OPEN:
            print(f"ðŸ”´ Circuit breaker OPEN (failures: {self.failure_count})")
            self.state = CircuitState.OPEN
            # In production, send alert here
            # alert_oncall("Circuit breaker opened", self.get_stats())

    def _transition_to_half_open(self):
        """Transition to HALF_OPEN state"""
        print("ðŸŸ¡ Circuit breaker HALF-OPEN (testing recovery)")
        self.state = CircuitState.HALF_OPEN
        self.success_count = 0

    def _transition_to_closed(self):
        """Transition to CLOSED state"""
        print("ðŸŸ¢ Circuit breaker CLOSED (recovery successful)")
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.failure_timestamps = []

    def get_stats(self) -> dict:
        """Get circuit breaker statistics"""
        return {
            'state': self.state.value,
            'total_calls': self.total_calls,
            'total_successes': self.total_successes,
            'total_failures': self.total_failures,
            'total_rejected': self.total_rejected,
            'success_rate': (
                self.total_successes / self.total_calls
                if self.total_calls > 0 else 0
            ),
            'recent_failures': len(self.failure_timestamps),
            'time_until_retry': self._time_until_retry()
        }


# Example usage
async def main():
    """Demonstrate circuit breaker in action"""

    # Create circuit breaker
    breaker = CircuitBreaker(
        config=CircuitBreakerConfig(
            failure_threshold=3,
            recovery_timeout=10,
            success_threshold=2
        )
    )

    # Simulated API call (fails first 5 times, then succeeds)
    call_count = 0

    async def unreliable_api():
        nonlocal call_count
        call_count += 1
        await asyncio.sleep(0.1)

        if call_count <= 5:
            raise Exception("API error")
        return {"status": "success"}

    # Make calls
    for i in range(20):
        try:
            result = await breaker.call(unreliable_api)
            print(f"Call {i+1}: âœ… {result}")
        except CircuitOpenError as e:
            print(f"Call {i+1}: â›” {e}")
        except Exception as e:
            print(f"Call {i+1}: âŒ {e}")

        await asyncio.sleep(0.5)

    # Print final stats
    print(f"\nFinal stats: {breaker.get_stats()}")


if __name__ == "__main__":
    asyncio.run(main())
```

**Run it:**

```bash
python circuit_breaker.py
```

**Expected output:**

```
Call 1: âŒ API error
Call 2: âŒ API error
Call 3: âŒ API error
ðŸ”´ Circuit breaker OPEN (failures: 3)
Call 4: â›” Circuit breaker is OPEN. Retry in 10 seconds
Call 5: â›” Circuit breaker is OPEN. Retry in 9 seconds
...
Call 20: â›” Circuit breaker is OPEN. Retry in 5 seconds
(after 10 seconds)
ðŸŸ¡ Circuit breaker HALF-OPEN (testing recovery)
Call 21: âœ… {'status': 'success'}
Call 22: âœ… {'status': 'success'}
ðŸŸ¢ Circuit breaker CLOSED (recovery successful)
```

---

### When to Use Circuit Breakers

**Use circuit breakers when:**
- âœ… Calling external APIs (Claude, AWS, databases)
- âœ… Network operations that can fail
- âœ… Operations that impact other services
- âœ… High-cost operations (API calls with billing)

**Don't use circuit breakers for:**
- âŒ Pure in-memory operations
- âŒ File system operations (use retries instead)
- âŒ Operations that must eventually succeed

---

## Section 2: Idempotency â€” Safe to Retry

### What is Idempotency?

**Definition:** An operation is idempotent if performing it multiple times has the same effect as performing it once.

**Mathematical definition:**
```
f(f(x)) = f(x)
```

**Real-world examples:**

| Operation | Idempotent? | Why |
|-----------|-------------|-----|
| `SET user_status = "active"` | âœ… Yes | Same result whether run 1Ã— or 10Ã— |
| `INCREMENT page_views` | âŒ No | Different result each time |
| `DELETE user WHERE id=123` | âœ… Yes | User deleted after first call |
| `CREATE user WITH id=123` | âŒ No | Second call fails (duplicate) |
| `PUT /api/users/123` | âœ… Yes | Updates user to same state |
| `POST /api/users` | âŒ No | Creates new user each time |

---

### Why Idempotency Matters for Agents

**Without idempotency:**
```python
# Agent crashes mid-execution
await increase_memory(pod, by="500Mi")  # Executed
await restart_pod(pod)                   # NOT executed (crash)

# Agent retries from start
await increase_memory(pod, by="500Mi")  # Executed AGAIN! (now +1000Mi)
await restart_pod(pod)                   # Executed

# Result: Memory increased by 1000Mi instead of 500Mi
```

**With idempotency:**
```python
# Agent sets absolute values, not relative changes
await set_memory(pod, to="1024Mi")     # Executed
await restart_pod(pod)                  # NOT executed (crash)

# Agent retries from start
await set_memory(pod, to="1024Mi")     # Executed AGAIN (same result!)
await restart_pod(pod)                  # Executed

# Result: Memory is 1024Mi (correct)
```

---

### Building Idempotent Agent Actions

**Pattern 1: Use Absolute Values, Not Deltas**

```python
# âŒ BAD: Relative changes (not idempotent)
async def scale_up(deployment: str):
    current_replicas = await get_replicas(deployment)
    await set_replicas(deployment, current_replicas + 1)

# If this runs twice, you get +2 instead of +1

# âœ… GOOD: Absolute values (idempotent)
async def ensure_replicas(deployment: str, target: int):
    await set_replicas(deployment, target)

# Safe to run multiple times
```

**Pattern 2: Check Before Acting**

```python
# âœ… GOOD: Check-then-act pattern
async def ensure_config_exists(config_name: str, data: dict):
    # Check if already exists
    existing = await get_config(config_name)

    if existing and existing['data'] == data:
        # Already in desired state, no action needed
        return {"status": "already_exists", "action": "none"}

    if existing:
        # Update existing
        await update_config(config_name, data)
        return {"status": "updated", "action": "update"}
    else:
        # Create new
        await create_config(config_name, data)
        return {"status": "created", "action": "create"}
```

**Pattern 3: Use Idempotency Keys**

```python
# âœ… GOOD: Idempotency key pattern (Stripe-style)
async def charge_customer(
    customer_id: str,
    amount: float,
    idempotency_key: str
):
    # Check if already processed
    existing = await db.charges.find_one({
        'idempotency_key': idempotency_key
    })

    if existing:
        # Already processed, return cached result
        return existing

    # Process charge
    charge = await stripe.charge.create(
        customer=customer_id,
        amount=amount,
        idempotency_key=idempotency_key
    )

    # Cache result
    await db.charges.insert_one({
        'idempotency_key': idempotency_key,
        'charge_id': charge.id,
        'timestamp': time.time()
    })

    return charge
```

---

### Idempotent Agent Implementation

**File: `idempotent_agent.py`**

```python
"""
Idempotent agent with action tracking.

Ensures all actions are safe to retry.
"""

import hashlib
import json
import asyncio
from typing import Dict, Callable, Any, Optional
from dataclasses import dataclass, field


@dataclass
class ActionRecord:
    """Record of a completed action"""
    action_id: str
    action_name: str
    params: Dict
    result: Any
    timestamp: float
    completed: bool = False


class IdempotentAgent:
    """
    Agent that tracks completed actions and ensures idempotency.

    All actions must be idempotent and identified by unique keys.
    """

    def __init__(self, state_file: str = "agent_state.json"):
        self.state_file = state_file
        self.completed_actions: Dict[str, ActionRecord] = {}
        self._load_state()

    def _load_state(self):
        """Load completed actions from disk (crash recovery)"""
        try:
            with open(self.state_file, 'r') as f:
                data = json.load(f)
                for action_id, record in data.items():
                    self.completed_actions[action_id] = ActionRecord(**record)
        except FileNotFoundError:
            pass

    def _save_state(self):
        """Persist completed actions to disk"""
        data = {
            action_id: {
                'action_id': record.action_id,
                'action_name': record.action_name,
                'params': record.params,
                'result': record.result,
                'timestamp': record.timestamp,
                'completed': record.completed
            }
            for action_id, record in self.completed_actions.items()
        }

        with open(self.state_file, 'w') as f:
            json.dump(data, f, indent=2)

    def _generate_action_id(self, action_name: str, params: Dict) -> str:
        """Generate unique ID for action based on name and params"""
        # Sort params for consistent hashing
        params_str = json.dumps(params, sort_keys=True)
        content = f"{action_name}:{params_str}"
        return hashlib.sha256(content.encode()).hexdigest()

    async def execute(
        self,
        action_name: str,
        action_func: Callable,
        params: Optional[Dict] = None,
        **kwargs
    ) -> Any:
        """
        Execute action idempotently.

        Args:
            action_name: Name of action
            action_func: Async function to execute
            params: Parameters for action (used in idempotency key)
            **kwargs: Additional arguments to pass to action_func

        Returns:
            Result from action_func
        """
        params = params or {}

        # Generate idempotency key
        action_id = self._generate_action_id(action_name, params)

        # Check if already completed
        if action_id in self.completed_actions:
            record = self.completed_actions[action_id]
            print(f"âœ“ Action '{action_name}' already completed, returning cached result")
            return record.result

        # Execute action
        print(f"â–¶ Executing action '{action_name}' (id: {action_id[:8]}...)")
        result = await action_func(**kwargs)

        # Record completion
        record = ActionRecord(
            action_id=action_id,
            action_name=action_name,
            params=params,
            result=result,
            timestamp=asyncio.get_event_loop().time(),
            completed=True
        )

        self.completed_actions[action_id] = record
        self._save_state()

        print(f"âœ“ Action '{action_name}' completed successfully")
        return result

    def reset(self):
        """Clear all completed actions (for testing)"""
        self.completed_actions.clear()
        self._save_state()


# Example usage
async def main():
    """Demonstrate idempotent agent"""

    agent = IdempotentAgent(state_file="demo_state.json")
    agent.reset()  # Start fresh

    # Define some actions
    async def set_pod_memory(pod_name: str, memory: str):
        print(f"  â†’ Setting {pod_name} memory to {memory}")
        await asyncio.sleep(0.5)
        return {"pod": pod_name, "memory": memory}

    async def restart_pod(pod_name: str):
        print(f"  â†’ Restarting {pod_name}")
        await asyncio.sleep(0.5)
        return {"pod": pod_name, "status": "restarted"}

    # Simulate workflow with crash and retry
    print("=== Attempt 1 (will 'crash' after first action) ===\n")

    # Action 1: Set memory
    await agent.execute(
        "set_pod_memory",
        set_pod_memory,
        params={"pod": "payment-service", "memory": "2Gi"},
        pod_name="payment-service",
        memory="2Gi"
    )

    # Simulate crash here!
    print("\nðŸ’¥ Simulated crash!\n")

    print("=== Attempt 2 (retry from start) ===\n")

    # Action 1: Set memory (will skip, already done)
    await agent.execute(
        "set_pod_memory",
        set_pod_memory,
        params={"pod": "payment-service", "memory": "2Gi"},
        pod_name="payment-service",
        memory="2Gi"
    )

    # Action 2: Restart pod (will execute)
    await agent.execute(
        "restart_pod",
        restart_pod,
        params={"pod": "payment-service"},
        pod_name="payment-service"
    )

    print("\n=== Workflow completed successfully! ===")


if __name__ == "__main__":
    asyncio.run(main())
```

**Run it:**

```bash
python idempotent_agent.py
```

**Expected output:**

```
=== Attempt 1 (will 'crash' after first action) ===

â–¶ Executing action 'set_pod_memory' (id: a3f5b2c8...)
  â†’ Setting payment-service memory to 2Gi
âœ“ Action 'set_pod_memory' completed successfully

ðŸ’¥ Simulated crash!

=== Attempt 2 (retry from start) ===

âœ“ Action 'set_pod_memory' already completed, returning cached result
â–¶ Executing action 'restart_pod' (id: 9d7e1f4a...)
  â†’ Restarting payment-service
âœ“ Action 'restart_pod' completed successfully

=== Workflow completed successfully! ===
```

---

## Section 3: State Checkpointing and Recovery

### What is Checkpointing?

**Definition:** Saving progress at regular intervals so you can resume from the last checkpoint instead of starting over.

**Analogy:** Like save points in a video game:
- âŒ No checkpoints: Die â†’ restart entire game
- âœ… Checkpoints: Die â†’ restart from last save point

---

### Implementing Checkpoints

**Pattern: Checkpoint After Each Major Step**

```python
"""
Agent with checkpointing for crash recovery.
"""

import json
import asyncio
from typing import Dict, Any, List
from dataclasses import dataclass, asdict


@dataclass
class Checkpoint:
    """Represents a saved checkpoint"""
    workflow_id: str
    step: int
    step_name: str
    state: Dict[str, Any]
    timestamp: float


class CheckpointedAgent:
    """
    Agent that saves checkpoints for crash recovery.

    Can resume from last checkpoint after crash.
    """

    def __init__(self, workflow_id: str, checkpoint_file: str = "checkpoints.json"):
        self.workflow_id = workflow_id
        self.checkpoint_file = checkpoint_file
        self.current_step = 0
        self.state = {}

    def save_checkpoint(self, step_name: str):
        """Save current state as checkpoint"""
        checkpoint = Checkpoint(
            workflow_id=self.workflow_id,
            step=self.current_step,
            step_name=step_name,
            state=self.state.copy(),
            timestamp=asyncio.get_event_loop().time()
        )

        # Load existing checkpoints
        try:
            with open(self.checkpoint_file, 'r') as f:
                checkpoints = json.load(f)
        except FileNotFoundError:
            checkpoints = {}

        # Save checkpoint
        checkpoints[self.workflow_id] = asdict(checkpoint)

        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoints, f, indent=2)

        print(f"ðŸ’¾ Checkpoint saved: step {self.current_step} ({step_name})")

    def load_checkpoint(self) -> bool:
        """Load last checkpoint if exists"""
        try:
            with open(self.checkpoint_file, 'r') as f:
                checkpoints = json.load(f)

            if self.workflow_id in checkpoints:
                checkpoint_data = checkpoints[self.workflow_id]
                self.current_step = checkpoint_data['step'] + 1  # Resume from next step
                self.state = checkpoint_data['state']
                print(f"ðŸ“‚ Loaded checkpoint: step {checkpoint_data['step']} ({checkpoint_data['step_name']})")
                print(f"   Resuming from step {self.current_step}")
                return True

        except FileNotFoundError:
            pass

        return False

    async def run_workflow(self, steps: List[tuple]):
        """
        Run workflow with checkpoints.

        Args:
            steps: List of (step_name, step_func, step_args) tuples
        """
        # Try to load checkpoint
        resumed = self.load_checkpoint()

        if resumed:
            print(f"â–¶ Resuming workflow from step {self.current_step}\n")
        else:
            print(f"â–¶ Starting new workflow\n")

        # Execute steps
        for i, (step_name, step_func, step_args) in enumerate(steps):
            # Skip already completed steps
            if i < self.current_step:
                print(f"â­  Skipping step {i}: {step_name} (already completed)")
                continue

            print(f"\nStep {i}: {step_name}")
            result = await step_func(**step_args)

            # Update state
            self.state[step_name] = result
            self.current_step = i

            # Save checkpoint
            self.save_checkpoint(step_name)

        print(f"\nâœ“ Workflow completed!")


# Example usage
async def main():
    """Demonstrate checkpointed workflow"""

    # Define workflow steps
    async def analyze_logs(incident_id: str):
        print(f"  Analyzing logs for {incident_id}...")
        await asyncio.sleep(1)
        return {"errors": 15, "warnings": 42}

    async def check_metrics(incident_id: str):
        print(f"  Checking metrics for {incident_id}...")
        await asyncio.sleep(1)
        return {"cpu": "85%", "memory": "92%"}

    async def identify_root_cause(incident_id: str):
        print(f"  Identifying root cause for {incident_id}...")
        await asyncio.sleep(1)
        return {"cause": "memory leak", "service": "payment-api"}

    async def apply_fix(incident_id: str):
        print(f"  Applying fix for {incident_id}...")
        # Simulate crash here on first run
        if not agent.load_checkpoint():
            print("  ðŸ’¥ Simulated crash!")
            raise Exception("Crash during fix")
        await asyncio.sleep(1)
        return {"fix_applied": True}

    async def verify_fix(incident_id: str):
        print(f"  Verifying fix for {incident_id}...")
        await asyncio.sleep(1)
        return {"verified": True}

    workflow_steps = [
        ("analyze_logs", analyze_logs, {"incident_id": "INC-12345"}),
        ("check_metrics", check_metrics, {"incident_id": "INC-12345"}),
        ("identify_root_cause", identify_root_cause, {"incident_id": "INC-12345"}),
        ("apply_fix", apply_fix, {"incident_id": "INC-12345"}),
        ("verify_fix", verify_fix, {"incident_id": "INC-12345"}),
    ]

    # First attempt (will crash)
    print("="*60)
    print("ATTEMPT 1 (will crash during 'apply_fix')")
    print("="*60)

    agent = CheckpointedAgent(workflow_id="incident-INC-12345")

    try:
        await agent.run_workflow(workflow_steps)
    except Exception as e:
        print(f"\nâŒ Workflow failed: {e}")

    # Second attempt (will resume from checkpoint)
    print("\n" + "="*60)
    print("ATTEMPT 2 (resuming from checkpoint)")
    print("="*60 + "\n")

    agent2 = CheckpointedAgent(workflow_id="incident-INC-12345")
    await agent2.run_workflow(workflow_steps)


if __name__ == "__main__":
    asyncio.run(main())
```

**Run it:**

```bash
python checkpointed_agent.py
```

**Expected output:**

```
============================================================
ATTEMPT 1 (will crash during 'apply_fix')
============================================================
â–¶ Starting new workflow

Step 0: analyze_logs
  Analyzing logs for INC-12345...
ðŸ’¾ Checkpoint saved: step 0 (analyze_logs)

Step 1: check_metrics
  Checking metrics for INC-12345...
ðŸ’¾ Checkpoint saved: step 1 (check_metrics)

Step 2: identify_root_cause
  Identifying root cause for INC-12345...
ðŸ’¾ Checkpoint saved: step 2 (identify_root_cause)

Step 3: apply_fix
  Applying fix for INC-12345...
  ðŸ’¥ Simulated crash!

âŒ Workflow failed: Crash during fix

============================================================
ATTEMPT 2 (resuming from checkpoint)
============================================================

ðŸ“‚ Loaded checkpoint: step 2 (identify_root_cause)
   Resuming from step 3
â–¶ Resuming workflow from step 3

â­  Skipping step 0: analyze_logs (already completed)
â­  Skipping step 1: check_metrics (already completed)
â­  Skipping step 2: identify_root_cause (already completed)

Step 3: apply_fix
  Applying fix for INC-12345...
ðŸ’¾ Checkpoint saved: step 3 (apply_fix)

Step 4: verify_fix
  Verifying fix for INC-12345...
ðŸ’¾ Checkpoint saved: step 4 (verify_fix)

âœ“ Workflow completed!
```

---

## 4. Exponential Backoff with Jitter

### What is Exponential Backoff?

**Exponential backoff** is a retry strategy where the wait time between retries increases exponentially: 1 second, 2 seconds, 4 seconds, 8 seconds, etc. This prevents overwhelming a failing service with retry attempts.

**Jitter** adds randomness to prevent synchronized retries from multiple clients hitting the service at the same time (thundering herd problem).

**Mathematical Formula:**

```python
# Without jitter (bad)
wait_time = min(base_delay * (2 ** attempt), max_delay)

# With jitter (good)
wait_time = min(base_delay * (2 ** attempt), max_delay) * random.uniform(0.5, 1.5)
```

**Analogy:**

Imagine a restaurant that's closed for maintenance:

âŒ **Linear backoff (bad):** Everyone checks every 5 minutes â†’ they all arrive at the same time
âœ… **Exponential backoff (better):** Wait 1min, 2min, 4min, 8min â†’ spreads arrivals
âœ… **Exponential + jitter (best):** Wait ~1min Â±30s, ~2min Â±1min â†’ fully randomized arrivals

### Why It Matters

**Real-World Impact:**

In December 2012, **AWS experienced a massive outage** when one of their Elastic Load Balancer (ELB) services failed. Thousands of customers implemented retry logic WITHOUT jitter, causing:

- **Synchronized retry storms** every N seconds
- **Thundering herd problem** overwhelming recovery attempts
- **Extended outage duration** because the service couldn't stabilize

After this incident, AWS published official guidance recommending **exponential backoff with jitter** for all retry logic.

**Production Statistics:**

- **Linear retries**: Can cause 10x traffic spikes during recovery
- **Exponential without jitter**: 3-5x traffic spikes (synchronized waves)
- **Exponential with jitter**: ~1.2x traffic spike (smooth recovery)

### Pattern Examples

#### âŒ Bad: Linear Backoff (Fixed Delays)

```python
async def call_api_bad(url: str, max_retries: int = 5):
    """BAD: Fixed 5-second delay between retries"""
    for attempt in range(max_retries):
        try:
            response = await http_client.get(url)
            return response.json()
        except Exception as e:
            if attempt < max_retries - 1:
                await asyncio.sleep(5)  # Always 5 seconds âŒ
            else:
                raise

# Problem: If 1000 clients fail at the same time, they all retry
# at t=5s, t=10s, t=15s â†’ synchronized thundering herd
```

#### âš ï¸ Better: Exponential Backoff (No Jitter)

```python
async def call_api_better(url: str, max_retries: int = 5):
    """BETTER: Exponential delays but synchronized"""
    base_delay = 1

    for attempt in range(max_retries):
        try:
            response = await http_client.get(url)
            return response.json()
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = base_delay * (2 ** attempt)  # 1, 2, 4, 8, 16
                await asyncio.sleep(wait_time)
            else:
                raise

# Problem: Still synchronized! All clients retry at t=1s, t=3s, t=7s, t=15s
# Better than linear, but still causes spikes
```

#### âœ… Best: Exponential Backoff with Full Jitter

```python
import random

async def call_api_best(url: str, max_retries: int = 5):
    """BEST: Exponential backoff with full jitter"""
    base_delay = 1
    max_delay = 32

    for attempt in range(max_retries):
        try:
            response = await http_client.get(url)
            return response.json()
        except Exception as e:
            if attempt < max_retries - 1:
                # Exponential backoff
                exp_delay = min(base_delay * (2 ** attempt), max_delay)

                # Full jitter: random value between 0 and exp_delay
                wait_time = random.uniform(0, exp_delay)

                await asyncio.sleep(wait_time)
            else:
                raise

# Result: Clients retry at completely randomized times
# No thundering herd, smooth recovery curve
```

### Production Implementation

**Complete Retry Strategy with Multiple Jitter Types:**

```python
"""
Production-ready retry strategy with exponential backoff and jitter.

Installation:
    pip install aiohttp tenacity  # For async HTTP and advanced retry decorators

Usage:
    retry = RetryStrategy(base_delay=1, max_delay=60, max_retries=5, jitter_type="full")
    result = await retry.execute(my_async_function, arg1, arg2)
"""

import asyncio
import random
import time
from typing import Callable, Optional, Any, Type, Tuple
from enum import Enum
from dataclasses import dataclass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class JitterType(Enum):
    """Types of jitter strategies"""
    NONE = "none"          # No jitter (synchronized retries)
    FULL = "full"          # Uniform random [0, delay]
    EQUAL = "equal"        # delay/2 + random[0, delay/2]
    DECORRELATED = "decorrelated"  # Smooth cap based on previous delay


@dataclass
class RetryConfig:
    """Configuration for retry behavior"""
    base_delay: float = 1.0        # Initial delay in seconds
    max_delay: float = 60.0        # Maximum delay cap
    max_retries: int = 5           # Maximum retry attempts
    jitter_type: JitterType = JitterType.FULL
    exponential_base: int = 2      # Base for exponential growth (usually 2)

    # Retry budget (max total time willing to wait)
    max_total_wait_time: Optional[float] = None  # None = no limit

    # Which exceptions to retry
    retryable_exceptions: Tuple[Type[Exception], ...] = (Exception,)

    # Callback for retry events
    on_retry: Optional[Callable[[int, Exception, float], None]] = None


class RetryBudgetExceeded(Exception):
    """Raised when total retry time exceeds budget"""
    pass


class RetryStrategy:
    """
    Production-ready retry strategy with exponential backoff and jitter.

    Based on AWS Architecture Blog recommendations:
    https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/
    """

    def __init__(self, config: Optional[RetryConfig] = None):
        self.config = config or RetryConfig()
        self.total_wait_time = 0.0
        self.last_delay = self.config.base_delay  # For decorrelated jitter

    def _calculate_delay(self, attempt: int) -> float:
        """Calculate delay for given attempt with jitter"""

        # Exponential backoff: base_delay * (exponential_base ^ attempt)
        exponential_delay = self.config.base_delay * (
            self.config.exponential_base ** attempt
        )

        # Cap at max_delay
        capped_delay = min(exponential_delay, self.config.max_delay)

        # Apply jitter based on strategy
        if self.config.jitter_type == JitterType.NONE:
            # No jitter (not recommended for production)
            return capped_delay

        elif self.config.jitter_type == JitterType.FULL:
            # Full jitter: random between 0 and capped_delay
            # Most aggressive jitter, best for high-concurrency scenarios
            return random.uniform(0, capped_delay)

        elif self.config.jitter_type == JitterType.EQUAL:
            # Equal jitter: delay/2 + random[0, delay/2]
            # Guarantees at least 50% of the calculated delay
            return capped_delay / 2 + random.uniform(0, capped_delay / 2)

        elif self.config.jitter_type == JitterType.DECORRELATED:
            # Decorrelated jitter: smooth growth based on previous delay
            # Prevents sudden jumps, good for gradual recovery
            self.last_delay = min(
                self.config.max_delay,
                random.uniform(self.config.base_delay, self.last_delay * 3)
            )
            return self.last_delay

        else:
            raise ValueError(f"Unknown jitter type: {self.config.jitter_type}")

    def _is_retryable(self, exception: Exception) -> bool:
        """Check if exception is retryable"""
        return isinstance(exception, self.config.retryable_exceptions)

    def _check_budget(self):
        """Check if retry budget is exceeded"""
        if self.config.max_total_wait_time is not None:
            if self.total_wait_time >= self.config.max_total_wait_time:
                raise RetryBudgetExceeded(
                    f"Exceeded retry budget of {self.config.max_total_wait_time}s "
                    f"(waited {self.total_wait_time:.1f}s)"
                )

    async def execute(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        Execute function with retry logic.

        Args:
            func: Async function to execute
            *args: Positional arguments for func
            **kwargs: Keyword arguments for func

        Returns:
            Result from successful function call

        Raises:
            Exception: Last exception if all retries exhausted
            RetryBudgetExceeded: If total wait time exceeds budget
        """
        last_exception = None

        for attempt in range(self.config.max_retries + 1):
            try:
                # Attempt execution
                result = await func(*args, **kwargs)

                if attempt > 0:
                    logger.info(
                        f"âœ“ Succeeded after {attempt} retries "
                        f"(total wait: {self.total_wait_time:.1f}s)"
                    )

                return result

            except Exception as e:
                last_exception = e

                # Check if exception is retryable
                if not self._is_retryable(e):
                    logger.error(f"Non-retryable exception: {e}")
                    raise

                # Check if we have retries left
                if attempt >= self.config.max_retries:
                    logger.error(
                        f"âŒ All {self.config.max_retries} retries exhausted. "
                        f"Total wait time: {self.total_wait_time:.1f}s"
                    )
                    raise

                # Calculate delay with jitter
                delay = self._calculate_delay(attempt)

                # Check retry budget before waiting
                self._check_budget()

                # Log retry attempt
                logger.warning(
                    f"âš ï¸  Attempt {attempt + 1}/{self.config.max_retries + 1} failed: {e}"
                )
                logger.info(
                    f"   Retrying in {delay:.2f}s (jitter: {self.config.jitter_type.value})"
                )

                # Call retry callback if provided
                if self.config.on_retry:
                    self.config.on_retry(attempt, e, delay)

                # Wait before retry
                await asyncio.sleep(delay)
                self.total_wait_time += delay

        # Should never reach here, but just in case
        raise last_exception


# ============================================================
# DEMONSTRATION: Compare Different Jitter Strategies
# ============================================================

async def flaky_api_call(fail_count: int = 3) -> str:
    """Simulates a flaky API that fails N times then succeeds"""
    if not hasattr(flaky_api_call, 'attempts'):
        flaky_api_call.attempts = 0

    flaky_api_call.attempts += 1

    if flaky_api_call.attempts <= fail_count:
        raise ConnectionError(f"API unavailable (attempt {flaky_api_call.attempts})")

    return "Success!"


async def demo_jitter_comparison():
    """Demonstrate different jitter strategies"""

    print("=" * 70)
    print("JITTER STRATEGY COMPARISON")
    print("=" * 70)
    print()

    jitter_types = [
        JitterType.NONE,
        JitterType.FULL,
        JitterType.EQUAL,
        JitterType.DECORRELATED
    ]

    for jitter_type in jitter_types:
        print(f"\n{'='*70}")
        print(f"Testing: {jitter_type.value.upper()} jitter")
        print(f"{'='*70}\n")

        # Reset flaky API
        flaky_api_call.attempts = 0

        config = RetryConfig(
            base_delay=1.0,
            max_delay=16.0,
            max_retries=5,
            jitter_type=jitter_type,
            max_total_wait_time=30.0
        )

        retry = RetryStrategy(config)

        start_time = time.time()

        try:
            result = await retry.execute(flaky_api_call, fail_count=3)
            elapsed = time.time() - start_time

            print(f"\nâœ“ Result: {result}")
            print(f"  Total time: {elapsed:.2f}s")
            print(f"  Total wait time: {retry.total_wait_time:.2f}s")

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"\nâŒ Failed: {e}")
            print(f"   Total time: {elapsed:.2f}s")


async def demo_retry_budget():
    """Demonstrate retry budget (max total wait time)"""

    print("\n" + "=" * 70)
    print("RETRY BUDGET DEMO")
    print("=" * 70)
    print("\nScenario: API is down, but we have a 10-second budget")
    print()

    # Reset flaky API to always fail
    flaky_api_call.attempts = 0

    config = RetryConfig(
        base_delay=2.0,
        max_delay=60.0,
        max_retries=10,  # High retry count
        jitter_type=JitterType.FULL,
        max_total_wait_time=10.0  # But budget is only 10 seconds
    )

    retry = RetryStrategy(config)

    start_time = time.time()

    try:
        # This will fail many times, but budget will stop it
        result = await retry.execute(flaky_api_call, fail_count=100)
    except RetryBudgetExceeded as e:
        elapsed = time.time() - start_time
        print(f"\nâ±  Budget exceeded: {e}")
        print(f"   Total time: {elapsed:.2f}s")
        print(f"   Total wait time: {retry.total_wait_time:.2f}s")
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"\nâŒ Failed: {e}")
        print(f"   Total time: {elapsed:.2f}s")


async def demo_with_circuit_breaker():
    """Demonstrate retry + circuit breaker integration"""

    print("\n" + "=" * 70)
    print("RETRY + CIRCUIT BREAKER INTEGRATION")
    print("=" * 70)
    print("\nBest practice: Combine retry logic with circuit breaker")
    print()

    # Import circuit breaker from previous section
    # (In real code, this would be: from .circuit_breaker import CircuitBreaker)

    print("âœ“ Retry handles transient failures (network blips)")
    print("âœ“ Circuit breaker handles sustained failures (service down)")
    print("âœ“ Together: Retry a few times, then open circuit to stop wasting resources")
    print()
    print("Example flow:")
    print("  1. API call fails â†’ Retry with backoff (attempt 1)")
    print("  2. API call fails â†’ Retry with backoff (attempt 2)")
    print("  3. API call fails â†’ Retry with backoff (attempt 3)")
    print("  4. Circuit breaker: 3 failures â†’ Open circuit")
    print("  5. Subsequent calls fail fast (circuit open)")
    print("  6. After timeout â†’ Half-open â†’ Test if service recovered")


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("EXPONENTIAL BACKOFF WITH JITTER - DEMONSTRATION")
    print("=" * 70)

    asyncio.run(demo_jitter_comparison())
    asyncio.run(demo_retry_budget())
    asyncio.run(demo_with_circuit_breaker())

    print("\n" + "=" * 70)
    print("KEY TAKEAWAYS")
    print("=" * 70)
    print()
    print("âœ“ ALWAYS use exponential backoff for retries")
    print("âœ“ ALWAYS add jitter to prevent thundering herd")
    print("âœ“ Full jitter is best for high-concurrency scenarios")
    print("âœ“ Set retry budgets to prevent infinite waiting")
    print("âœ“ Combine with circuit breakers for best results")
    print("âœ“ Only retry transient failures, not permanent errors")
    print()
```

**Expected Output:**

```
======================================================================
JITTER STRATEGY COMPARISON
======================================================================

======================================================================
Testing: NONE jitter
======================================================================

âš ï¸  Attempt 1/6 failed: API unavailable (attempt 1)
   Retrying in 1.00s (jitter: none)
âš ï¸  Attempt 2/6 failed: API unavailable (attempt 2)
   Retrying in 2.00s (jitter: none)
âš ï¸  Attempt 3/6 failed: API unavailable (attempt 3)
   Retrying in 4.00s (jitter: none)
âœ“ Succeeded after 3 retries (total wait: 7.0s)

âœ“ Result: Success!
  Total time: 7.02s
  Total wait time: 7.00s

======================================================================
Testing: FULL jitter
======================================================================

âš ï¸  Attempt 1/6 failed: API unavailable (attempt 1)
   Retrying in 0.47s (jitter: full)
âš ï¸  Attempt 2/6 failed: API unavailable (attempt 2)
   Retrying in 1.23s (jitter: full)
âš ï¸  Attempt 3/6 failed: API unavailable (attempt 3)
   Retrying in 2.89s (jitter: full)
âœ“ Succeeded after 3 retries (total wait: 4.6s)

âœ“ Result: Success!
  Total time: 4.62s
  Total wait time: 4.59s

======================================================================
Testing: EQUAL jitter
======================================================================

âš ï¸  Attempt 1/6 failed: API unavailable (attempt 1)
   Retrying in 0.73s (jitter: equal)
âš ï¸  Attempt 2/6 failed: API unavailable (attempt 2)
   Retrying in 1.45s (jitter: equal)
âš ï¸  Attempt 3/6 failed: API unavailable (attempt 3)
   Retrying in 3.12s (jitter: equal)
âœ“ Succeeded after 3 retries (total wait: 5.3s)

âœ“ Result: Success!
  Total time: 5.32s
  Total wait time: 5.30s

======================================================================
RETRY BUDGET DEMO
======================================================================

Scenario: API is down, but we have a 10-second budget

âš ï¸  Attempt 1/11 failed: API unavailable (attempt 1)
   Retrying in 1.23s (jitter: full)
âš ï¸  Attempt 2/11 failed: API unavailable (attempt 2)
   Retrying in 2.87s (jitter: full)
âš ï¸  Attempt 3/11 failed: API unavailable (attempt 3)
   Retrying in 3.45s (jitter: full)
âš ï¸  Attempt 4/11 failed: API unavailable (attempt 4)
   Retrying in 5.67s (jitter: full)

â±  Budget exceeded: Exceeded retry budget of 10.0s (waited 10.2s)
   Total time: 10.25s
   Total wait time: 10.22s
```

### When to Use Exponential Backoff

âœ… **Use exponential backoff when:**

- **Retrying failed API calls** (HTTP 429, 500, 503 errors)
- **Polling for resource availability** (waiting for instance to start)
- **Distributed system communication** (service-to-service calls)
- **Rate-limited operations** (GitHub API, AWS API throttling)
- **Transient failure recovery** (network blips, temporary outages)
- **Queue processing** (retrying failed message processing)

âŒ **Don't use exponential backoff when:**

- **Permanent failures** (HTTP 404, 401, 403 â†’ fail fast, don't retry)
- **User-facing operations** (users won't wait 30+ seconds)
- **Time-sensitive operations** (stock trading, real-time bidding)
- **Known failure modes** (use circuit breaker instead for sustained outages)
- **Local operations** (file I/O, database on same network â†’ simple retry)

### Integration with Other Patterns

**Exponential backoff works best when combined with:**

1. **Circuit Breaker** (from Section 1)
   - Retry handles transient failures
   - Circuit breaker handles sustained failures
   - Retry a few times â†’ then open circuit

2. **Idempotency** (from Section 2)
   - Ensure retries don't cause duplicate actions
   - Safe to retry because operations are idempotent

3. **Timeouts**
   - Set overall timeout for the entire retry sequence
   - Don't let retries run indefinitely

4. **Observability**
   - Log retry attempts and delays
   - Track retry success rates
   - Alert on excessive retries (indicates systemic issue)

**Production Configuration Example:**

```python
# Production-ready configuration for most scenarios
production_config = RetryConfig(
    base_delay=1.0,           # Start with 1 second
    max_delay=60.0,           # Cap at 1 minute
    max_retries=3,            # 3 retries = 4 total attempts
    jitter_type=JitterType.FULL,  # Best for avoiding thundering herd
    max_total_wait_time=120.0,    # Give up after 2 minutes total
    retryable_exceptions=(
        ConnectionError,
        TimeoutError,
        # Add specific HTTP errors from your HTTP library
    ),
)

# For high-traffic APIs with strict rate limits
aggressive_config = RetryConfig(
    base_delay=2.0,           # Start slower
    max_delay=120.0,          # Allow longer waits
    max_retries=5,            # More retries
    jitter_type=JitterType.FULL,
    exponential_base=3,       # Grow faster (2, 6, 18, 54 seconds)
)

# For user-facing operations (keep it fast)
user_facing_config = RetryConfig(
    base_delay=0.5,           # Start quickly
    max_delay=5.0,            # Don't make users wait too long
    max_retries=2,            # Only 2 retries
    jitter_type=JitterType.EQUAL,  # Ensure minimum delay
    max_total_wait_time=10.0,      # 10 seconds max
)
```

---

## 5. Graceful Degradation Strategies

### What is Graceful Degradation?

**Graceful degradation** means your system continues to provide value (even if reduced) when components fail, rather than failing completely. It's about building systems that degrade in capability but not in stability.

**Philosophy:**

> "It's better to give users 80% functionality with 100% uptime than 100% functionality with 80% uptime."

**Analogy:**

Think of a car with a flat tire:

âŒ **No degradation**: Car completely stops working (total failure)
âœ… **Graceful degradation**: Car switches to "limp mode" (reduced speed, still functional)

Or a smartphone with low battery:

âŒ **Abrupt failure**: Phone dies suddenly at 20%
âœ… **Graceful degradation**: Battery saver mode â†’ reduced features â†’ critical functions only

### Why It Matters

**Real-World Impact:**

**Netflix** is famous for graceful degradation. When their recommendation engine fails:
- âŒ **Bad approach**: Show error page, user can't watch anything
- âœ… **Netflix approach**: Show "Popular Now" and "Recently Watched" instead

Result: Users barely notice the issue and continue watching content.

**Production Statistics:**

- **Monolithic failures**: Average 99.9% uptime (8.7 hours downtime/year)
- **Gracefully degraded systems**: 99.99% uptime (52 minutes downtime/year)
- **Cost difference**: 10x reduction in revenue loss during incidents

### Degradation Levels

Design systems with multiple fallback levels:

**Level 0: Full Functionality (Ideal)**
- All features working
- Best performance
- Complete user experience

**Level 1: Slight Degradation (Acceptable)**
- Cache responses (slightly stale data)
- Reduce refresh rates
- Disable non-critical features
- Users barely notice

**Level 2: Moderate Degradation (Functional)**
- Static content only
- Read-only mode
- Simplified UI
- Core functionality preserved

**Level 3: Minimal Degradation (Critical Only)**
- Essential features only
- Emergency data sources
- Maintenance page with status
- Users can still accomplish critical tasks

**Level 4: Full Failure (Unavoidable)**
- Graceful error messages
- Clear status communication
- Estimated recovery time
- Alternative contact methods

### Pattern Examples

#### âŒ Bad: All-or-Nothing Design

```python
async def get_incident_details(incident_id: str) -> dict:
    """BAD: Fails completely if any component is down"""

    # Get data from multiple services
    metrics = await metrics_service.get_metrics(incident_id)  # âŒ Hard dependency
    logs = await logs_service.get_logs(incident_id)          # âŒ Hard dependency
    alerts = await alerts_service.get_alerts(incident_id)    # âŒ Hard dependency

    # If ANY service fails, entire function fails
    return {
        'metrics': metrics,
        'logs': logs,
        'alerts': alerts
    }

# Problem: If logs service is down, you get NOTHING (not even metrics and alerts)
```

#### âœ… Good: Graceful Degradation with Fallbacks

```python
async def get_incident_details_graceful(incident_id: str) -> dict:
    """GOOD: Returns partial data when some services fail"""

    result = {
        'incident_id': incident_id,
        'status': 'full',  # Will downgrade if components fail
        'degraded_components': []
    }

    # Try to get metrics (with fallback)
    try:
        result['metrics'] = await metrics_service.get_metrics(incident_id)
    except Exception as e:
        logger.warning(f"Metrics service unavailable: {e}")
        result['metrics'] = {'error': 'Metrics temporarily unavailable'}
        result['degraded_components'].append('metrics')
        result['status'] = 'degraded'

    # Try to get logs (with fallback)
    try:
        result['logs'] = await logs_service.get_logs(incident_id)
    except Exception as e:
        logger.warning(f"Logs service unavailable: {e}")
        result['logs'] = {'error': 'Logs temporarily unavailable'}
        result['degraded_components'].append('logs')
        result['status'] = 'degraded'

    # Try to get alerts (with fallback)
    try:
        result['alerts'] = await alerts_service.get_alerts(incident_id)
    except Exception as e:
        logger.warning(f"Alerts service unavailable: {e}")
        result['alerts'] = {'error': 'Alerts temporarily unavailable'}
        result['degraded_components'].append('alerts')
        result['status'] = 'degraded'

    # If EVERYTHING failed, escalate
    if len(result['degraded_components']) == 3:
        result['status'] = 'critical'
        raise ServiceDegradationError("All incident services unavailable")

    return result

# Result: Even if 2/3 services are down, you still get useful data
```

### Production Implementation

**Complete Graceful Degradation Framework:**

```python
"""
Production-ready graceful degradation framework for agents.

Installation:
    pip install aiohttp redis  # For caching fallbacks

Usage:
    degraded_agent = DegradableAgent(
        degradation_strategy=DegradationStrategy.CACHE_FALLBACK
    )
    result = await degraded_agent.execute_with_fallbacks(action, params)
"""

import asyncio
from typing import Optional, Dict, Any, Callable, List
from enum import Enum
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DegradationLevel(Enum):
    """System degradation levels"""
    FULL = "full"                    # All features working
    SLIGHT = "slight"                # Minor degradation
    MODERATE = "moderate"            # Significant degradation
    MINIMAL = "minimal"              # Critical features only
    FAILED = "failed"                # Complete failure


class DegradationStrategy(Enum):
    """Strategies for handling degradation"""
    CACHE_FALLBACK = "cache_fallback"      # Use cached data
    STATIC_FALLBACK = "static_fallback"    # Use static defaults
    PARTIAL_RESPONSE = "partial_response"  # Return what's available
    ALTERNATE_SOURCE = "alternate_source"  # Try backup data source
    FAIL_FAST = "fail_fast"                # Don't degrade, fail immediately


@dataclass
class DegradationConfig:
    """Configuration for degradation behavior"""
    strategy: DegradationStrategy = DegradationStrategy.CACHE_FALLBACK

    # Cache TTL for fallback data
    cache_ttl: timedelta = timedelta(hours=1)

    # Maximum acceptable degradation level before failing
    max_degradation: DegradationLevel = DegradationLevel.MODERATE

    # Component weights (for calculating overall degradation)
    component_weights: Dict[str, float] = field(default_factory=dict)


@dataclass
class ComponentResult:
    """Result from a single component"""
    name: str
    success: bool
    data: Any
    error: Optional[str] = None
    cached: bool = False
    degraded: bool = False


class ServiceDegradationError(Exception):
    """Raised when degradation exceeds acceptable level"""
    pass


class DegradableAgent:
    """
    Agent that gracefully degrades when components fail.

    Based on Netflix's Hystrix patterns for resilient systems.
    """

    def __init__(self, config: Optional[DegradationConfig] = None):
        self.config = config or DegradationConfig()
        self.cache: Dict[str, Any] = {}  # Simple in-memory cache
        self.cache_timestamps: Dict[str, datetime] = {}
        self.component_health: Dict[str, bool] = {}

    def _is_cache_valid(self, key: str) -> bool:
        """Check if cached data is still valid"""
        if key not in self.cache_timestamps:
            return False

        age = datetime.now() - self.cache_timestamps[key]
        return age < self.config.cache_ttl

    def _cache_get(self, key: str) -> Optional[Any]:
        """Get from cache if valid"""
        if self._is_cache_valid(key):
            logger.info(f"ðŸ“¦ Using cached data for {key}")
            return self.cache[key]
        return None

    def _cache_set(self, key: str, value: Any):
        """Store in cache"""
        self.cache[key] = value
        self.cache_timestamps[key] = datetime.now()

    async def _execute_component(
        self,
        name: str,
        func: Callable,
        *args,
        **kwargs
    ) -> ComponentResult:
        """
        Execute a single component with fallback logic.
        """
        cache_key = f"{name}:{args}:{kwargs}"

        try:
            # Try primary execution
            result = await func(*args, **kwargs)

            # Cache successful result
            self._cache_set(cache_key, result)
            self.component_health[name] = True

            return ComponentResult(
                name=name,
                success=True,
                data=result,
                cached=False,
                degraded=False
            )

        except Exception as e:
            logger.warning(f"âš ï¸  Component '{name}' failed: {e}")
            self.component_health[name] = False

            # Try degradation strategy
            if self.config.strategy == DegradationStrategy.CACHE_FALLBACK:
                cached_data = self._cache_get(cache_key)
                if cached_data is not None:
                    return ComponentResult(
                        name=name,
                        success=True,
                        data=cached_data,
                        cached=True,
                        degraded=True
                    )

            elif self.config.strategy == DegradationStrategy.STATIC_FALLBACK:
                static_data = self._get_static_fallback(name)
                if static_data is not None:
                    return ComponentResult(
                        name=name,
                        success=True,
                        data=static_data,
                        degraded=True
                    )

            elif self.config.strategy == DegradationStrategy.PARTIAL_RESPONSE:
                # Return error but don't fail
                return ComponentResult(
                    name=name,
                    success=False,
                    data=None,
                    error=str(e),
                    degraded=True
                )

            elif self.config.strategy == DegradationStrategy.FAIL_FAST:
                # Don't degrade, fail immediately
                raise

            # Default: return failure
            return ComponentResult(
                name=name,
                success=False,
                data=None,
                error=str(e),
                degraded=True
            )

    def _get_static_fallback(self, component_name: str) -> Optional[Any]:
        """Get static fallback data for component"""
        # In production, this would load from config or database
        static_fallbacks = {
            'metrics': {'cpu': 'unknown', 'memory': 'unknown'},
            'logs': {'message': 'Logs temporarily unavailable'},
            'alerts': []
        }
        return static_fallbacks.get(component_name)

    def _calculate_degradation_level(
        self,
        results: List[ComponentResult]
    ) -> DegradationLevel:
        """Calculate overall system degradation level"""

        if not results:
            return DegradationLevel.FAILED

        # Count successes, failures, cached, degraded
        total = len(results)
        successes = sum(1 for r in results if r.success)
        cached = sum(1 for r in results if r.cached)
        degraded = sum(1 for r in results if r.degraded)

        # Calculate success rate
        success_rate = successes / total
        degradation_rate = degraded / total
        cache_rate = cached / total

        # Determine level based on rates
        if success_rate == 1.0 and cached == 0:
            return DegradationLevel.FULL

        elif success_rate >= 0.8 and cache_rate < 0.3:
            return DegradationLevel.SLIGHT

        elif success_rate >= 0.6 or cache_rate >= 0.3:
            return DegradationLevel.MODERATE

        elif success_rate >= 0.3:
            return DegradationLevel.MINIMAL

        else:
            return DegradationLevel.FAILED

    async def execute_with_fallbacks(
        self,
        components: Dict[str, Callable],
        *args,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Execute multiple components with graceful degradation.

        Args:
            components: Dictionary of {component_name: async_function}
            *args, **kwargs: Arguments passed to each component function

        Returns:
            Dictionary with results and degradation status

        Raises:
            ServiceDegradationError: If degradation exceeds max_degradation level
        """

        # Execute all components in parallel
        tasks = {
            name: self._execute_component(name, func, *args, **kwargs)
            for name, func in components.items()
        }

        results = await asyncio.gather(*tasks.values(), return_exceptions=False)
        results_list = list(results)

        # Calculate degradation level
        degradation = self._calculate_degradation_level(results_list)

        # Check if degradation is acceptable
        degradation_hierarchy = {
            DegradationLevel.FULL: 0,
            DegradationLevel.SLIGHT: 1,
            DegradationLevel.MODERATE: 2,
            DegradationLevel.MINIMAL: 3,
            DegradationLevel.FAILED: 4
        }

        if degradation_hierarchy[degradation] > degradation_hierarchy[self.config.max_degradation]:
            failed_components = [r.name for r in results_list if not r.success]
            raise ServiceDegradationError(
                f"System degradation {degradation.value} exceeds maximum "
                f"{self.config.max_degradation.value}. "
                f"Failed components: {failed_components}"
            )

        # Build response
        response = {
            'degradation_level': degradation.value,
            'components': {},
            'metadata': {
                'total_components': len(results_list),
                'successful_components': sum(1 for r in results_list if r.success),
                'cached_responses': sum(1 for r in results_list if r.cached),
                'degraded_components': [r.name for r in results_list if r.degraded]
            }
        }

        # Add component results
        for result in results_list:
            response['components'][result.name] = {
                'success': result.success,
                'data': result.data,
                'cached': result.cached,
                'degraded': result.degraded,
                'error': result.error
            }

        return response


# ============================================================
# DEMONSTRATION: Graceful Degradation in Action
# ============================================================

# Simulate external services
class SimulatedService:
    """Simulates a flaky external service"""

    def __init__(self, name: str, failure_rate: float = 0.0):
        self.name = name
        self.failure_rate = failure_rate
        self.call_count = 0

    async def call(self, *args, **kwargs) -> Dict:
        """Simulate service call that might fail"""
        self.call_count += 1

        import random
        if random.random() < self.failure_rate:
            raise ConnectionError(f"{self.name} service unavailable")

        await asyncio.sleep(0.1)  # Simulate network delay
        return {
            'service': self.name,
            'data': f'Response from {self.name}',
            'call_count': self.call_count
        }


async def demo_graceful_degradation():
    """Demonstrate graceful degradation with failing services"""

    print("=" * 70)
    print("GRACEFUL DEGRADATION DEMONSTRATION")
    print("=" * 70)
    print()

    # Create services with different failure rates
    metrics_service = SimulatedService("Metrics", failure_rate=0.3)  # 30% failure
    logs_service = SimulatedService("Logs", failure_rate=0.5)        # 50% failure
    alerts_service = SimulatedService("Alerts", failure_rate=0.1)    # 10% failure

    # Create degradable agent
    agent = DegradableAgent(
        config=DegradationConfig(
            strategy=DegradationStrategy.CACHE_FALLBACK,
            max_degradation=DegradationLevel.MODERATE
        )
    )

    # Run multiple attempts to see degradation in action
    for attempt in range(5):
        print(f"\n{'='*70}")
        print(f"Attempt {attempt + 1}")
        print(f"{'='*70}\n")

        try:
            result = await agent.execute_with_fallbacks(
                components={
                    'metrics': metrics_service.call,
                    'logs': logs_service.call,
                    'alerts': alerts_service.call
                },
                incident_id='INC-12345'
            )

            print(f"âœ“ Request succeeded with degradation level: {result['degradation_level']}")
            print(f"  Successful: {result['metadata']['successful_components']}/{result['metadata']['total_components']}")
            print(f"  Cached responses: {result['metadata']['cached_responses']}")
            print(f"  Degraded components: {result['metadata']['degraded_components']}")

            # Show component details
            for name, component in result['components'].items():
                status = "âœ“" if component['success'] else "âŒ"
                cache_indicator = " (cached)" if component['cached'] else ""
                print(f"  {status} {name}: {component['data']}{cache_indicator}")

        except ServiceDegradationError as e:
            print(f"âŒ Request failed: {e}")

        await asyncio.sleep(1)


async def demo_degradation_levels():
    """Demonstrate different degradation levels"""

    print("\n" + "=" * 70)
    print("DEGRADATION LEVELS DEMONSTRATION")
    print("=" * 70)
    print()

    configs = [
        ("Permissive", DegradationLevel.MINIMAL),
        ("Moderate", DegradationLevel.MODERATE),
        ("Strict", DegradationLevel.SLIGHT),
    ]

    for config_name, max_degradation in configs:
        print(f"\n{config_name} Configuration (max: {max_degradation.value}):")

        # Create highly unreliable services
        metrics_service = SimulatedService("Metrics", failure_rate=0.7)
        logs_service = SimulatedService("Logs", failure_rate=0.7)

        agent = DegradableAgent(
            config=DegradationConfig(
                strategy=DegradationStrategy.PARTIAL_RESPONSE,
                max_degradation=max_degradation
            )
        )

        try:
            result = await agent.execute_with_fallbacks(
                components={
                    'metrics': metrics_service.call,
                    'logs': logs_service.call,
                }
            )
            print(f"  âœ“ Accepted degradation: {result['degradation_level']}")
        except ServiceDegradationError as e:
            print(f"  âŒ Rejected: {e}")


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("GRACEFUL DEGRADATION - DEMONSTRATION")
    print("=" * 70)

    asyncio.run(demo_graceful_degradation())
    asyncio.run(demo_degradation_levels())

    print("\n" + "=" * 70)
    print("KEY TAKEAWAYS")
    print("=" * 70)
    print()
    print("âœ“ Design for degradation from the start")
    print("âœ“ Cache successful responses for fallbacks")
    print("âœ“ Return partial data rather than complete failure")
    print("âœ“ Define acceptable degradation levels for your use case")
    print("âœ“ Monitor and alert on degraded components")
    print("âœ“ Test degradation scenarios regularly (chaos engineering)")
    print()
```

**Expected Output:**

```
======================================================================
GRACEFUL DEGRADATION DEMONSTRATION
======================================================================

======================================================================
Attempt 1
======================================================================

âœ“ Request succeeded with degradation level: slight
  Successful: 3/3
  Cached responses: 0
  Degraded components: []
  âœ“ metrics: {'service': 'Metrics', 'data': 'Response from Metrics', 'call_count': 1}
  âœ“ logs: {'service': 'Logs', 'data': 'Response from Logs', 'call_count': 1}
  âœ“ alerts: {'service': 'Alerts', 'data': 'Response from Alerts', 'call_count': 1}

======================================================================
Attempt 2
======================================================================

âš ï¸  Component 'logs' failed: Logs service unavailable
ðŸ“¦ Using cached data for logs:('INC-12345',):{}
âœ“ Request succeeded with degradation level: slight
  Successful: 3/3
  Cached responses: 1
  Degraded components: ['logs']
  âœ“ metrics: {'service': 'Metrics', 'data': 'Response from Metrics', 'call_count': 2}
  âœ“ logs: {'service': 'Logs', 'data': 'Response from Logs', 'call_count': 1} (cached)
  âœ“ alerts: {'service': 'Alerts', 'data': 'Response from Alerts', 'call_count': 2}

======================================================================
Attempt 3
======================================================================

âš ï¸  Component 'metrics' failed: Metrics service unavailable
ðŸ“¦ Using cached data for metrics:('INC-12345',):{}
âœ“ Request succeeded with degradation level: moderate
  Successful: 3/3
  Cached responses: 2
  Degraded components: ['metrics', 'logs']
  âœ“ metrics: {'service': 'Metrics', 'data': 'Response from Metrics', 'call_count': 2} (cached)
  âœ“ logs: {'service': 'Logs', 'data': 'Response from Logs', 'call_count': 2}
  âœ“ alerts: {'service': 'Alerts', 'data': 'Response from Alerts', 'call_count': 3}
```

### When to Use Graceful Degradation

âœ… **Use graceful degradation for:**

- **User-facing applications** (never show blank error pages)
- **Multi-component systems** (microservices architectures)
- **Real-time dashboards** (better to show stale data than no data)
- **Critical business processes** (e-commerce checkout, payments)
- **SLA-critical services** (where uptime is contractual)

âŒ **Don't degrade when:**

- **Data accuracy is critical** (financial transactions, medical records)
- **Stale data is dangerous** (stock trading, real-time safety systems)
- **Compliance requires fresh data** (regulatory reporting)
- **User explicitly requested refresh** (pull-to-refresh should fail, not show cache)

### Best Practices

1. **Cache Aggressively**
   - Cache successful responses for fallback use
   - Set appropriate TTL based on data staleness tolerance
   - Clear cache when known to be invalid

2. **Communicate Degradation**
   - Show users when data is cached/stale
   - Display "âš ï¸ Limited functionality" banners
   - Provide "Try again" options

3. **Monitor Degradation**
   - Track degradation events
   - Alert when specific components frequently degrade
   - Measure time spent in degraded states

4. **Test Degradation**
   - Use chaos engineering (randomly fail components)
   - Test all degradation levels regularly
   - Ensure users can still complete critical tasks

5. **Define SLOs for Degradation**
   - "99% of requests return full data"
   - "<5% of requests use cached fallbacks"
   - "No degradation lasts >5 minutes"

---

## 6. Self-Healing Architectures

### What is Self-Healing?

**Self-healing** means systems automatically detect problems and take corrective action without human intervention. It combines all the patterns we've learned (circuit breakers, retries, idempotency, checkpoints, degradation) into an autonomous recovery system.

**Philosophy:**

> "The best on-call engineer is one who is never paged."

**Analogy:**

Think of your immune system:

âŒ **No self-healing**: Every cut or infection requires a doctor visit
âœ… **Self-healing**: Body automatically clots blood, fights infections, heals wounds

Or modern cars:

âŒ **Old cars**: Check engine light â†’ manual diagnosis â†’ mechanic visit
âœ… **Self-healing cars**: Tire pressure low â†’ auto-adjust, engine overheating â†’ reduce power

### Why It Matters

**Real-World Impact:**

**Google's Borg** (predecessor to Kubernetes) has self-healing at its core:
- Pod crashes â†’ Automatically restarts
- Node fails â†’ Reschedules pods to healthy nodes
- Health checks fail â†’ Replaces unhealthy instances
- Resource exhaustion â†’ Evicts low-priority workloads

Result: Google runs millions of containers with minimal manual intervention.

**Production Statistics:**

- **Manual recovery**: Average 30 minutes MTTR (Mean Time To Recovery)
- **Semi-automated**: Average 5 minutes MTTR
- **Fully self-healing**: Average 30 seconds MTTR

**Cost Impact:**
- 60x faster recovery = 60x less downtime cost
- Reduced on-call burden = happier engineers
- Fewer human errors during recovery

### Self-Healing Patterns

**Pattern 1: Health Checks + Auto-Restart**
- Continuously monitor component health
- Detect failures through health checks
- Automatically restart failing components
- Track restart frequency (prevent infinite loops)

**Pattern 2: Automatic Fallback Switching**
- Maintain primary and backup data sources
- Health check both continuously
- Automatically switch to backup when primary fails
- Switch back when primary recovers

**Pattern 3: Automatic Scaling**
- Monitor resource utilization (CPU, memory, queue depth)
- Scale up when overwhelmed
- Scale down when idle
- Prevent thrashing with cooldown periods

**Pattern 4: Self-Correcting State**
- Detect state corruption or inconsistency
- Automatically reconcile from source of truth
- Trigger repair workflows
- Log and alert for investigation

### Pattern Examples

#### âŒ Bad: No Self-Healing (Manual Recovery)

```python
async def process_incidents():
    """BAD: Crashes and requires manual restart"""

    while True:
        incident = await get_next_incident()

        # If this fails, process crashes and stays down âŒ
        await analyze_incident(incident)
        await apply_fix(incident)

        # No health checks, no auto-restart, no recovery
```

#### âœ… Good: Self-Healing with Auto-Recovery

```python
async def process_incidents_with_healing():
    """GOOD: Automatically recovers from failures"""

    consecutive_failures = 0
    max_consecutive_failures = 5

    while True:
        try:
            # Health check before processing
            if not await health_check():
                await self_heal()  # Attempt recovery
                continue

            incident = await get_next_incident()
            await analyze_incident(incident)
            await apply_fix(incident)

            # Reset failure counter on success
            consecutive_failures = 0

        except Exception as e:
            consecutive_failures += 1
            logger.error(f"Processing failed: {e}")

            if consecutive_failures >= max_consecutive_failures:
                # Too many failures, escalate to humans
                await alert_on_call("Agent stuck, needs human intervention")
                break
            else:
                # Attempt self-healing
                await self_heal()
                await asyncio.sleep(exponential_backoff(consecutive_failures))


async def self_heal():
    """Self-healing actions"""
    logger.info("ðŸ”§ Initiating self-heal...")

    # Clear caches
    cache.clear()

    # Reset connections
    await db_pool.reset_all()

    # Reload configuration
    config.reload()

    # Run health check
    if await health_check():
        logger.info("âœ“ Self-heal successful")
    else:
        logger.warning("âš ï¸ Self-heal failed, may need human intervention")
```

### Production Implementation

**Complete Self-Healing Agent Framework:**

```python
"""
Production-ready self-healing agent framework.

Installation:
    pip install asyncio psutil aiohttp

Usage:
    agent = SelfHealingAgent(
        health_check_interval=30,
        max_consecutive_failures=5
    )
    await agent.run()
"""

import asyncio
import time
from typing import Optional, Callable, Dict, Any, List
from dataclasses import dataclass
from enum import Enum
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HealthStatus(Enum):
    """Health status levels"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    CRITICAL = "critical"


@dataclass
class HealthCheckResult:
    """Result of a health check"""
    status: HealthStatus
    checks: Dict[str, bool]  # Individual check results
    message: str
    timestamp: datetime


@dataclass
class SelfHealingConfig:
    """Configuration for self-healing behavior"""
    health_check_interval: float = 30.0  # Seconds between health checks
    max_consecutive_failures: int = 5    # Max failures before escalation
    restart_cooldown: float = 60.0       # Min time between restarts
    enable_auto_restart: bool = True      # Enable automatic restarts
    enable_auto_failover: bool = True     # Enable automatic failover
    escalation_callback: Optional[Callable] = None  # Called on escalation


class SelfHealingAgent:
    """
    Self-healing agent that automatically recovers from failures.

    Based on Google SRE principles and Kubernetes design patterns.
    """

    def __init__(self, config: Optional[SelfHealingConfig] = None):
        self.config = config or SelfHealingConfig()
        self.consecutive_failures = 0
        self.last_restart_time = datetime.min
        self.health_history: List[HealthCheckResult] = []
        self.is_running = False
        self.primary_backend_healthy = True
        self.using_backup_backend = False

    async def _health_check(self) -> HealthCheckResult:
        """
        Perform comprehensive health check.

        Returns:
            HealthCheckResult with status and individual check results
        """
        checks = {}

        # Check 1: Memory usage
        try:
            import psutil
            memory = psutil.virtual_memory()
            checks['memory'] = memory.percent < 90
        except Exception as e:
            logger.warning(f"Memory check failed: {e}")
            checks['memory'] = False

        # Check 2: Connectivity (simulate)
        try:
            # In production: check database, APIs, etc.
            checks['connectivity'] = True  # Placeholder
        except Exception as e:
            checks['connectivity'] = False

        # Check 3: Response time (simulate)
        try:
            start = time.time()
            # Simulate a quick operation
            await asyncio.sleep(0.001)
            elapsed = time.time() - start
            checks['response_time'] = elapsed < 1.0
        except Exception:
            checks['response_time'] = False

        # Check 4: Queue depth (simulate)
        try:
            # In production: check message queue depth
            queue_depth = 0  # Placeholder
            checks['queue_depth'] = queue_depth < 1000
        except Exception:
            checks['queue_depth'] = False

        # Calculate overall status
        healthy_count = sum(1 for v in checks.values() if v)
        total_checks = len(checks)

        if healthy_count == total_checks:
            status = HealthStatus.HEALTHY
            message = "All systems operational"
        elif healthy_count >= total_checks * 0.75:
            status = HealthStatus.DEGRADED
            message = f"{healthy_count}/{total_checks} checks passing"
        elif healthy_count >= total_checks * 0.50:
            status = HealthStatus.UNHEALTHY
            message = f"Multiple failures: {healthy_count}/{total_checks} checks passing"
        else:
            status = HealthStatus.CRITICAL
            message = f"Critical failures: {healthy_count}/{total_checks} checks passing"

        return HealthCheckResult(
            status=status,
            checks=checks,
            message=message,
            timestamp=datetime.now()
        )

    async def _attempt_self_heal(self, health_result: HealthCheckResult):
        """
        Attempt automatic recovery based on failed health checks.

        Args:
            health_result: Result from health check showing what failed
        """
        logger.info(f"ðŸ”§ Attempting self-heal (status: {health_result.status.value})")

        healing_actions = []

        # Healing action 1: Clear caches if memory is high
        if not health_result.checks.get('memory', True):
            logger.info("  â†’ Clearing caches (high memory usage)")
            # In production: clear application caches
            healing_actions.append("cleared_caches")
            await asyncio.sleep(0.1)  # Simulate cache clear

        # Healing action 2: Reset connections if connectivity fails
        if not health_result.checks.get('connectivity', True):
            logger.info("  â†’ Resetting connections")
            # In production: reset database connection pools, etc.
            healing_actions.append("reset_connections")
            await asyncio.sleep(0.1)  # Simulate connection reset

        # Healing action 3: Reduce load if response time is slow
        if not health_result.checks.get('response_time', True):
            logger.info("  â†’ Reducing load (slow response times)")
            # In production: decrease concurrency, reject new requests temporarily
            healing_actions.append("reduced_load")
            await asyncio.sleep(0.1)

        # Healing action 4: Scale up if queue is backing up
        if not health_result.checks.get('queue_depth', True):
            logger.info("  â†’ Requesting scale up (queue backlog)")
            # In production: trigger auto-scaling
            healing_actions.append("requested_scale_up")

        logger.info(f"âœ“ Self-heal completed: {healing_actions}")

    async def _should_restart(self) -> bool:
        """Check if automatic restart is needed and allowed"""
        if not self.config.enable_auto_restart:
            return False

        # Check cooldown period
        time_since_last_restart = datetime.now() - self.last_restart_time
        if time_since_last_restart < timedelta(seconds=self.config.restart_cooldown):
            logger.warning(f"Restart on cooldown ({self.config.restart_cooldown}s)")
            return False

        return True

    async def _restart_self(self):
        """Restart the agent (simulate)"""
        logger.warning("ðŸ”„ Initiating self-restart...")
        self.last_restart_time = datetime.now()

        # In production, this might:
        # - Save current state
        # - Close connections gracefully
        # - Exit with specific code that triggers restart by supervisor
        # For demo, we'll just reset counters

        self.consecutive_failures = 0
        logger.info("âœ“ Restart completed")

    async def _failover_to_backup(self):
        """Switch to backup backend"""
        if not self.config.enable_auto_failover:
            return

        if self.using_backup_backend:
            logger.warning("Already using backup backend")
            return

        logger.warning("ðŸ”€ Failing over to backup backend...")
        self.using_backup_backend = True
        self.primary_backend_healthy = False

        # In production:
        # - Update connection strings
        # - Switch DNS/load balancer
        # - Update service discovery

        logger.info("âœ“ Failover completed, using backup backend")

    async def _failback_to_primary(self):
        """Switch back to primary backend when recovered"""
        if not self.using_backup_backend:
            return

        logger.info("ðŸ”™ Failing back to primary backend...")
        self.using_backup_backend = False
        self.primary_backend_healthy = True

        logger.info("âœ“ Failback completed, using primary backend")

    async def _escalate_to_humans(self):
        """Escalate issue to on-call engineer"""
        logger.error("ðŸš¨ ESCALATING TO HUMANS")
        logger.error(f"   Consecutive failures: {self.consecutive_failures}")
        logger.error(f"   Using backup: {self.using_backup_backend}")

        # Show recent health history
        logger.error("   Recent health checks:")
        for check in self.health_history[-5:]:
            logger.error(f"     {check.timestamp}: {check.status.value} - {check.message}")

        if self.config.escalation_callback:
            await self.config.escalation_callback({
                'consecutive_failures': self.consecutive_failures,
                'using_backup': self.using_backup_backend,
                'health_history': [
                    {
                        'timestamp': str(check.timestamp),
                        'status': check.status.value,
                        'message': check.message,
                        'checks': check.checks
                    }
                    for check in self.health_history[-10:]
                ]
            })

    async def run(self):
        """
        Main agent loop with self-healing.

        This demonstrates continuous operation with automatic recovery.
        """
        self.is_running = True
        logger.info("ðŸš€ Self-healing agent started")

        iteration = 0

        while self.is_running:
            iteration += 1

            try:
                # Perform health check
                health_result = await self._health_check()
                self.health_history.append(health_result)

                # Keep only last 100 checks
                if len(self.health_history) > 100:
                    self.health_history = self.health_history[-100:]

                logger.info(f"Health check #{iteration}: {health_result.status.value} - {health_result.message}")

                # React based on health status
                if health_result.status == HealthStatus.HEALTHY:
                    # All good
                    self.consecutive_failures = 0

                    # If we were using backup, try to failback
                    if self.using_backup_backend:
                        await self._failback_to_primary()

                elif health_result.status == HealthStatus.DEGRADED:
                    # Minor issues, attempt self-heal
                    logger.warning(f"âš ï¸ System degraded, attempting self-heal...")
                    await self._attempt_self_heal(health_result)

                elif health_result.status == HealthStatus.UNHEALTHY:
                    # Serious issues
                    self.consecutive_failures += 1
                    logger.warning(f"âŒ System unhealthy (failure {self.consecutive_failures}/{self.config.max_consecutive_failures})")

                    # Try self-heal
                    await self._attempt_self_heal(health_result)

                    # Consider failover
                    if not self.using_backup_backend:
                        await self._failover_to_backup()

                elif health_result.status == HealthStatus.CRITICAL:
                    # Critical issues
                    self.consecutive_failures += 1
                    logger.error(f"ðŸ’€ System critical (failure {self.consecutive_failures}/{self.config.max_consecutive_failures})")

                    # Try everything
                    await self._attempt_self_heal(health_result)
                    await self._failover_to_backup()

                    # Consider restart
                    if await self._should_restart():
                        await self._restart_self()

                # Check if we should escalate
                if self.consecutive_failures >= self.config.max_consecutive_failures:
                    await self._escalate_to_humans()
                    self.is_running = False
                    break

                # Simulate some work
                await asyncio.sleep(0.5)

                # Wait until next health check
                await asyncio.sleep(self.config.health_check_interval)

            except Exception as e:
                logger.error(f"Unexpected error in agent loop: {e}")
                self.consecutive_failures += 1

                if self.consecutive_failures >= self.config.max_consecutive_failures:
                    await self._escalate_to_humans()
                    self.is_running = False
                    break

        logger.info("Self-healing agent stopped")


# ============================================================
# DEMONSTRATION: Self-Healing in Action
# ============================================================

async def simulate_failure_injection():
    """
    Simulate intermittent failures to demonstrate self-healing.

    This would typically be done with chaos engineering tools like
    Chaos Monkey or Gremlin in production.
    """
    import random

    # Simulate system degradation after 3 iterations
    await asyncio.sleep(3 * 31)  # Wait for 3 health check cycles

    logger.warning("ðŸ’¥ SIMULATING FAILURE (for demo purposes)")

    # In production, chaos engineering would:
    # - Kill random pods
    # - Inject network latency
    # - Fill up disk space
    # - Simulate database connection failures
    # etc.


async def demo_self_healing():
    """Demonstrate self-healing agent"""

    print("=" * 70)
    print("SELF-HEALING AGENT DEMONSTRATION")
    print("=" * 70)
    print()
    print("Watch as the agent automatically:")
    print("  - Monitors its own health")
    print("  - Detects degradation")
    print("  - Attempts self-healing")
    print("  - Fails over to backup if needed")
    print("  - Escalates if all recovery attempts fail")
    print()

    async def on_escalation(context: Dict):
        """Called when agent escalates to humans"""
        print("\nðŸ“ž ON-CALL ENGINEER PAGED:")
        print(f"   Failures: {context['consecutive_failures']}")
        print(f"   Using backup: {context['using_backup']}")

    config = SelfHealingConfig(
        health_check_interval=10.0,  # Check every 10 seconds (fast for demo)
        max_consecutive_failures=3,   # Escalate after 3 failures
        restart_cooldown=30.0,
        escalation_callback=on_escalation
    )

    agent = SelfHealingAgent(config)

    # Run agent (would run forever in production)
    try:
        # Start agent
        agent_task = asyncio.create_task(agent.run())

        # Optionally inject failures for demo
        # failure_task = asyncio.create_task(simulate_failure_injection())

        # Wait for agent to finish (or timeout for demo)
        await asyncio.wait_for(agent_task, timeout=120.0)

    except asyncio.TimeoutError:
        print("\nâ± Demo timeout reached, stopping agent...")
        agent.is_running = False


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("SELF-HEALING ARCHITECTURES - DEMONSTRATION")
    print("=" * 70)

    asyncio.run(demo_self_healing())

    print("\n" + "=" * 70)
    print("KEY TAKEAWAYS")
    print("=" * 70)
    print()
    print("âœ“ Implement continuous health checks")
    print("âœ“ Automate common recovery actions (restart, clear cache, etc.)")
    print("âœ“ Use failover for redundancy")
    print("âœ“ Set limits on automatic recovery attempts")
    print("âœ“ Always escalate to humans when self-healing fails")
    print("âœ“ Log all healing actions for postmortem analysis")
    print("âœ“ Test self-healing regularly with chaos engineering")
    print()
```

**Expected Output:**

```
======================================================================
SELF-HEALING AGENT DEMONSTRATION
======================================================================

Watch as the agent automatically:
  - Monitors its own health
  - Detects degradation
  - Attempts self-healing
  - Fails over to backup if needed
  - Escalates if all recovery attempts fail

ðŸš€ Self-healing agent started
Health check #1: healthy - All systems operational
Health check #2: healthy - All systems operational
Health check #3: degraded - 3/4 checks passing
âš ï¸ System degraded, attempting self-heal...
ðŸ”§ Attempting self-heal (status: degraded)
  â†’ Resetting connections
âœ“ Self-heal completed: ['reset_connections']
Health check #4: healthy - All systems operational
Health check #5: healthy - All systems operational
```

### When to Implement Self-Healing

âœ… **Implement self-healing for:**

- **Production services** (especially customer-facing)
- **High-availability requirements** (99.9%+ uptime)
- **Known failure modes** (transient issues that are safe to auto-recover)
- **24/7 operations** (reduce on-call burden)
- **Large-scale systems** (too many components to manually monitor)

âŒ **Be cautious with self-healing when:**

- **Root cause unknown** (might mask serious issues)
- **Data integrity risks** (auto-restart could corrupt state)
- **Compliance requirements** (some industries require manual approval)
- **Learning phase** (new systems need human observation first)

### Self-Healing Best Practices

1. **Start Conservative**
   - Begin with manual recovery, observe patterns
   - Automate only well-understood failures
   - Gradually expand automation scope

2. **Set Clear Boundaries**
   - Limit restart frequency (prevent infinite restart loops)
   - Maximum auto-recovery attempts before escalation
   - Cooldown periods between actions

3. **Monitor and Alert**
   - Log every self-healing action
   - Alert on frequent healing (indicates systemic issue)
   - Track healing success rate

4. **Test Regularly**
   - Use chaos engineering to verify healing works
   - GameDays: simulate outages and test recovery
   - Verify escalation paths work

5. **Human Oversight**
   - Always have escalation to humans
   - Never fully autonomous for critical systems
   - Review healing logs regularly

### Integration with Previous Patterns

Self-healing combines all the patterns we've learned:

| Pattern | Role in Self-Healing |
|---------|---------------------|
| **Circuit Breaker** | Prevents cascading failures during healing |
| **Exponential Backoff** | Used for retry timing during recovery |
| **Idempotency** | Ensures healing actions are safe to retry |
| **Checkpointing** | Allows resuming after restart |
| **Graceful Degradation** | Provides fallback during healing |

**Example Integration:**

```python
async def self_healing_incident_processor():
    """Combines all resilience patterns"""

    # Circuit breaker protects downstream services
    circuit_breaker = CircuitBreaker()

    # Retry strategy for transient failures
    retry = RetryStrategy(jitter_type=JitterType.FULL)

    # Idempotent actions prevent duplicates
    idempotent_agent = IdempotentAgent()

    # Checkpoints for crash recovery
    checkpointed_agent = CheckpointedAgent()

    # Graceful degradation for partial failures
    degradable_agent = DegradableAgent()

    # Self-healing orchestrates everything
    self_healing = SelfHealingAgent()

    # Result: Resilient system that handles failures at multiple levels
```

---

## 7. Real-World Scenario: Building a Production-Ready Incident Response Agent

### Scenario Overview

You're building an **AI-powered incident response agent** for a SaaS company that processes 10,000 transactions per second. The agent must:

- Monitor infrastructure health 24/7
- Automatically respond to incidents
- Escalate to humans when necessary
- Never cause worse problems than it fixes
- Meet 99.99% uptime SLA

**Business Constraints:**
- Downtime costs $10,000 per minute
- Manual incident response averages 15 minutes
- On-call engineers are expensive and tired
- Some failures are transient (70%), some permanent (30%)

### Architecture Requirements

**Resilience Patterns Needed:**

1. **Circuit Breaker** - Protect external services (Datadog, PagerDuty, AWS APIs)
2. **Exponential Backoff** - Retry transient failures without overwhelming systems
3. **Idempotency** - Ensure remediation actions aren't duplicated
4. **State Checkpointing** - Resume workflows after agent crashes
5. **Graceful Degradation** - Operate with partial data when services are down
6. **Self-Healing** - Automatically recover from common failures

### Complete Production Implementation

```python
"""
Production-ready incident response agent with all resilience patterns.

This demonstrates how all 6 patterns work together in a real system.

Installation:
    pip install asyncio aiohttp prometheus-client psutil

Usage:
    agent = ProductionIncidentAgent()
    await agent.start()
"""

import asyncio
import hashlib
import json
from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================
# Import all our resilience components
# ============================================================

# In production, these would be: from .patterns import ...
# For this demo, we'll define minimal versions inline

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"


class JitterType(Enum):
    FULL = "full"


@dataclass
class IncidentContext:
    """Complete context for an incident"""
    incident_id: str
    severity: str  # critical, high, medium, low
    service: str
    description: str
    metrics: Optional[Dict] = None
    logs: Optional[Dict] = None
    alerts: Optional[Dict] = None
    remediation_attempts: List[str] = field(default_factory=list)
    checkpoint_step: int = 0
    created_at: datetime = field(default_factory=datetime.now)


class ProductionIncidentAgent:
    """
    Production-ready incident response agent.

    Combines all 6 resilience patterns for robust incident handling.
    """

    def __init__(self):
        # Circuit breakers for external services
        self.datadog_circuit = self._create_circuit_breaker("Datadog")
        self.pagerduty_circuit = self._create_circuit_breaker("PagerDuty")
        self.aws_circuit = self._create_circuit_breaker("AWS")

        # Retry strategies with exponential backoff
        self.retry_strategy = self._create_retry_strategy()

        # Idempotency tracking
        self.completed_actions: Dict[str, Any] = {}

        # State checkpointing
        self.checkpoints: Dict[str, IncidentContext] = {}

        # Graceful degradation caches
        self.metrics_cache: Dict[str, Any] = {}
        self.logs_cache: Dict[str, Any] = {}

        # Self-healing state
        self.consecutive_failures = 0
        self.max_consecutive_failures = 5
        self.is_healthy = True

        logger.info("ðŸš€ Production incident agent initialized")

    def _create_circuit_breaker(self, name: str):
        """Create a circuit breaker for a service"""
        # Simplified for demo - in production use full CircuitBreaker class
        return {
            'name': name,
            'state': CircuitState.CLOSED,
            'failures': 0,
            'last_failure_time': None
        }

    def _create_retry_strategy(self):
        """Create retry strategy with exponential backoff"""
        # Simplified for demo
        return {
            'base_delay': 1.0,
            'max_delay': 32.0,
            'max_retries': 3
        }

    # ========================================
    # Pattern 1: Circuit Breaker
    # ========================================

    async def _call_with_circuit_breaker(
        self,
        circuit: Dict,
        func: callable,
        *args,
        **kwargs
    ) -> Any:
        """Call function with circuit breaker protection"""

        if circuit['state'] == CircuitState.OPEN:
            # Check if we should try half-open
            if circuit['last_failure_time']:
                time_since_failure = datetime.now() - circuit['last_failure_time']
                if time_since_failure > timedelta(seconds=30):
                    logger.info(f"ðŸ”Œ Circuit {circuit['name']} transitioning to HALF_OPEN")
                    circuit['state'] = CircuitState.HALF_OPEN
                else:
                    raise ConnectionError(f"Circuit {circuit['name']} is OPEN")

        try:
            result = await func(*args, **kwargs)

            # Success - close circuit
            if circuit['state'] == CircuitState.HALF_OPEN:
                logger.info(f"âœ“ Circuit {circuit['name']} closing (recovered)")
                circuit['state'] = CircuitState.CLOSED
                circuit['failures'] = 0

            return result

        except Exception as e:
            circuit['failures'] += 1
            circuit['last_failure_time'] = datetime.now()

            if circuit['failures'] >= 3:
                logger.warning(f"âš ï¸ Circuit {circuit['name']} opening (too many failures)")
                circuit['state'] = CircuitState.OPEN

            raise

    # ========================================
    # Pattern 2: Exponential Backoff + Jitter
    # ========================================

    async def _retry_with_backoff(
        self,
        func: callable,
        *args,
        **kwargs
    ) -> Any:
        """Retry function with exponential backoff and jitter"""
        import random

        max_retries = self.retry_strategy['max_retries']
        base_delay = self.retry_strategy['base_delay']
        max_delay = self.retry_strategy['max_delay']

        for attempt in range(max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                if attempt >= max_retries:
                    raise

                # Exponential backoff with full jitter
                exp_delay = min(base_delay * (2 ** attempt), max_delay)
                delay = random.uniform(0, exp_delay)

                logger.warning(f"Retry {attempt + 1}/{max_retries} after {delay:.2f}s: {e}")
                await asyncio.sleep(delay)

    # ========================================
    # Pattern 3: Idempotency
    # ========================================

    def _generate_action_id(self, action: str, context: IncidentContext) -> str:
        """Generate unique ID for action + context"""
        data = f"{action}:{context.incident_id}:{context.service}"
        return hashlib.sha256(data.encode()).hexdigest()[:16]

    async def _execute_idempotent(
        self,
        action: str,
        context: IncidentContext,
        func: callable,
        *args,
        **kwargs
    ) -> Any:
        """Execute action idempotently"""

        action_id = self._generate_action_id(action, context)

        # Check if already completed
        if action_id in self.completed_actions:
            logger.info(f"â­ Skipping {action} (already completed)")
            return self.completed_actions[action_id]

        # Execute
        logger.info(f"â–¶ Executing {action}")
        result = await func(*args, **kwargs)

        # Mark complete
        self.completed_actions[action_id] = result
        context.remediation_attempts.append(action)

        return result

    # ========================================
    # Pattern 4: State Checkpointing
    # ========================================

    def _save_checkpoint(self, context: IncidentContext):
        """Save incident state checkpoint"""
        self.checkpoints[context.incident_id] = context
        logger.debug(f"ðŸ’¾ Checkpoint saved: step {context.checkpoint_step}")

    def _load_checkpoint(self, incident_id: str) -> Optional[IncidentContext]:
        """Load incident state checkpoint"""
        return self.checkpoints.get(incident_id)

    # ========================================
    # Pattern 5: Graceful Degradation
    # ========================================

    async def _get_metrics_with_fallback(
        self,
        context: IncidentContext
    ) -> Dict:
        """Get metrics with fallback to cache"""

        try:
            # Try to get fresh metrics through circuit breaker
            metrics = await self._call_with_circuit_breaker(
                self.datadog_circuit,
                self._fetch_metrics,
                context.service
            )

            # Cache successful result
            self.metrics_cache[context.service] = metrics
            return metrics

        except Exception as e:
            logger.warning(f"Metrics service unavailable: {e}")

            # Fall back to cache
            if context.service in self.metrics_cache:
                logger.info("ðŸ“¦ Using cached metrics (degraded mode)")
                return self.metrics_cache[context.service]

            # No cache available
            return {"error": "Metrics unavailable", "degraded": True}

    async def _fetch_metrics(self, service: str) -> Dict:
        """Simulate fetching metrics from Datadog"""
        await asyncio.sleep(0.1)
        return {
            "cpu_percent": 85.5,
            "memory_percent": 72.3,
            "error_rate": 0.05
        }

    # ========================================
    # Pattern 6: Self-Healing
    # ========================================

    async def _health_check(self) -> bool:
        """Check agent health"""
        # Check memory, connections, etc.
        # Simplified for demo
        return self.consecutive_failures < self.max_consecutive_failures

    async def _self_heal(self):
        """Attempt self-healing"""
        logger.info("ðŸ”§ Attempting self-heal...")

        # Clear caches
        self.metrics_cache.clear()
        self.logs_cache.clear()

        # Reset circuit breaker counters (if too many failures)
        for circuit in [self.datadog_circuit, self.pagerduty_circuit, self.aws_circuit]:
            if circuit['failures'] > 10:
                circuit['failures'] = 5  # Reset but not to zero
                logger.info(f"  â†’ Reset {circuit['name']} circuit breaker")

        # Check if healing worked
        if await self._health_check():
            logger.info("âœ“ Self-heal successful")
            self.consecutive_failures = 0
        else:
            logger.warning("âš ï¸ Self-heal failed")

    # ========================================
    # Main Incident Response Workflow
    # ========================================

    async def handle_incident(self, context: IncidentContext):
        """
        Handle incident using all resilience patterns.

        Workflow:
        1. Gather context (metrics, logs, alerts) - graceful degradation
        2. Analyze root cause - retry with backoff
        3. Apply remediation - idempotent actions
        4. Verify fix - circuit breaker protection
        5. Checkpoint state throughout - crash recovery
        """

        logger.info(f"\n{'='*70}")
        logger.info(f"ðŸš¨ INCIDENT: {context.incident_id}")
        logger.info(f"   Service: {context.service}")
        logger.info(f"   Severity: {context.severity}")
        logger.info(f"   Description: {context.description}")
        logger.info(f"{'='*70}\n")

        try:
            # Step 1: Gather context (with graceful degradation)
            if context.checkpoint_step < 1:
                context.checkpoint_step = 1
                self._save_checkpoint(context)

                logger.info("Step 1: Gathering incident context...")

                # Get metrics (with fallback to cache)
                context.metrics = await self._get_metrics_with_fallback(context)

                # Get logs (with circuit breaker + retry)
                try:
                    context.logs = await self._retry_with_backoff(
                        self._call_with_circuit_breaker,
                        self.datadog_circuit,
                        self._fetch_logs,
                        context.service
                    )
                except Exception as e:
                    logger.warning(f"Could not fetch logs: {e}")
                    context.logs = {"error": "Logs unavailable"}

                logger.info(f"  âœ“ Context gathered (degraded: {context.metrics.get('degraded', False)})")

            # Step 2: Analyze root cause
            if context.checkpoint_step < 2:
                context.checkpoint_step = 2
                self._save_checkpoint(context)

                logger.info("Step 2: Analyzing root cause...")

                # Analysis logic here
                await asyncio.sleep(0.5)  # Simulate analysis

                logger.info("  âœ“ Root cause identified: High error rate")

            # Step 3: Apply remediation (idempotent)
            if context.checkpoint_step < 3:
                context.checkpoint_step = 3
                self._save_checkpoint(context)

                logger.info("Step 3: Applying remediation...")

                # Restart pods (idempotent action)
                await self._execute_idempotent(
                    "restart_pods",
                    context,
                    self._restart_pods,
                    context.service
                )

                # Clear cache (idempotent action)
                await self._execute_idempotent(
                    "clear_cache",
                    context,
                    self._clear_service_cache,
                    context.service
                )

                logger.info("  âœ“ Remediation applied")

            # Step 4: Verify fix
            if context.checkpoint_step < 4:
                context.checkpoint_step = 4
                self._save_checkpoint(context)

                logger.info("Step 4: Verifying fix...")

                # Wait for recovery
                await asyncio.sleep(2)

                # Check if fixed (with retry)
                is_fixed = await self._retry_with_backoff(
                    self._verify_fix,
                    context.service
                )

                if is_fixed:
                    logger.info("âœ… INCIDENT RESOLVED")
                    logger.info(f"   Total attempts: {len(context.remediation_attempts)}")
                    logger.info(f"   Actions taken: {context.remediation_attempts}")
                else:
                    logger.warning("âš ï¸ Automatic remediation failed, escalating to humans")
                    await self._escalate_to_human(context)

            # Success - reset failure counter
            self.consecutive_failures = 0

        except Exception as e:
            logger.error(f"âŒ Incident handling failed: {e}")
            self.consecutive_failures += 1

            # Check if we need self-healing
            if not await self._health_check():
                await self._self_heal()

            # Escalate if too many failures
            if self.consecutive_failures >= self.max_consecutive_failures:
                logger.error("ðŸš¨ Too many consecutive failures, escalating")
                await self._escalate_to_human(context)
            else:
                # Try again
                logger.info("Retrying incident handling...")
                await asyncio.sleep(5)
                await self.handle_incident(context)

    async def _fetch_logs(self, service: str) -> Dict:
        """Simulate fetching logs"""
        await asyncio.sleep(0.1)
        return {"recent_errors": ["Connection timeout", "Database deadlock"]}

    async def _restart_pods(self, service: str):
        """Simulate restarting Kubernetes pods"""
        logger.info(f"  ðŸ”„ Restarting pods for {service}...")
        await asyncio.sleep(1)
        logger.info(f"  âœ“ Pods restarted")

    async def _clear_service_cache(self, service: str):
        """Simulate clearing service cache"""
        logger.info(f"  ðŸ—‘ Clearing cache for {service}...")
        await asyncio.sleep(0.5)
        logger.info(f"  âœ“ Cache cleared")

    async def _verify_fix(self, service: str) -> bool:
        """Verify incident is resolved"""
        await asyncio.sleep(0.5)
        # Simulate checking metrics
        return True  # Fixed!

    async def _escalate_to_human(self, context: IncidentContext):
        """Escalate incident to on-call engineer"""
        logger.error("\nðŸ“ž ESCALATING TO ON-CALL ENGINEER")
        logger.error(f"   Incident ID: {context.incident_id}")
        logger.error(f"   Service: {context.service}")
        logger.error(f"   Attempted: {context.remediation_attempts}")

        # In production: send to PagerDuty
        try:
            await self._call_with_circuit_breaker(
                self.pagerduty_circuit,
                self._send_page,
                context
            )
        except Exception as e:
            logger.error(f"Failed to send page: {e}")

    async def _send_page(self, context: IncidentContext):
        """Simulate sending PagerDuty page"""
        await asyncio.sleep(0.1)
        logger.info("  âœ“ Page sent to on-call engineer")


# ============================================================
# DEMONSTRATION
# ============================================================

async def demo_production_agent():
    """Demonstrate production agent handling incidents"""

    agent = ProductionIncidentAgent()

    # Simulate an incident
    incident = IncidentContext(
        incident_id="INC-20240111-001",
        severity="high",
        service="payment-api",
        description="High error rate detected (5% of requests failing)"
    )

    await agent.handle_incident(incident)


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("PRODUCTION INCIDENT RESPONSE AGENT")
    print("=" * 70)
    print("\nThis demonstrates all 6 resilience patterns working together:")
    print("  1. Circuit Breaker - protects external services")
    print("  2. Exponential Backoff - retries with jitter")
    print("  3. Idempotency - prevents duplicate actions")
    print("  4. Checkpointing - enables crash recovery")
    print("  5. Graceful Degradation - operates with partial data")
    print("  6. Self-Healing - auto-recovers from failures")
    print()

    asyncio.run(demo_production_agent())

    print("\n" + "=" * 70)
    print("PRODUCTION READY")
    print("=" * 70)
    print()
    print("This agent is ready for:")
    print("  âœ“ 24/7 production use")
    print("  âœ“ High-scale incident response")
    print("  âœ“ Graceful handling of external service failures")
    print("  âœ“ Automatic recovery from crashes")
    print("  âœ“ Safe remediation with idempotency")
    print("  âœ“ Human escalation when needed")
    print()
```

**Expected Output:**

```
======================================================================
PRODUCTION INCIDENT RESPONSE AGENT
======================================================================

This demonstrates all 6 resilience patterns working together:
  1. Circuit Breaker - protects external services
  2. Exponential Backoff - retries with jitter
  3. Idempotency - prevents duplicate actions
  4. Checkpointing - enables crash recovery
  5. Graceful Degradation - operates with partial data
  6. Self-Healing - auto-recovers from failures

ðŸš€ Production incident agent initialized

======================================================================
ðŸš¨ INCIDENT: INC-20240111-001
   Service: payment-api
   Severity: high
   Description: High error rate detected (5% of requests failing)
======================================================================

Step 1: Gathering incident context...
  âœ“ Context gathered (degraded: False)
ðŸ’¾ Checkpoint saved: step 1

Step 2: Analyzing root cause...
  âœ“ Root cause identified: High error rate
ðŸ’¾ Checkpoint saved: step 2

Step 3: Applying remediation...
â–¶ Executing restart_pods
  ðŸ”„ Restarting pods for payment-api...
  âœ“ Pods restarted
â–¶ Executing clear_cache
  ðŸ—‘ Clearing cache for payment-api...
  âœ“ Cache cleared
  âœ“ Remediation applied
ðŸ’¾ Checkpoint saved: step 3

Step 4: Verifying fix...
âœ… INCIDENT RESOLVED
   Total attempts: 2
   Actions taken: ['restart_pods', 'clear_cache']
ðŸ’¾ Checkpoint saved: step 4
```

### Key Architectural Decisions

**1. Circuit Breaker Placement**
- Wrap ALL external service calls (Datadog, PagerDuty, AWS)
- Set appropriate failure thresholds (3 failures = open)
- Include half-open state for recovery testing

**2. Retry Strategy**
- Use exponential backoff for transient failures
- Add full jitter to prevent thundering herd
- Set maximum retry budget (don't retry forever)

**3. Idempotency Design**
- Hash action + context to create unique IDs
- Store completed actions in memory/database
- Safe to retry any action multiple times

**4. Checkpoint Granularity**
- Save state after each major step
- Include enough context to resume workflow
- Clean up old checkpoints periodically

**5. Degradation Hierarchy**
- Fresh data > Cached data > Static defaults > Error
- Define acceptable staleness for each data type
- Clearly indicate when operating in degraded mode

**6. Self-Healing Boundaries**
- Limit automatic attempts (5 consecutive failures max)
- Always escalate when self-healing fails
- Log all healing actions for audit trail

---

## 8. Hands-On Exercise: Build Your Own Resilient Agent

### Exercise Overview

Build a resilient **Database Migration Agent** that automatically handles schema migrations with proper safeguards.

**Requirements:**

1. Execute migrations idempotently (safe to retry)
2. Checkpoint progress for crash recovery
3. Retry transient database connection failures
4. Degrade gracefully when monitoring is unavailable
5. Circuit breaker for database connections
6. Self-heal when connection pool exhausted

### Starter Code

```python
"""
Exercise: Build a resilient database migration agent.

TODO: Implement all 6 resilience patterns.
"""

import asyncio
from typing import List, Dict
from dataclasses import dataclass


@dataclass
class Migration:
    """Database migration"""
    id: str
    name: str
    sql: str
    requires: List[str]  # Dependencies


class DatabaseMigrationAgent:
    """
    Your task: Make this agent resilient!

    Implement:
    1. Circuit breaker for database connections
    2. Exponential backoff for retries
    3. Idempotent migration execution
    4. State checkpointing
    5. Graceful degradation (monitoring optional)
    6. Self-healing for connection issues
    """

    def __init__(self):
        self.applied_migrations = set()

    async def apply_migrations(self, migrations: List[Migration]):
        """
        TODO: Implement resilient migration application.

        Hints:
        - Check which migrations are already applied (idempotency)
        - Save checkpoints after each migration
        - Retry connection failures with backoff
        - Use circuit breaker for database
        - Report to monitoring (with fallback)
        - Self-heal on connection pool exhaustion
        """
        pass

    async def _connect_to_database(self):
        """TODO: Add circuit breaker here"""
        pass

    async def _execute_migration(self, migration: Migration):
        """TODO: Make this idempotent"""
        pass

    async def _is_migration_applied(self, migration_id: str) -> bool:
        """TODO: Check if migration already applied"""
        pass

    async def _save_checkpoint(self, migration_id: str):
        """TODO: Save progress checkpoint"""
        pass

    async def _load_checkpoint(self) -> set:
        """TODO: Load progress from checkpoint"""
        pass

    async def _report_metrics(self, migration: Migration):
        """TODO: Report to monitoring with graceful degradation"""
        pass


# Test migrations
MIGRATIONS = [
    Migration("001", "create_users_table", "CREATE TABLE users (...)", []),
    Migration("002", "add_email_column", "ALTER TABLE users ADD email VARCHAR(255)", ["001"]),
    Migration("003", "create_orders_table", "CREATE TABLE orders (...)", ["001"]),
]


async def main():
    agent = DatabaseMigrationAgent()
    await agent.apply_migrations(MIGRATIONS)


if __name__ == "__main__":
    asyncio.run(main())
```

### Exercise Steps

**Step 1: Add Circuit Breaker**
- Wrap database connection calls
- Track consecutive failures
- Implement CLOSED â†’ OPEN â†’ HALF_OPEN state machine

**Step 2: Add Exponential Backoff**
- Retry failed connections with increasing delays
- Add jitter to prevent thundering herd
- Set maximum retry limit

**Step 3: Make Migrations Idempotent**
- Check if migration already applied before executing
- Store applied migration IDs in database
- Hash migration content for verification

**Step 4: Add Checkpointing**
- Save progress after each successful migration
- Load checkpoint on startup
- Resume from last successful migration

**Step 5: Add Graceful Degradation**
- Make monitoring optional (don't fail if unavailable)
- Cache migration history for fallback
- Operate in degraded mode if needed

**Step 6: Add Self-Healing**
- Detect connection pool exhaustion
- Automatically reset connections
- Clear statement caches
- Escalate if healing fails

### Solution Outline

```python
# Hints for implementation:

# 1. Circuit breaker state
class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

# 2. Retry with backoff
async def _retry_with_backoff(self, func, *args):
    for attempt in range(self.max_retries):
        try:
            return await func(*args)
        except Exception as e:
            delay = min(2 ** attempt, 32) * random.uniform(0.5, 1.5)
            await asyncio.sleep(delay)

# 3. Idempotency check
async def _is_migration_applied(self, migration_id: str) -> bool:
    result = await self.db.query(
        "SELECT 1 FROM schema_migrations WHERE id = $1",
        migration_id
    )
    return result is not None

# 4. Checkpoint save/load
def _save_checkpoint(self, migration_id: str):
    with open('.migration_checkpoint', 'w') as f:
        json.dump({'last_applied': migration_id}, f)

# 5. Graceful degradation
async def _report_metrics(self, migration: Migration):
    try:
        await self.metrics_client.report(migration)
    except Exception:
        logger.warning("Metrics unavailable, continuing anyway")

# 6. Self-healing
async def _self_heal(self):
    logger.info("Resetting database connections...")
    await self.db_pool.close_all()
    await self.db_pool.open()
```

### Bonus Challenges

1. **Add Dependency Resolution**
   - Ensure migrations run in correct order
   - Skip if dependencies not met

2. **Add Rollback Support**
   - Store rollback SQL for each migration
   - Implement circuit breaker for rollbacks too

3. **Add Dry-Run Mode**
   - Preview migrations without applying
   - Validate SQL syntax

4. **Add Monitoring Dashboard**
   - Track migration success rate
   - Alert on frequent retries
   - Show degradation status

### Learning Objectives

By completing this exercise, you'll understand:

âœ“ How to combine multiple resilience patterns
âœ“ When to use each pattern
âœ“ How patterns interact with each other
âœ“ Common pitfalls and how to avoid them
âœ“ Production-ready error handling

---

## Summary

In this chapter, we learned how to **prevent** Ralph Wiggum loops by building resilient systems:

| Pattern | Purpose | When to Use |
|---------|---------|-------------|
| **Circuit Breaker** | Stop cascading failures | External service calls |
| **Exponential Backoff** | Retry without overwhelming | Transient failures |
| **Idempotency** | Safe retries | State-changing operations |
| **Checkpointing** | Crash recovery | Long-running workflows |
| **Graceful Degradation** | Partial functionality | Multi-component systems |
| **Self-Healing** | Automatic recovery | Production services |

**Key Takeaways:**

1. **Prevention > Detection** - Design systems that can't loop (Chapter 21) rather than only detecting loops (Chapter 20)
2. **Defense in Depth** - Use multiple patterns together for comprehensive resilience
3. **Fail Gracefully** - Better to provide reduced functionality than complete failure
4. **Human Escalation** - Always have a path to escalate when automation fails
5. **Test Regularly** - Use chaos engineering to verify resilience patterns work

**Next Steps:**

- Review Chapter 20 for loop detection techniques
- Practice combining patterns in your own agents
- Study production incident postmortems
- Implement chaos engineering tests
- Build monitoring for all resilience patterns

---

**Chapter 21 Complete!** You now have the tools to build production-ready resilient agentic systems. âœ…