# Chapter 2: Understanding Large Language Models (LLMs) and Tokens

## The Technology Powering Modern AI Assistants

**ğŸ“– Reading time:** ~15 minutes | **âš™ï¸ Hands-on time:** ~20 minutes
**ğŸ¯ Quick nav:** [What is an LLM?](#21-what-is-a-large-language-model) | [How LLMs Work](#22-how-llms-actually-work) | [Tokens](#23-understanding-tokens) | [Context Window](#24-the-context-window) | [Costs](#25-token-economics-understanding-costs) | [ğŸ‹ï¸ Skip to Exercises](#29-hands-on-exercises)

---

## ğŸ“‹ TL;DR (5-Minute Version)

**What you'll learn:** LLMs are trained on vast amounts of text to predict the next word. They don't "think" - they're incredibly sophisticated pattern matchers. Understanding tokens (how LLMs count text) is crucial for managing costs and working within context limits.

**Key concepts:**
- **LLMs** = Massive neural networks trained to predict next tokens based on patterns (not databases, not search engines)
- **Tokens** â‰ˆ 4 characters or Â¾ of a word. "Hello DevOps!" = ~3 tokens, not 2 words
- **Context window** = Maximum tokens LLM can "remember" (Claude Sonnet 4.5 = 200K tokens â‰ˆ 150K words)
- **Costs** = Charged per token: input tokens (what you send) + output tokens (what AI generates)
- **Optimization** = Shorter prompts = lower cost. But too short = worse results. Balance matters.

**Most important takeaway:** Tokens = money and memory. Every character counts. Understanding token economics helps you use AI effectively without blowing your budget. A 10K token prompt on Sonnet costs ~$0.30 - small, but adds up at scale.

**Hands-on:** [Jump to exercises](#29-hands-on-exercises) to practice token counting, plan context windows, and experiment with model behavior.

---

*ğŸ’¡ Want to understand the internals? Keep reading. Just need token basics? You're good - try the exercises!*

---

As a DevOps engineer, understanding how LLMs work will help you use them more effectively, estimate costs, troubleshoot issues, and make informed decisions about integrating AI into your workflows.

---

## 2.1 What is a Large Language Model?

### The Simple Explanation

A **Large Language Model (LLM)** is an AI system trained on massive amounts of text to predict what comes next in a sequence. That's itâ€”at its core, it's a very sophisticated autocomplete.

```
Input:  "The capital of France is"
LLM:    "Paris" (predicted with high confidence)

Input:  "To check disk space in Linux, use the"
LLM:    "df command" (predicted based on patterns in training data)
```

### Why "Large"?

```
Model Size Comparison (Parameters)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GPT-2 (2019)          â–ˆâ–ˆ 1.5 Billion
GPT-3 (2020)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 175 Billion
Claude 4.5 (2025)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ~200B+ (estimated)
GPT-4 (2023)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ~1.7 Trillion (rumored)

For context:
- Human brain: ~86 billion neurons
- Parameters â‰  neurons, but gives sense of scale
```

### The Technical Definition

An LLM is a **neural network** with billions of **parameters** (learned weights) that has been trained on terabytes of text data to model the statistical relationships between words and concepts.

---

## 2.2 How LLMs Actually Work

### The Training Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM TRAINING PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Step 1: Data Collection                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Books (millions of them)                                  â”‚ â”‚
â”‚  â”‚ â€¢ Websites (Common Crawl - billions of pages)               â”‚ â”‚
â”‚  â”‚ â€¢ Wikipedia, academic papers                                â”‚ â”‚
â”‚  â”‚ â€¢ Code repositories (GitHub)                                â”‚ â”‚
â”‚  â”‚ â€¢ Documentation, forums, Q&A sites                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â†“                                      â”‚
â”‚  Step 2: Preprocessing                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Clean and filter data                                     â”‚ â”‚
â”‚  â”‚ â€¢ Remove duplicates, harmful content                        â”‚ â”‚
â”‚  â”‚ â€¢ Convert to tokens                                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â†“                                      â”‚
â”‚  Step 3: Training (Self-Supervised Learning)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Input: "The DevOps engineer deployed the"                   â”‚ â”‚
â”‚  â”‚ Target: "application"                                       â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚ Model predicts â†’ compares to actual â†’ adjusts weights       â”‚ â”‚
â”‚  â”‚ Repeat trillions of times                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â†“                                      â”‚
â”‚  Step 4: Fine-Tuning (Making it Useful)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ RLHF (Reinforcement Learning from Human Feedback)         â”‚ â”‚
â”‚  â”‚ â€¢ Human raters score responses                              â”‚ â”‚
â”‚  â”‚ â€¢ Model learns to be helpful, harmless, honest              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Transformer Architecture

LLMs use a architecture called **Transformers** (introduced in the famous "Attention is All You Need" paper, 2017).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRANSFORMER ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚    Input: "kubectl get pods -n"                                â”‚
â”‚                    â†“                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚         TOKENIZATION                 â”‚                    â”‚
â”‚    â”‚  "kubectl" "get" "pods" "-" "n"      â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                    â†“                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚      EMBEDDING LAYER                 â”‚                    â”‚
â”‚    â”‚  Convert tokens to vectors           â”‚                    â”‚
â”‚    â”‚  [0.23, -0.45, 0.12, ...]            â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                    â†“                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚      ATTENTION MECHANISM             â”‚  â† The magic!      â”‚
â”‚    â”‚  "Which words relate to which?"      â”‚                    â”‚
â”‚    â”‚                                      â”‚                    â”‚
â”‚    â”‚  "kubectl" pays attention to "pods"  â”‚                    â”‚
â”‚    â”‚  "-n" pays attention to "namespace"  â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                    â†“                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚      FEED FORWARD LAYERS             â”‚                    â”‚
â”‚    â”‚  Process and transform               â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                    â†“                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚      OUTPUT LAYER                    â”‚                    â”‚
â”‚    â”‚  Probability distribution over       â”‚                    â”‚
â”‚    â”‚  all possible next tokens            â”‚                    â”‚
â”‚    â”‚                                      â”‚                    â”‚
â”‚    â”‚  "production": 0.35                  â”‚                    â”‚
â”‚    â”‚  "default": 0.28                     â”‚                    â”‚
â”‚    â”‚  "kube-system": 0.22                 â”‚                    â”‚
â”‚    â”‚  ...                                 â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                    â†“                                           â”‚
â”‚    Output: "production" (highest probability selected)         â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Attention Mechanism (Simplified)

The key innovation is **self-attention**, which allows the model to weigh the importance of different parts of the input.

```
Example: "The server crashed because the database connection pool was exhausted"

Attention weights (which words relate to "crashed"):

"The"        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.02
"server"     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 0.75  â† High attention (subject of crash)
"crashed"    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.00  â† Self
"because"    â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.30  â† Indicates causation
"the"        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.01
"database"   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 0.55  â† Related to cause
"connection" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 0.45  â† Related to cause
"pool"       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 0.65  â† Key technical detail
"was"        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.03
"exhausted"  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 0.90  â† Root cause!

The model learns: "crashed" strongly relates to "server" and "exhausted"
This understanding is learned from seeing millions of similar patterns
```

---

## 2.3 Understanding Tokens

### What is a Token?

A **token** is the basic unit of text that LLMs process. It's NOT the same as a word.

```
Tokenization Examples (using GPT-style tokenization):

"Hello"           â†’ ["Hello"]                    # 1 token
"Hello world"     â†’ ["Hello", " world"]          # 2 tokens
"Kubernetes"      â†’ ["Kub", "ernetes"]           # 2 tokens
"kubectl"         â†’ ["kub", "ect", "l"]          # 3 tokens
"authentication"  â†’ ["auth", "ent", "ication"]   # 3 tokens

# Special characters and numbers
"192.168.1.1"     â†’ ["192", ".", "168", ".", "1", ".", "1"]  # 7 tokens
"#!/bin/bash"     â†’ ["#!", "/", "bin", "/", "bash"]         # 5 tokens
```

### Why Tokens Instead of Words?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 WHY TOKENIZATION?                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Problem with word-based:                                   â”‚
â”‚  â€¢ Vocabulary would be enormous (millions of words)         â”‚
â”‚  â€¢ Can't handle new words, typos, technical terms           â”‚
â”‚  â€¢ Different languages need different vocabularies          â”‚
â”‚                                                             â”‚
â”‚  Problem with character-based:                              â”‚
â”‚  â€¢ Sequences become very long                               â”‚
â”‚  â€¢ Loses semantic meaning                                   â”‚
â”‚  â€¢ Computationally expensive                                â”‚
â”‚                                                             â”‚
â”‚  Tokens = Best of both worlds:                              â”‚
â”‚  â€¢ Fixed vocabulary size (typically 50K-100K tokens)        â”‚
â”‚  â€¢ Can represent ANY text                                   â”‚
â”‚  â€¢ Captures meaningful subunits                             â”‚
â”‚  â€¢ Efficient computation                                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Common Tokenization Algorithms

#### 1. **Byte-Pair Encoding (BPE)**
Used by GPT models, Claude, and others.

```python
# How BPE Works (Simplified)

# Starting with character-level vocabulary
vocab = ['a', 'b', 'c', ..., 'z', ' ', '.', ...]

# Find most common pair and merge
"the cat sat" â†’ "th" "e " "ca" "t " "sa" "t"

# After training on billions of words:
# Common words become single tokens: "the", " is", " and"
# Rare words get split: "Kubernetes" â†’ "Kub" + "ernetes"
```

#### 2. **WordPiece**
Used by BERT-based models.

```
"unhappiness" â†’ ["un", "##happy", "##ness"]

# The ## indicates continuation of previous token
```

### Token Counting: DevOps Examples

```python
# Approximate token counts (varies by model)

# Simple text
"Hello, world!"
# Tokens: ~4 (Hello, ,, world, !)

# Kubernetes manifest
"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
"""
# Tokens: ~35

# A typical bash script (50 lines)
# Tokens: ~400-600

# A Python file (200 lines of code)
# Tokens: ~1,500-2,500

# Terraform module (100 lines)
# Tokens: ~800-1,200
```

### Rule of Thumb for Token Estimation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TOKEN ESTIMATION RULES                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  English text:                                             â”‚
â”‚  â€¢ ~4 characters per token                                 â”‚
â”‚  â€¢ ~0.75 words per token                                   â”‚
â”‚  â€¢ 1 page of text â‰ˆ 500-700 tokens                         â”‚
â”‚                                                            â”‚
â”‚  Code:                                                     â”‚
â”‚  â€¢ More tokens per line (special characters, indentation)  â”‚
â”‚  â€¢ Python/JS: ~8-12 tokens per line                        â”‚
â”‚  â€¢ YAML/JSON: ~5-8 tokens per line                         â”‚
â”‚  â€¢ Variable names may be multiple tokens                   â”‚
â”‚                                                            â”‚
â”‚  Quick mental math:                                        â”‚
â”‚  â€¢ 1,000 words â‰ˆ 1,300 tokens                              â”‚
â”‚  â€¢ 100 lines of code â‰ˆ 1,000 tokens                        â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2.4 The Context Window

### What is Context Window?

The **context window** is the maximum number of tokens an LLM can process at once (input + output combined).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTEXT WINDOW                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  Your Input (Prompt)                    â”‚   â”‚
â”‚  â”‚  â€¢ Your question/request                                â”‚   â”‚
â”‚  â”‚  â€¢ Code you paste                                       â”‚   â”‚
â”‚  â”‚  â€¢ System instructions                                  â”‚   â”‚
â”‚  â”‚  â€¢ Conversation history                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           +                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  Model Output                           â”‚   â”‚
â”‚  â”‚  â€¢ The response generated                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           =                                    â”‚
â”‚         Must fit within Context Window limit                   â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Context Window Sizes (2025)

```
Model Context Windows
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GPT-4 Turbo        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  128K tokens (~100K words)
Claude Opus 4.5    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  200K tokens (~150K words)
Claude Sonnet 4.5  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  200K tokens
Claude Haiku 4.5   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  200K tokens

Practical comparison:
â”œâ”€â”€ 8K tokens   = ~20 pages of text
â”œâ”€â”€ 32K tokens  = ~80 pages of text
â”œâ”€â”€ 128K tokens = ~300 pages (a short book!)
â””â”€â”€ 200K tokens = ~500 pages
```

### Why Context Window Matters for DevOps

```yaml
# Scenario: Debugging a complex issue

context_needed:
  - error_logs: "~2,000 tokens"
  - config_files: "~3,000 tokens"
  - recent_code_changes: "~4,000 tokens"
  - conversation_history: "~2,000 tokens"
  - system_prompt: "~500 tokens"
  total_input: "~11,500 tokens"

# With GPT-3.5 (16K window):
#   Input: 11,500 + Output room: 4,500 = OK

# With a smaller context window:
#   Would need to truncate logs or split the task
```

### Strategies for Working with Context Limits

```python
# Strategy 1: Summarize large files
# Instead of pasting 1000 lines of logs

bad_prompt = f"Here are all my logs: {giant_log_file}"  # May exceed limit

good_prompt = """
Here's a summary of my logs:
- 500 INFO messages (normal operation)
- 15 WARN messages (mostly about deprecation)
- 3 ERROR messages (pasted below)

Error 1: [timestamp] Connection refused...
Error 2: [timestamp] Timeout after 30s...
Error 3: [timestamp] Out of memory...

What's causing these errors?
"""

# Strategy 2: Focus on relevant sections
# Instead of entire codebase

bad_prompt = "Here's my entire application: {all_files}"

good_prompt = """
I'm having an issue with database connections.
Here's the relevant code:

database.py (connection handling):
{connection_code}

config.py (database settings):
{db_config}

The error occurs in database.py line 45.
"""

# Strategy 3: Iterative refinement
# Break large tasks into steps

step1 = "Analyze this error message: {error}"
step2 = "Based on the error, what files should I check?"
step3 = "Here's the relevant file. What's wrong?"
```

---

## 2.5 Token Economics: Understanding Costs

### How Pricing Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM PRICING MODEL                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  You pay for:                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  INPUT TOKENS                                            â”‚  â”‚
â”‚  â”‚  â€¢ Your prompt                                           â”‚  â”‚
â”‚  â”‚  â€¢ System instructions                                   â”‚  â”‚
â”‚  â”‚  â€¢ Context you provide                                   â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Usually cheaper (the model just reads)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           +                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OUTPUT TOKENS                                           â”‚  â”‚
â”‚  â”‚  â€¢ The model's response                                  â”‚  â”‚
â”‚  â”‚  â€¢ Usually more expensive (computation intensive)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pricing Examples (Approximate, as of 2024)

```
Cost per 1 Million Tokens (USD) - 2025
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model               Input        Output
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Claude Haiku 4.5    $1.00        $5.00
Claude Sonnet 4.5   $3.00        $15.00
Claude Opus 4.5     $5.00        $25.00
GPT-4 Turbo         $10.00       $30.00

Note: Prices change frequently. Check current pricing.
```

### Real-World Cost Calculations

```python
# Example 1: Code Review Bot

# Input per review:
# - Code diff: ~500 tokens
# - Instructions: ~200 tokens
# - Context: ~300 tokens
# Total input: 1,000 tokens

# Output per review:
# - Feedback: ~400 tokens

# Using Claude Sonnet 4.5:
# Input cost: 1,000 / 1,000,000 * $3 = $0.003
# Output cost: 400 / 1,000,000 * $15 = $0.006
# Total per review: $0.009

# 100 PRs per day = $0.90/day = ~$27/month


# Example 2: Log Analysis Pipeline

# Input per analysis:
# - Log batch: ~5,000 tokens
# - System prompt: ~500 tokens
# Total input: 5,500 tokens

# Output:
# - Summary: ~300 tokens

# Using Claude Haiku 4.5 (cheaper for simple tasks):
# Input cost: 5,500 / 1,000,000 * $1 = $0.0055
# Output cost: 300 / 1,000,000 * $5 = $0.0015
# Total per analysis: $0.007

# 1,000 analyses per day = $7/day = ~$210/month


# Example 3: Interactive Debugging Session

# Average conversation:
# - 10 back-and-forth exchanges
# - ~2,000 tokens input per exchange (including history)
# - ~500 tokens output per exchange

# Using GPT-4 Turbo:
# Input: 20,000 tokens â†’ $0.20
# Output: 5,000 tokens â†’ $0.15
# Total per session: $0.35

# 50 sessions per month = $17.50/month
```

### Cost Optimization Tips

```yaml
# DevOps Cost Optimization for LLM Usage

1_use_appropriate_model:
  description: "Don't use Opus for simple tasks"
  examples:
    - simple_formatting: "Use Claude Haiku 4.5"
    - code_generation: "Use Claude Sonnet 4.5"
    - complex_reasoning: "Use Claude Opus 4.5"

2_minimize_context:
  description: "Only include what's necessary"
  before: |
    Here's my entire application with 50 files...
  after: |
    Here's the relevant error and the 2 files involved...

3_cache_common_patterns:
  description: "Don't re-ask the same questions"
  implementation:
    - hash: "Hash the prompt"
    - cache: "Store in Redis/DynamoDB"
    - ttl: "Set appropriate expiration"

4_batch_requests:
  description: "Combine multiple small requests"
  before: |
    Request 1: Review file1.py
    Request 2: Review file2.py
    Request 3: Review file3.py
  after: |
    Request 1: Review these 3 files: file1.py, file2.py, file3.py

5_use_streaming:
  description: "Stream responses for better UX without extra cost"
  benefit: "User sees response faster, same token cost"
```

---

## 2.6 How LLMs Generate Text

### The Generation Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TEXT GENERATION PROCESS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Input: "Write a bash script to"                               â”‚
â”‚                                                                â”‚
â”‚  Step 1: Process input tokens                                  â”‚
â”‚  ["Write", " a", " bash", " script", " to"]                    â”‚
â”‚                                                                â”‚
â”‚  Step 2: Predict next token                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Probability distribution:              â”‚                    â”‚
â”‚  â”‚ " check"     : 0.15                    â”‚                    â”‚
â”‚  â”‚ " backup"    : 0.12                    â”‚                    â”‚
â”‚  â”‚ " monitor"   : 0.10                    â”‚                    â”‚
â”‚  â”‚ " list"      : 0.08                    â”‚                    â”‚
â”‚  â”‚ " delete"    : 0.05                    â”‚                    â”‚
â”‚  â”‚ ...                                    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                â”‚
â”‚  Step 3: Select token (based on sampling settings)             â”‚
â”‚  Selected: " backup"                                           â”‚
â”‚                                                                â”‚
â”‚  Step 4: Append and repeat                                     â”‚
â”‚  New context: "Write a bash script to backup"                  â”‚
â”‚  Predict next: " the" (0.25), " MySQL" (0.18), ...             â”‚
â”‚                                                                â”‚
â”‚  Continue until:                                               â”‚
â”‚  â€¢ End token generated                                         â”‚
â”‚  â€¢ Max length reached                                          â”‚
â”‚  â€¢ Stop sequence encountered                                   â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Generation Parameters

#### Temperature

```
Temperature: Controls randomness in selection

Temperature = 0.0 (Deterministic)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"backup"  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.25  â† Always picks this
"check"   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.20
"monitor" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.15

Temperature = 0.7 (Balanced)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"backup"  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 0.22  â† Usually this
"check"   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 0.21  â† Sometimes this
"monitor" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.18  â† Occasionally this

Temperature = 1.5 (Creative/Random)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"backup"  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.18
"check"   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.17
"monitor" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.16  â† More variety
"analyze" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.14
"encrypt" â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.12

DevOps Use Cases:
â€¢ temp=0: Consistent code generation, deterministic outputs
â€¢ temp=0.7: General assistance, explanations
â€¢ temp=1.0+: Brainstorming, creative solutions
```

#### Top-P (Nucleus Sampling)

```
Top-P = 0.9: Only consider tokens in top 90% of probability mass

Example distribution:
Token        Probability  Cumulative  Included?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"backup"     0.25         0.25        Yes âœ“
"check"      0.20         0.45        Yes âœ“
"monitor"    0.15         0.60        Yes âœ“
"list"       0.12         0.72        Yes âœ“
"delete"     0.10         0.82        Yes âœ“
"create"     0.08         0.90        Yes âœ“
"encrypt"    0.04         0.94        No âœ— (over 90%)
"hash"       0.03         0.97        No âœ—
"pipe"       0.02         0.99        No âœ—
"dance"      0.01         1.00        No âœ— (filtered out!)
```

#### Max Tokens

```python
# Control maximum response length

# Short response (save tokens/money)
response = client.generate(
    prompt="What's the kubectl command to get pods?",
    max_tokens=100  # Expect short answer
)

# Long response (detailed explanation)
response = client.generate(
    prompt="Explain Kubernetes networking in detail",
    max_tokens=2000  # Allow comprehensive response
)

# Code generation (needs more space)
response = client.generate(
    prompt="Write a complete Terraform module for EKS",
    max_tokens=4000  # Complex code needs room
)
```

---

## 2.7 Understanding Model Behavior

### Why LLMs Can Be "Wrong"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 WHY LLMs MAKE MISTAKES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  1. STATISTICAL PATTERNS, NOT TRUE UNDERSTANDING               â”‚
â”‚     â€¢ LLMs predict likely text, not factually correct text     â”‚
â”‚     â€¢ "Hallucinations" occur when patterns mislead             â”‚
â”‚                                                                â”‚
â”‚  2. TRAINING DATA CUTOFF                                       â”‚
â”‚     â€¢ Model doesn't know events after training date            â”‚
â”‚     â€¢ New tools, versions, APIs may be unknown                 â”‚
â”‚                                                                â”‚
â”‚  3. TRAINING DATA BIASES                                       â”‚
â”‚     â€¢ Reflects patterns in training data                       â”‚
â”‚     â€¢ Popular != correct                                       â”‚
â”‚                                                                â”‚
â”‚  4. CONTEXT LIMITATIONS                                        â”‚
â”‚     â€¢ Can lose track in very long conversations                â”‚
â”‚     â€¢ May "forget" earlier context                             â”‚
â”‚                                                                â”‚
â”‚  5. AMBIGUOUS PROMPTS                                          â”‚
â”‚     â€¢ Garbage in, garbage out                                  â”‚
â”‚     â€¢ Vague questions get vague answers                        â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hallucination Examples

```yaml
# Example 1: Made-up CLI flags

prompt: "What's the kubectl command to restart a deployment?"

bad_response: |
  kubectl restart deployment nginx -n production
  # WRONG! 'restart' is not a kubectl command

correct_response: |
  kubectl rollout restart deployment nginx -n production
  # CORRECT! 'rollout restart' is the proper command


# Example 2: Invented package names

prompt: "What npm package monitors node memory?"

bad_response: |
  npm install node-memory-profiler-live
  # This package might not exist!

correct_response: |
  # There are several options:
  # - process.memoryUsage() (built-in)
  # - v8.getHeapStatistics() (built-in)
  # - heapdump package (for snapshots)
  # - clinic.js (comprehensive profiling)
  # Always verify packages on npmjs.com


# Example 3: Outdated information

prompt: "How do I set up a Kubernetes cluster?"

potentially_outdated: |
  Use kubernetes-the-hard-way with version 1.18...
  # Models may suggest older versions

best_practice: |
  "What's the current recommended way to set up Kubernetes
   clusters in 2024? Please note your training cutoff date."
```

### Verification Strategies for DevOps

```bash
# Always verify AI-generated commands before running!

# Strategy 1: Use --dry-run
kubectl apply -f ai-generated.yaml --dry-run=client

# Strategy 2: Use explain/help
kubectl explain deployment.spec.replicas
terraform plan  # Never apply without plan

# Strategy 3: Check documentation
man <command>
<command> --help

# Strategy 4: Test in isolation
docker run --rm -it ubuntu bash
# Test AI-suggested commands in container first

# Strategy 5: Version check
# AI might suggest deprecated syntax
kubectl version
terraform version
ansible --version
```

---

## 2.8 Practical Token Management

### Counting Tokens Programmatically

```python
# Using tiktoken (OpenAI's tokenizer)
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """Count tokens for a given text and model."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# Example usage
dockerfile = """
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
"""

tokens = count_tokens(dockerfile)
print(f"Dockerfile tokens: {tokens}")  # ~45 tokens

# For Claude, use Anthropic's tokenizer
from anthropic import Anthropic

client = Anthropic()
# Claude's API handles tokenization internally
# You can estimate ~4 chars per token for planning
```

### Token Budget Planning

```python
# Example: Building a code review system with token budgets

class CodeReviewBudget:
    def __init__(self, max_context=100000):  # Claude's context
        self.max_context = max_context
        self.reserved_output = 4000  # Reserve for response
        self.system_prompt = 500     # Fixed system instructions

    def available_for_code(self):
        return self.max_context - self.reserved_output - self.system_prompt

    def can_review(self, code: str) -> bool:
        code_tokens = len(code) // 4  # Rough estimate
        return code_tokens <= self.available_for_code()

    def split_for_review(self, code: str) -> list:
        """Split large codebases into reviewable chunks."""
        available = self.available_for_code()
        chars_per_chunk = available * 4  # Convert back to chars

        chunks = []
        for i in range(0, len(code), chars_per_chunk):
            chunks.append(code[i:i + chars_per_chunk])
        return chunks

# Usage
budget = CodeReviewBudget()
large_codebase = open("main.py").read()

if budget.can_review(large_codebase):
    review(large_codebase)
else:
    chunks = budget.split_for_review(large_codebase)
    for i, chunk in enumerate(chunks):
        print(f"Reviewing chunk {i+1}/{len(chunks)}")
        review(chunk)
```

---

## 2.9 Hands-On Exercises

### Exercise 1: Token Counting Practice

```markdown
## Token Estimation Exercise

Estimate the tokens for each, then verify with a tokenizer:

1. Your company's main README.md file
   Estimate: _______
   Actual: _______

2. A typical Kubernetes deployment manifest
   Estimate: _______
   Actual: _______

3. Your longest shell script
   Estimate: _______
   Actual: _______

4. The last error log you debugged
   Estimate: _______
   Actual: _______
```

### Exercise 2: Context Window Planning

```yaml
# Plan a conversation for debugging a production issue

Scenario: API returning 500 errors intermittently

Available context: 100,000 tokens

# Plan your token budget:
system_instructions:
  content: "You are a DevOps expert..."
  estimated_tokens: _____

error_logs:
  content: "Last 100 relevant log entries"
  estimated_tokens: _____

config_files:
  content: "Relevant YAML configs"
  estimated_tokens: _____

code_context:
  content: "Related source files"
  estimated_tokens: _____

conversation_history:
  content: "Previous Q&A in session"
  estimated_tokens: _____

reserved_for_response:
  estimated_tokens: _____

# Total: _____ / 100,000
```

### Exercise 3: Temperature Experiment

```markdown
## Temperature Effects on Code Generation

Try the same prompt with different temperatures:

Prompt: "Write a bash function to retry a command 3 times with backoff"

Temperature 0.0:
```
[Response here]
```

Temperature 0.7:
```
[Response here]
```

Temperature 1.2:
```
[Response here]
```

## Observations:
- Consistency at temp 0:
- Variation at temp 0.7:
- Creativity/Errors at temp 1.2:
```

---

## 2.10 Chapter Summary

### Key Takeaways

1. **LLMs are sophisticated pattern matchers** - They predict statistically likely text based on training data

2. **Tokens are the currency of LLMs** - Everything (input + output) is measured in tokens; ~4 characters per token

3. **Context window is your working memory** - Plan your prompts to fit within limits; larger windows enable more complex tasks

4. **Cost = Input tokens + Output tokens** - Use appropriate models for different tasks to optimize costs

5. **Temperature controls creativity vs. consistency** - Use low temperature for deterministic outputs, higher for brainstorming

6. **Always verify AI outputs** - LLMs can hallucinate; trust but verify, especially for commands and configurations

### Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM QUICK REFERENCE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Token Estimation:                                             â”‚
â”‚  â€¢ ~4 characters = 1 token                                     â”‚
â”‚  â€¢ ~0.75 words = 1 token                                       â”‚
â”‚  â€¢ 100 lines of code â‰ˆ 1,000 tokens                            â”‚
â”‚                                                                â”‚
â”‚  Temperature Guide:                                            â”‚
â”‚  â€¢ 0.0 = Deterministic (code, configs)                         â”‚
â”‚  â€¢ 0.7 = Balanced (general use)                                â”‚
â”‚  â€¢ 1.0+ = Creative (brainstorming)                             â”‚
â”‚                                                                â”‚
â”‚  Context Windows (2025):                                       â”‚
â”‚  â€¢ GPT-4 Turbo: 128K tokens                                    â”‚
â”‚  â€¢ Claude 4.5: 200K tokens                                     â”‚
â”‚                                                                â”‚
â”‚  Verification Commands:                                        â”‚
â”‚  â€¢ kubectl --dry-run=client                                    â”‚
â”‚  â€¢ terraform plan                                              â”‚
â”‚  â€¢ shellcheck script.sh                                        â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

[â† Previous: Introduction to AI](./01-introduction-to-ai.md) | [Next: The Art of Prompting â†’](./03-the-art-of-prompting.md)

---

**Part of**: AI and Claude Code - A Comprehensive Guide for DevOps Engineers  
**Created by**: Michel Abboud with Claude Sonnet 4.5 (Anthropic)  
**Copyright**: Â© 2026 Michel Abboud. All rights reserved.  
**License**: CC BY-NC 4.0
